{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3603932",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mjnp\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/extravaganza/env/lib/python3.10/site-packages/torch/__init__.py:1187\u001b[0m\n\u001b[1;32m   1180\u001b[0m         __all__\u001b[38;5;241m.\u001b[39mappend(name)\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;66;03m# Import interface functions defined in Python\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m \n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# needs to be after the above ATen bindings so we can overwrite from Python side\u001b[39;00m\n\u001b[0;32m-> 1187\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Remove unnecessary members\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _StorageBase\n",
      "File \u001b[0;32m~/Desktop/extravaganza/env/lib/python3.10/site-packages/torch/functional.py:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _add_docstr\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopt_einsum\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mopt_einsum\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lowrank\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m svd_lowrank, pca_lowrank\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moverrides\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     11\u001b[0m     has_torch_function, has_torch_function_unary, has_torch_function_variadic,\n\u001b[1;32m     12\u001b[0m     handle_torch_function)\n",
      "File \u001b[0;32m~/Desktop/extravaganza/env/lib/python3.10/site-packages/torch/nn/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparameter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     Parameter \u001b[38;5;28;01mas\u001b[39;00m Parameter,\n\u001b[1;32m      4\u001b[0m     UninitializedParameter \u001b[38;5;28;01mas\u001b[39;00m UninitializedParameter,\n\u001b[1;32m      5\u001b[0m     UninitializedBuffer \u001b[38;5;28;01mas\u001b[39;00m UninitializedBuffer,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataParallel \u001b[38;5;28;01mas\u001b[39;00m DataParallel\n",
      "File \u001b[0;32m~/Desktop/extravaganza/env/lib/python3.10/site-packages/torch/nn/modules/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodule\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Module\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Identity, Linear, Bilinear, LazyLinear\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conv1d, Conv2d, Conv3d, \\\n\u001b[1;32m      4\u001b[0m     ConvTranspose1d, ConvTranspose2d, ConvTranspose3d, \\\n\u001b[1;32m      5\u001b[0m     LazyConv1d, LazyConv2d, LazyConv3d, LazyConvTranspose1d, LazyConvTranspose2d, LazyConvTranspose3d\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Threshold, ReLU, Hardtanh, ReLU6, Sigmoid, Tanh, \\\n\u001b[1;32m      7\u001b[0m     Softmax, Softmax2d, LogSoftmax, ELU, SELU, CELU, GELU, Hardshrink, LeakyReLU, LogSigmoid, \\\n\u001b[1;32m      8\u001b[0m     Softplus, Softshrink, MultiheadAttention, PReLU, Softsign, Softmin, Tanhshrink, RReLU, GLU, \\\n\u001b[1;32m      9\u001b[0m     Hardsigmoid, Hardswish, SiLU, Mish\n",
      "File \u001b[0;32m~/Desktop/extravaganza/env/lib/python3.10/site-packages/torch/nn/modules/linear.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparameter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Parameter, UninitializedParameter\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional \u001b[38;5;28;01mas\u001b[39;00m F\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodule\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Module\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:879\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1012\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:672\u001b[0m, in \u001b[0;36m_compile_bytecode\u001b[0;34m(data, name, bytecode_path, source_path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from abc import abstractmethod\n",
    "import inspect\n",
    "from typing import Tuple\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "import tqdm\n",
    "import scipy\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "\"\"\"\n",
    "at the moment DL is in pytorch, once its finalized i can convert to jax for efficiency :)\n",
    "\"\"\"\n",
    "\n",
    "from models import MLP\n",
    "from rescalers import FIXED_RESCALE, EMA_RESCALE, ADAM, D_ADAM, DoWG, IDENTITY\n",
    "from stats import Stats\n",
    "\n",
    "def _sigmoid(t):\n",
    "    return 1 / (1 + np.exp(-t))\n",
    "\n",
    "def _inv_sigmoid(s):\n",
    "    return np.log(s / (1 - s + 1e-8))\n",
    "\n",
    "def _d_sigmoid(t):\n",
    "    s = _sigmoid(t)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def _rescale(t, bounds, use_sigmoid):\n",
    "    \"\"\"\n",
    "    rescales from `[0, 1] -> [tmin, tmax]`\n",
    "    \"\"\"\n",
    "    tmin, tmax = bounds\n",
    "    if use_sigmoid: t = _sigmoid(t)\n",
    "    return tmin + (tmax - tmin) * t\n",
    "\n",
    "def _d_rescale(t, bounds, use_sigmoid):\n",
    "    tmin, tmax = bounds\n",
    "    d = tmax - tmin\n",
    "    if use_sigmoid: d *= _d_sigmoid(t)\n",
    "    return d\n",
    "\n",
    "def _inv_rescale(s, bounds, use_sigmoid):\n",
    "    tmin, tmax = bounds\n",
    "    t = (s - tmin) / (tmax - tmin)\n",
    "    if use_sigmoid: t = _inv_sigmoid(t)\n",
    "    return t\n",
    "\n",
    "def _generate_uniform(shape, norm=1.00):\n",
    "    v = np.random.normal(size=shape)\n",
    "    v = norm * v / np.linalg.norm(v)\n",
    "    v = jnp.array(v)\n",
    "    return v\n",
    "\n",
    "def _exponential_linspace_int(start, end, num, divisible_by=1):\n",
    "    \"\"\"Exponentially increasing values of integers.\"\"\"\n",
    "    base = np.exp(np.log(end / start) / (num - 1))\n",
    "    return [int(np.round(start * base**i / divisible_by) * divisible_by) for i in range(num)]\n",
    "\n",
    "def _append(arr, val):\n",
    "    \"\"\"\n",
    "    rightmost recent appending, i.e. arr = (val_{t-h}, ..., val_{t-1}, val_t)\n",
    "    \"\"\"\n",
    "    if isinstance(arr, torch.Tensor):\n",
    "        if not isinstance(val, torch.Tensor):\n",
    "            val = torch.tensor(val, dtype=arr.dtype)\n",
    "        arr = torch.roll(arr, -1, dims=(0,)).clone()  # have to do it this way for autograd, but its not slower for some reason\n",
    "        arr[-1] = val\n",
    "    elif isinstance(arr, jnp.ndarray):\n",
    "        if not isinstance(val, jnp.ndarray):\n",
    "            val = jnp.array(val, dtype=arr.dtype)\n",
    "        arr = arr.at[0].set(val)\n",
    "        arr = jnp.roll(arr, -1, axis=0)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0b3475",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SysID:\n",
    "    \"\"\"\n",
    "    Determine `A` and `B` matrices for a LDS `x_{t+1} = A @ x_t + B @ u_t\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 method: str,  # must be one of ['HAZAN', 'REGRESSION']\n",
    "                 control_dim: int,\n",
    "                 state_dim: int,\n",
    "                 scale: float):\n",
    "        \n",
    "        assert method in ['HAZAN', 'REGRESSION']\n",
    "        self.method = method\n",
    "        self.control_dim = control_dim\n",
    "        self.state_dim = state_dim\n",
    "        self.scale = scale\n",
    "        \n",
    "        self.control_history = []\n",
    "        self.state_history = []\n",
    "        self.eps_history = []\n",
    "        \n",
    "        self.t = 1\n",
    "        self.A = self.B = None\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def perturb_control(self,\n",
    "                        state: jnp.ndarray,\n",
    "                        control: jnp.ndarray=None):\n",
    "        \"\"\"\n",
    "        if `control` is not None, we perturb around this control only when 'HAZAN' mode. otherwise, we don't perturb\n",
    "        \"\"\"\n",
    "        assert state.shape == (self.state_dim,)\n",
    "        \n",
    "        if self.method == 'HAZAN' or control is None:\n",
    "            control = control if control is not None else jnp.zeros(self.control_dim)\n",
    "#             eps = 1 - 2 * np.random.randint(2, size=(self.control_dim,))  # random rademacher direction\n",
    "            eps = np.random.randn(self.control_dim)\n",
    "            control = control + self.scale * eps / self.t ** 0.25\n",
    "        else:\n",
    "            eps = np.zeros(self.control_dim)\n",
    "\n",
    "        self.control_history.append(control)\n",
    "        self.state_history.append(state)\n",
    "        self.eps_history.append(eps)\n",
    "        self.t += 1\n",
    "        return control\n",
    "    \n",
    "    \n",
    "    def sysid(self):\n",
    "        assert self.t > 1\n",
    "        \n",
    "        if self.A is not None: return self.A, self.B\n",
    "        \n",
    "        if self.method == 'HAZAN':\n",
    "            k = int(0.15 * self.t)\n",
    "\n",
    "            states = jnp.array(self.state_history)\n",
    "            eps = jnp.array(self.eps_history)\n",
    "\n",
    "            # prepare vectors and retrieve B\n",
    "            scan_len = self.t - k - 1 # need extra -1 because we iterate over j = 0, ..., k\n",
    "            N_j = jnp.array([jnp.dot(states[j + 1: j + 1 + scan_len].T, eps[:scan_len]) for j in range(k + 1)]) / scan_len\n",
    "            B = N_j[0] # jnp.dot(states[1:].T, eps[:-1]) / (self.t - 1)\n",
    "\n",
    "            # retrieve A\n",
    "            C_0, C_1 = N_j[:-1], N_j[1:]\n",
    "            C_inv = jnp.linalg.inv(jnp.tensordot(C_0, C_0, axes=((0, 2), (0, 2))) + 1e-3 * np.identity(self.state_dim))\n",
    "            A = jnp.tensordot(C_1, C_0, axes=((0, 2), (0, 2))) @ C_inv\n",
    "\n",
    "        elif self.method == 'REGRESSION':\n",
    "            # transform x and u into regular numpy arrays for least squares\n",
    "            states = np.array(self.state_history)\n",
    "            controls = np.array(self.control_history)\n",
    "\n",
    "            # regression on A and B jointly\n",
    "            A_B = scipy.linalg.lstsq(np.hstack((states[:-1], controls[:-1])), states[1:])[0]\n",
    "            A, B = jnp.array(A_B[:self.state_dim]).T, jnp.array(A_B[self.state_dim:]).T\n",
    "        \n",
    "        self.A, self.B = A, B\n",
    "        return A, B\n",
    "    \n",
    "    \n",
    "    def dynamics(self,\n",
    "                 state: jnp.ndarray,\n",
    "                 control: jnp.ndarray):\n",
    "        assert state.shape == (self.state_dim,) and control.shape == (self.control_dim,)\n",
    "        A, B = self.sysid()  # make sure we have an estimate first\n",
    "        \n",
    "        return A @ state + B @ control\n",
    "        \n",
    "    \n",
    "    def get_lqr(self):\n",
    "        A, B = self.sysid()  # make sure we have an estimate first\n",
    "            \n",
    "        # compute stabilizing controller for squared costs\n",
    "        Q = jnp.eye(self.state_dim)\n",
    "#         Q = jnp.zeros((self.state_dim, self.state_dim)).at[-1, -1].set(1.); print('solving DARE with constrained Q')\n",
    "        R = jnp.eye(self.control_dim) * 1e-8  # heuristic to weight state more\n",
    "        \n",
    "        X = scipy.linalg.solve_discrete_are(A, B, Q, R)  # solve the ricatti equation\n",
    "        K = jnp.linalg.inv(B.T @ X @ B + R) @ (B.T @ X @ A)  # compute LQR gain\n",
    "        return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147584c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lifter:\n",
    "    \"\"\"\n",
    "    Map the past `hh` costs and controls to lifted \"states\".\n",
    "    The given cost and control histories should be rightmost recent.\n",
    "    \"\"\"\n",
    "    @abstractmethod\n",
    "    def __init__(self,\n",
    "                 hh: int,\n",
    "                 control_dim: int,\n",
    "                 state_dim: int):\n",
    "        self.hh = hh\n",
    "        self.control_dim = control_dim\n",
    "        self.state_dim = state_dim\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def map(self,\n",
    "            cost_history: jnp.ndarray,\n",
    "            control_history: jnp.ndarray) -> jnp.ndarray:\n",
    "        \"\"\"\n",
    "        Maps a history of costs and controls to a \"state\" (which may just be the cost history or may be a \n",
    "        lifted state or anything).\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('{} not implemented'.format(inspect.stack()[0][3]))\n",
    "\n",
    "    @abstractmethod\n",
    "    def update(self,\n",
    "               prev_state: jnp.ndarray,\n",
    "               state: jnp.ndarray,\n",
    "               control: jnp.ndarray,\n",
    "               sysid: SysID) -> float:\n",
    "        \"\"\"\n",
    "        Called with the previous state, the control that was applied, the resulting state, and the current\n",
    "        estimate of system dynamics.\n",
    "        Can be used to update the lifting mechanism, or can be a no-op.\n",
    "        If updates, it can also return the loss.\n",
    "        \"\"\"\n",
    "        return None\n",
    "    \n",
    "    \n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "\n",
    "class NoLift(Lifter):\n",
    "    \"\"\"\n",
    "    lift to state that is simply history of costs\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 hh: int,\n",
    "                 control_dim: int):\n",
    "        super().__init__(hh, control_dim, hh)\n",
    "        pass\n",
    "    \n",
    "    def map(self,\n",
    "            cost_history: jnp.ndarray,\n",
    "            control_history: jnp.ndarray) -> jnp.ndarray:\n",
    "        \"\"\"\n",
    "        maps histories to lifted states. it's in pytorch rn, but that can change\n",
    "        \"\"\"\n",
    "        assert cost_history.shape == (self.hh,)\n",
    "        assert control_history.shape == (self.hh, self.control_dim)\n",
    "        \n",
    "        return cost_history\n",
    "    \n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "\n",
    "class RandomLift(Lifter):\n",
    "    \"\"\"\n",
    "    Uses a random init NN to transform the cost+control histories into a system that hopefully has linear dynamics.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 hh: int,\n",
    "                 control_dim: int,\n",
    "                 state_dim: int,\n",
    "                 depth: int):\n",
    "        super().__init__(hh, control_dim, state_dim)\n",
    "        \n",
    "        self.hh = hh\n",
    "        self.control_dim = control_dim\n",
    "        self.state_dim = state_dim\n",
    "        \n",
    "        # to compute lifted states which hopefully respond linearly to the controls\n",
    "        flat_dim = hh  # TODO could add control history as an input as well!\n",
    "        self.lift_model = MLP(layer_dims=_exponential_linspace_int(flat_dim, self.state_dim, depth), use_bias=False).train().float()\n",
    "        for p in self.lift_model.parameters(): p.data.uniform_(-0.1, 0.1)\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def map(self,\n",
    "            cost_history: jnp.ndarray,\n",
    "            control_history: jnp.ndarray) -> jnp.ndarray:\n",
    "        \"\"\"\n",
    "        maps histories to lifted states. it's in pytorch rn, but that can change\n",
    "        \"\"\"\n",
    "        assert cost_history.shape == (self.hh,)\n",
    "        assert control_history.shape == (self.hh, self.control_dim)\n",
    "        \n",
    "        # convert to pytorch tensors and back rq\n",
    "        with torch.no_grad():\n",
    "            cost_history, control_history = map(lambda j_arr: torch.from_numpy(np.array(j_arr)), [cost_history, control_history])\n",
    "            state = self.lift_model(cost_history.unsqueeze(0)).squeeze()\n",
    "        state = jnp.array(state.cpu().data)\n",
    "        \n",
    "        return state\n",
    "\n",
    "    def update(self,\n",
    "               prev_state: jnp.ndarray,\n",
    "               state: jnp.ndarray,\n",
    "               control: jnp.ndarray,\n",
    "               sysid: SysID):\n",
    "        pass\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "class LearnedLift(Lifter, SysID):\n",
    "    def __init__(self,\n",
    "                 hh: int,\n",
    "                 control_dim: int,\n",
    "                 state_dim: int,\n",
    "                 depth: int,\n",
    "                 scale: float=0.1,\n",
    "                 lift_lr: float=0.001,\n",
    "                 sysid_lr: float=0.001,\n",
    "                 learn_lift: bool=True,\n",
    "                 learn_sysid: bool=True):\n",
    "        \n",
    "        super().__init__(hh, control_dim, state_dim)\n",
    "        \n",
    "        self.hh = hh\n",
    "        self.control_dim = control_dim\n",
    "        self.state_dim = state_dim\n",
    "        self.scale = scale\n",
    "        self.learn_lift = learn_lift\n",
    "        self.learn_sysid = learn_sysid\n",
    "        \n",
    "        # for pytorch learning while being able to return jnp arrays\n",
    "        self.states = [torch.zeros(self.state_dim)]\n",
    "        \n",
    "        # to compute lifted states which hopefully respond linearly to the controls\n",
    "        flat_dim = hh  # TODO could add control history as an input as well!\n",
    "        self.lift_model = MLP(layer_dims=_exponential_linspace_int(flat_dim, self.state_dim, depth), use_bias=False).train().float()\n",
    "#         for p in self.lift_model.parameters(): p.data.uniform_(-0.01, 0.01)\n",
    "        self.lift_opt = torch.optim.Adam(self.lift_model.parameters(), lr=lift_lr)\n",
    "    \n",
    "        # to estimate linear dynamics of lifted states\n",
    "        self.A = torch.nn.Parameter(torch.randn((self.state_dim, self.state_dim), dtype=torch.float32))\n",
    "        self.B = torch.nn.Parameter(torch.randn((self.state_dim, self.control_dim), dtype=torch.float32))\n",
    "        self.sysid_opt = torch.optim.Adam([self.A, self.B], lr=sysid_lr)\n",
    "        \n",
    "        self.t = 1\n",
    "        pass\n",
    "    \n",
    "    def map(self,\n",
    "            cost_history: jnp.ndarray,\n",
    "            control_history: jnp.ndarray) -> jnp.ndarray:\n",
    "        \"\"\"\n",
    "        maps histories to lifted states. it's in pytorch rn, but that can change\n",
    "        \"\"\"\n",
    "        assert cost_history.shape == (self.hh,)\n",
    "        assert control_history.shape == (self.hh, self.control_dim)\n",
    "        \n",
    "        # convert to pytorch tensors and back rq\n",
    "        cost_history, control_history = map(lambda j_arr: torch.from_numpy(np.array(j_arr)), [cost_history, control_history])\n",
    "        state = self.lift_model(cost_history.unsqueeze(0)).squeeze()\n",
    "        self.states.append(state)\n",
    "        state = jnp.array(state.data.numpy())\n",
    "        \n",
    "        return state\n",
    "\n",
    "    def update(self,\n",
    "               prev_state: jnp.ndarray,\n",
    "               state: jnp.ndarray,\n",
    "               control: jnp.ndarray,\n",
    "               sysid: SysID) -> float:\n",
    "        assert jnp.allclose(state, jnp.array(self.state.data.numpy()))\n",
    "        assert len(self.states) >= 2\n",
    "        assert isinstance(self.states[-1], torch.Tensor) and self.states[-1].requires_grad\n",
    "        assert isinstance(self.states[-2], torch.Tensor) and self.states[-2].requires_grad\n",
    "        \n",
    "        if not isinstance(control, torch.Tensor): control = torch.Tensor(control).reshape(self.control_dim)\n",
    "        \n",
    "        prev_state, state = self.states[-2:]\n",
    "        pred = self.A @ prev_state + self.B @ control\n",
    "        diff = state - pred\n",
    "        \n",
    "        self.lift_opt.zero_grad()\n",
    "        self.sysid_opt.zero_grad()\n",
    "        loss = torch.mean(diff ** 2)\n",
    "        loss.backward()\n",
    "        if self.learn_lift: self.lift_opt.step()\n",
    "        if self.learn_sysid: self.sysid_opt.step()\n",
    "        return loss.item()\n",
    "    \n",
    "    def perturb_control(self,\n",
    "                        state: jnp.ndarray,\n",
    "                        control: jnp.ndarray=None):\n",
    "        assert state.shape == (self.state_dim,)\n",
    "        \n",
    "        if control is None:\n",
    "#             eps = 1 - 2 * np.random.randint(2, size=(self.control_dim,))  # random rademacher direction\n",
    "            eps = np.random.randn(self.control_dim)\n",
    "            control = self.scale * eps / self.t ** 0.25\n",
    "\n",
    "        self.t += 1\n",
    "        return control\n",
    "    \n",
    "    def sysid(self):\n",
    "        return jnp.array(self.A.data.numpy()), jnp.array(self.B.data.numpy())\n",
    "    \n",
    "    def dynamics(self,\n",
    "                 state: jnp.ndarray,\n",
    "                 control: jnp.ndarray):\n",
    "        assert state.shape == (self.state_dim,) and control.shape == (self.control_dim,)\n",
    "        A, B = self.sysid()\n",
    "        print(np.max(np.abs(np.linalg.eig(A)[0])))\n",
    "        \n",
    "        return A @ state + B @ control\n",
    "        \n",
    "    \n",
    "    def get_lqr(self):\n",
    "        A, B = self.sysid()\n",
    "            \n",
    "        # compute stabilizing controller for squared costs\n",
    "        Q = jnp.eye(self.state_dim)\n",
    "        R = jnp.eye(self.control_dim) * 1e-8  # heuristic to weight state more\n",
    "        \n",
    "        X = scipy.linalg.solve_discrete_are(A, B, Q, R)  # solve the ricatti equation\n",
    "        K = jnp.linalg.inv(B.T @ X @ B + R) @ (B.T @ X @ A)  # compute LQR gain\n",
    "        return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f9e828",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Controller:\n",
    "    def __init__(self,\n",
    "                 h: int,\n",
    "                 initial_u: jnp.ndarray,\n",
    "                 initial_scales: Tuple[float, float, float],\n",
    "                 lifter: Lifter,\n",
    "                 sysid: SysID,\n",
    "                 T0: int,\n",
    "                 bounds=None,\n",
    "                 method='FKM',\n",
    "                 K: jnp.ndarray=None,\n",
    "                 use_sigmoid=True,\n",
    "                 decay_scales=False):\n",
    "        \n",
    "        # check things make sense\n",
    "        assert lifter.state_dim == sysid.state_dim\n",
    "        assert lifter.control_dim == sysid.control_dim and initial_u.shape[0] == lifter.control_dim\n",
    "        self.control_dim = lifter.control_dim\n",
    "        self.state_dim = lifter.state_dim\n",
    "        assert method in ['FKM', 'REINFORCE']\n",
    "        assert all(map(lambda i: i >= 0, initial_scales))\n",
    "        if bounds is not None:\n",
    "            bounds = jnp.array(bounds).reshape(2, -1)\n",
    "            assert len(bounds[0]) == len(bounds[1]) and len(bounds[0]) == self.control_dim, 'improper bounds'\n",
    "            assert all(map(lambda i: bounds[0, i] < bounds[1, i], range(self.control_dim))), 'improper bounds'\n",
    "        if K is not None:\n",
    "            assert K.shape == (self.control_dim, self.state_dim)\n",
    "\n",
    "        # hyperparams\n",
    "        self.h = h\n",
    "        self.hh = lifter.hh\n",
    "        self.lifter = lifter\n",
    "        self.sysid = sysid\n",
    "        self.T0 = T0\n",
    "        self.method = method\n",
    "        self.bounds = bounds\n",
    "        self.decay_scales = decay_scales\n",
    "\n",
    "        # for rescaling u\n",
    "        self.rescale_u = lambda u: _rescale(u, self.bounds, use_sigmoid=use_sigmoid) if self.bounds is not None else u\n",
    "        self.inv_rescale_u = lambda ru: _inv_rescale(ru, self.bounds, use_sigmoid=use_sigmoid) if self.bounds is not None else ru\n",
    "        self.d_rescale_u = lambda u: _d_rescale(u, self.bounds, use_sigmoid=use_sigmoid) if self.bounds is not None else jnp.ones_like(u)        \n",
    "        \n",
    "        # dynamic parameters of the controller\n",
    "        self.M = jnp.zeros((self.h, self.control_dim, self.state_dim))\n",
    "        self.M0 = self.inv_rescale_u(initial_u)\n",
    "        self.K = K if K is not None else jnp.zeros((self.control_dim, self.state_dim)) # jnp.array(np.random.randn(self.control_dim, self.state_dim) / (self.control_dim * self.state_dim))\n",
    "        \n",
    "        self.M_scale, self.M0_scale, self.K_scale = initial_scales\n",
    "        self.prev_cost = 0.\n",
    "        self.prev_control = jnp.zeros(self.control_dim)\n",
    "        self.prev_state = jnp.zeros(self.state_dim)\n",
    "        self.cost_history = jnp.zeros(self.hh)   # histories are rightmost recent (increasing in time)\n",
    "        self.control_history = jnp.zeros((self.hh, self.control_dim))\n",
    "        self.state_history = jnp.zeros((self.h, self.state_dim))\n",
    "        self.disturbance_history = jnp.zeros((2 * self.h, self.state_dim))  # past 2h disturbances\n",
    "        self.t = 1\n",
    "\n",
    "        # grad estimation stuff -- `self.eps` should be divided by its variance!!\n",
    "        if self.method == 'FKM':\n",
    "            self.eps_M = jnp.zeros((self.h, self.h, self.control_dim, self.state_dim))  # noise history of M perturbations\n",
    "            self.eps_M0 = jnp.zeros((self.h, self.control_dim))  # noise history of M0 perturbations\n",
    "            self.eps_K = jnp.zeros((self.h, self.control_dim, self.state_dim))  # noise history of K perturbations\n",
    "            \n",
    "            def grad_M(diff):\n",
    "                return diff * jnp.sum(self.eps_M, axis=0) * self.control_dim * self.state_dim * self.h\n",
    "            def grad_M0(diff):\n",
    "                return diff * jnp.sum(self.eps_M0, axis=0) * self.control_dim * self.state_dim * self.h\n",
    "            def grad_K(diff):\n",
    "                return diff * jnp.sum(self.eps_K, axis=0) * self.control_dim * self.state_dim * self.h\n",
    "            \n",
    "        elif self.method == 'REINFORCE':\n",
    "            self.eps = jnp.zeros((self.h + 1, self.control_dim))  # noise history of u perturbations\n",
    "            \n",
    "            def grad_M(diff):\n",
    "#                 val = jnp.tensordot(self.eps[:self.h], self.disturbance_history[-self.h:], axes=(0, 0))\n",
    "                val = 0.\n",
    "                for i in range(self.h):\n",
    "                    val += self.eps[i].reshape(self.control_dim, 1) @ self.disturbance_history[-(self.h + i + 1): -(i + 1)].reshape(1, self.h * self.state_dim)\n",
    "                val = jnp.transpose(val.reshape(self.control_dim, self.h, self.state_dim), (1, 0, 2))\n",
    "                return diff * val * self.control_dim * self.state_dim * self.h\n",
    "            def grad_M0(diff):\n",
    "                return diff * self.eps[-1] * self.control_dim * self.state_dim * self.h\n",
    "            def grad_K(diff):\n",
    "                val = 0.\n",
    "                for i in range(self.h):\n",
    "                    val += self.eps[i].reshape(self.control_dim, 1) @ self.state_history[i].reshape(1, self.state_dim)\n",
    "                return diff * val * self.control_dim * self.state_dim * self.h\n",
    "#             raise NotImplementedError('im not sure if my REINFORCE is right, please check it :)')\n",
    "            \n",
    "        self.grads = deque([(jnp.zeros_like(self.M), jnp.zeros_like(self.M0), jnp.zeros_like(self.K))], maxlen=self.h)\n",
    "        self.grad_M = grad_M\n",
    "        self.grad_M0 = grad_M0\n",
    "        self.grad_K = grad_K\n",
    "        \n",
    "        self.M_update_rescaler = M_UPDATE_RESCALER()\n",
    "        self.M0_update_rescaler = M0_UPDATE_RESCALER()\n",
    "        self.K_update_rescaler = K_UPDATE_RESCALER()\n",
    "    \n",
    "        self.disturbance_rescaler = W_RESCALER()\n",
    "        \n",
    "        # stats\n",
    "        self.stats = Stats()\n",
    "        self.stats.register('rho(A)', float, plottable=True)  # operator norm (spectral radius) of A\n",
    "        if self.control_dim == 1:\n",
    "            if self.state_dim == 1:\n",
    "                self.stats.register('A', float, plottable=True)\n",
    "                self.stats.register('B', float, plottable=True)\n",
    "            self.stats.register('disturbances', float, plottable=True)\n",
    "            self.stats.register('K @ state', float, plottable=True)\n",
    "            self.stats.register('M \\cdot w', float, plottable=True)\n",
    "            self.stats.register('M0', float, plottable=True)\n",
    "        pass\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    def __call__(self, cost: float) -> jnp.ndarray:\n",
    "        \"\"\"\n",
    "        Returns the control based on current cost and internal parameters.\n",
    "        \"\"\"\n",
    "        \n",
    "        # observe next state and update histories\n",
    "        self.cost_history = _append(self.cost_history, cost)\n",
    "        self.control_history = _append(self.control_history, self.prev_control)\n",
    "        self.t += 1\n",
    "        state = self.lifter.map(self.cost_history, self.control_history)  # xhat_{t+1}\n",
    "        self.lifter.update(self.prev_state, state, self.prev_control, self.sysid)  # update lifter, if needed\n",
    "        self.state_history = _append(self.state_history, state)\n",
    "        \n",
    "        # explore for sysid, and then get stabilizing controller\n",
    "        if self.t < self.T0:\n",
    "            control = self.sysid.perturb_control(state)\n",
    "            self.prev_state = state\n",
    "            self.prev_control = control\n",
    "            return control\n",
    "        elif self.t == self.T0 and USE_K:  # get stabilizing controller\n",
    "            print('copying the K from {}'.format(self.sysid))\n",
    "            self.K = self.sysid.get_lqr()\n",
    "        \n",
    "        # compute disturbance\n",
    "        pred_state = self.sysid.dynamics(self.prev_state, self.prev_control)  # A @ xhat_t + B @ u_t\n",
    "        disturbance = state - pred_state  # xhat_{t+1} - (A @ xhat_t + B @ u_t)\n",
    "        disturbance = self.disturbance_rescaler.step(disturbance)\n",
    "        self.disturbance_history = _append(self.disturbance_history, disturbance)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
    "        \n",
    "        # compute change in cost, as well as the new scale\n",
    "        cost_diff = cost - self.prev_cost\n",
    "        M_scale, M0_scale, K_scale = map(lambda s: s / (self.t ** 0.25) if self.decay_scales else s, [self.M_scale, self.M0_scale, self.K_scale])\n",
    "\n",
    "        # update controller\n",
    "        d = self.d_rescale_u(self.prev_control)\n",
    "        grad_M = self.grad_M(cost_diff) * d\n",
    "        grad_M0 = self.grad_M0(cost_diff) * d\n",
    "        grad_K = self.grad_K(cost_diff) * d\n",
    "        self.grads.append((grad_M, grad_M0, grad_K))\n",
    "        if len(self.grads) == self.grads.maxlen:\n",
    "            grad_M, grad_M0, grad_K = self.grads[0]  # use update from h steps ago\n",
    "            self.M = self.M - self.M_update_rescaler.step(grad_M, iterate=self.M)\n",
    "            self.M0 = self.M0 - self.M0_update_rescaler.step(grad_M0, iterate=self.M0)\n",
    "            self.K = self.K - self.K_update_rescaler.step(grad_K, iterate=self.K)\n",
    "        \n",
    "        # compute newest perturbed control\n",
    "        M_tilde, M0_tilde, K_tilde = self.M, self.M0, self.K\n",
    "        \n",
    "        if self.method == 'FKM':  # perturb em all\n",
    "            eps_M = _generate_uniform((self.h, self.control_dim, self.state_dim))\n",
    "            eps_M0 = _generate_uniform(self.control_dim)\n",
    "            eps_K = _generate_uniform((self.control_dim, self.state_dim))\n",
    "            M_tilde = M_tilde + M_scale * eps_M\n",
    "            M0_tilde = M0_tilde + M0_scale * eps_M0\n",
    "            K_tilde = K_tilde + K_scale * eps_K\n",
    "            if M_scale > 0: self.eps_M = _append(self.eps_M, eps_M)\n",
    "            if M0_scale > 0: self.eps_M0 = _append(self.eps_M0, eps_M0)\n",
    "            if K_scale > 0: self.eps_K = _append(self.eps_K, eps_K)\n",
    "            \n",
    "        elif self.method == 'REINFORCE':  # perturb output only\n",
    "            eps = _generate_uniform(self.control_dim)\n",
    "            M0_tilde = M0_tilde + M0_scale * eps\n",
    "            if M0_scale > 0: self.eps = _append(self.eps, eps / M0_scale)\n",
    "            \n",
    "        control = -K_tilde @ state + M0_tilde + jnp.tensordot(M_tilde, self.disturbance_history[-self.h:], axes=([0, 2], [0, 1]))\n",
    "        control = self.rescale_u(control)\n",
    "#         control = self.sysid.perturb_control(state, control=control)  # perturb for sysid purposes\n",
    "\n",
    "        # cache it\n",
    "        self.prev_cost = cost\n",
    "        self.prev_state = state\n",
    "        self.prev_control = control \n",
    "        \n",
    "        # update stats\n",
    "        A, B = self.sysid.sysid()\n",
    "        self.stats.update('rho(A)', jnp.max(jnp.abs(jnp.linalg.eig(A))).item(), t=self.t)\n",
    "        self.stats.update('A', jnp.linalg.norm(A).item(), t=self.t)\n",
    "        self.stats.update('B', jnp.linalg.norm(B).item(), t=self.t)\n",
    "        if self.control_dim == 1:\n",
    "            self.stats.update('disturbances', disturbance.item(), t=self.t)\n",
    "            self.stats.update('K @ state', (-self.K @ state).item(), t=self.t)\n",
    "            self.stats.update('M \\cdot w', (jnp.tensordot(self.M, self.disturbance_history[-self.h:], axes=([0, 2], [0, 1]))).item(), t=self.t)\n",
    "            self.stats.update('M0', self.M0.item(), t=self.t)\n",
    "            \n",
    "        return control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe8603c",
   "metadata": {},
   "source": [
    "# LDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7503a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class System:\n",
    "    \"\"\"\n",
    "    LDS\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, control_dim):\n",
    "        self.state_dim = state_dim\n",
    "        self.control_dim = control_dim\n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "            self.A = torch.randn((state_dim, state_dim))\n",
    "            w, _ = torch.linalg.eig(self.A)\n",
    "            done = torch.max(torch.abs(w)) < 1 - 0.1\n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "            self.B = torch.randn((state_dim, control_dim))\n",
    "            w, _ = torch.linalg.eig(self.B)\n",
    "            done = torch.max(torch.abs(w)) < 1 - 0.1\n",
    "        pass\n",
    "        \n",
    "    def step(self, state, control):\n",
    "        if not isinstance(control, torch.Tensor):\n",
    "            control = torch.from_numpy(np.array(control))\n",
    "        if len(control.shape) > 1: control = control.squeeze(-1)\n",
    "        s = self.A @ state + self.B @ control\n",
    "        assert s.shape == (self.state_dim,), '{}  {}'.format(s.shape, self.state_dim)\n",
    "        return s\n",
    "    pass\n",
    "  \n",
    "def cost_fn(x, u):\n",
    "    if isinstance(x, float):\n",
    "        x = jnp.array(x)\n",
    "    elif isinstance(x, torch.Tensor):\n",
    "        x = jnp.array(x.detach().cpu().data)\n",
    "    cost = (x ** 2).sum()# + (u ** 2).sum()\n",
    "    return cost\n",
    "\n",
    "def disturbance(t, dim):\n",
    "    w = np.sin(4 * 2 * np.pi * t / T)\n",
    "#     w = t\n",
    "#     w = 1\n",
    "#     w = 0.1 * np.random.randn()\n",
    "#     w = 0\n",
    "    return w * np.ones(dim, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d248a984",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deluca.agents._gpc import GPC\n",
    "from deluca.agents._lqr import LQR\n",
    "from deluca.agents._bpc import BPC\n",
    "\n",
    "LIFT_LR = 0\n",
    "\n",
    "M_UPDATE_RESCALER = lambda : ADAM(0.00, betas=(0.9, 0.999))\n",
    "M0_UPDATE_RESCALER = lambda : ADAM(0.00, betas=(0.9, 0.999))\n",
    "K_UPDATE_RESCALER = lambda : ADAM(0.0, betas=(0.9, 0.999))\n",
    "\n",
    "W_RESCALER = lambda : IDENTITY()\n",
    "# W_RESCALER = lambda : EMA_RESCALE(beta=0.99)\n",
    "# W_RESCALER = lambda : ADAM(betas=(0.9, 0.9))\n",
    "\n",
    "T = 500\n",
    "T0 = 300\n",
    "USE_K = True\n",
    "\n",
    "du = 1  # control dim\n",
    "ds = 1\n",
    "h = 10  # controller memory length (# of w's to use on inference)\n",
    "hh = 20  # history length of the cost/control histories\n",
    "lift_dim = 40  # dimension to lift to\n",
    "\n",
    "sysid_args = {\n",
    "    'method': 'HAZAN',\n",
    "    'scale': 0.1,\n",
    "    'control_dim': du\n",
    "}\n",
    "\n",
    "controller_args = {\n",
    "    'h': h,\n",
    "    'method': 'FKM',\n",
    "    'initial_scales': (0.0, 0.0, 0),  # M, M0, K   (uses M0's scale for REINFORCE)\n",
    "    'T0': T0,\n",
    "    'bounds': None,\n",
    "    'initial_u': jnp.zeros(du),\n",
    "    'decay_scales': False,\n",
    "    'use_sigmoid': True\n",
    "}\n",
    "\n",
    "# torch.manual_seed(SEED)\n",
    "sys_init = System(state_dim=ds, control_dim=du)\n",
    "x_init = 1 * torch.randn(ds, dtype=torch.float32)  # initial state history\n",
    "\n",
    "# --------------- LQR ------------------------------------------------------\n",
    "\n",
    "Q = np.identity(ds)\n",
    "R = np.identity(du) * 1e-8\n",
    "\n",
    "sys = deepcopy(sys_init)\n",
    "controller = LQR(A=sys.A.numpy(), B=sys.B.numpy(), Q=Q, R=R)\n",
    "x = x_init.clone()\n",
    "\n",
    "xs_lqr = []\n",
    "costs_lqr = []\n",
    "us_lqr = []\n",
    "print('LQR')\n",
    "for t in tqdm.trange(T):\n",
    "    control = controller(x.numpy())\n",
    "    x = sys.step(x, control) + disturbance(t, ds) \n",
    "    \n",
    "    xs_lqr.append(x.numpy())\n",
    "    if du == 1: us_lqr.append(control.item())\n",
    "    costs_lqr.append(cost_fn(x, control).item())\n",
    "    \n",
    "# ----------------- GPC -----------------------------------------------------------\n",
    "\n",
    "sys = deepcopy(sys_init)\n",
    "controller = GPC(A=sys.A.numpy(), B=sys.B.numpy(), Q=Q, R=R, H=h, cost_fn=cost_fn, lr_scale=0.01, decay=False)\n",
    "x = x_init.clone()\n",
    "\n",
    "xs_gpc = []\n",
    "costs_gpc = []\n",
    "ws_gpc = []\n",
    "us_gpc = []\n",
    "grads_gpc = []\n",
    "u_decomp_gpc = []\n",
    "print('GPC')\n",
    "for t in tqdm.trange(T):\n",
    "    control = controller(x.numpy().reshape(-1, 1))\n",
    "    x = sys.step(x, control) + disturbance(t, ds) \n",
    "    \n",
    "    xs_gpc.append(x.numpy())\n",
    "    costs_gpc.append(cost_fn(x, control).item())\n",
    "    ws_gpc.append(jnp.linalg.norm(controller.noise_history[-1]))\n",
    "    if du == 1: us_gpc.append(control.item())\n",
    "    u_decomp_gpc.append((-controller.K @ controller.state,\n",
    "                        jnp.tensordot(controller.M, controller.last_h_noises(), axes=([0, 2], [0, 1])),\n",
    "                        jnp.zeros((du, 1))))\n",
    "    \n",
    "    \n",
    "# ---------------- BPC ------------------------------------------------------------\n",
    "\n",
    "sys = deepcopy(sys_init)\n",
    "controller = BPC(A=sys.A.numpy(), B=sys.B.numpy(), Q=Q, R=R, H=h, lr_scale=0.005, delta=0.01)\n",
    "x = x_init.clone()\n",
    "\n",
    "xs_bpc = []\n",
    "costs_bpc = []\n",
    "ws_bpc = []\n",
    "us_bpc = []\n",
    "grads_bpc = []\n",
    "u_decomp_bpc = []\n",
    "print('BPC')\n",
    "for t in tqdm.trange(T):\n",
    "    cost = cost_fn(x, control).item()\n",
    "    control = controller(x.numpy().reshape(-1, 1), cost)\n",
    "    x = sys.step(x, control) + disturbance(t, ds) \n",
    "    \n",
    "    xs_bpc.append(x.numpy())\n",
    "    costs_bpc.append(cost)\n",
    "    ws_bpc.append(jnp.linalg.norm(controller.noise_history[-1]))\n",
    "    if du == 1: us_bpc.append(control.item())\n",
    "    u_decomp_bpc.append((-controller.K @ controller.state,\n",
    "                    jnp.tensordot(controller.M, controller.noise_history, axes=([0, 2], [0, 1])),\n",
    "                    jnp.zeros((du, 1))))\n",
    "    \n",
    "# --------------- BPC NOLIFT ------------------------------------------------------\n",
    "\n",
    "lifter = NoLift(hh, du)\n",
    "sysid = SysID(state_dim=lifter.state_dim, **sysid_args)\n",
    "controller = Controller(lifter=lifter, sysid=sysid, **controller_args)\n",
    "sys = deepcopy(sys_init)\n",
    "x = x_init.clone()\n",
    "\n",
    "xs_bpc_nolift = []\n",
    "costs_bpc_nolift = []\n",
    "ws_bpc_nolift = []\n",
    "us_bpc_nolift = []\n",
    "grads_bpc_nolift = []\n",
    "u_decomp_bpc_nolift = []\n",
    "print('BPC NOLIFT')\n",
    "for t in tqdm.trange(T):\n",
    "    cost = cost_fn(x, control).item()\n",
    "    control = controller(cost)\n",
    "    x = sys.step(x, control) + disturbance(t, ds) \n",
    "    \n",
    "    xs_bpc_nolift.append(x.numpy())\n",
    "    costs_bpc_nolift.append(cost)\n",
    "    ws_bpc_nolift.append(jnp.linalg.norm(controller.disturbance_history[-1]))\n",
    "    if du == 1: us_bpc_nolift.append(control.item())\n",
    "    grads_bpc_nolift.append(jnp.linalg.norm(controller.grads[0][1]))\n",
    "    u_decomp_bpc_nolift.append((-controller.K @ controller.lifter.map(controller.cost_history, controller.control_history),\n",
    "                               jnp.tensordot(controller.M, controller.disturbance_history[-controller.h:], axes=([0, 2], [0, 1])),\n",
    "                               controller.M0))\n",
    "    \n",
    "# --------------- BPC LIFT ------------------------------------------------------\n",
    "\n",
    "lifter = RandomLift(hh, du, lift_dim, depth=2)\n",
    "sysid = SysID(state_dim=lifter.state_dim, **sysid_args)\n",
    "controller = Controller(lifter=lifter, sysid=sysid, **controller_args)\n",
    "sys = deepcopy(sys_init)\n",
    "x = x_init.clone()\n",
    "\n",
    "xs_bpc_lift = []\n",
    "costs_bpc_lift = []\n",
    "ws_bpc_lift = []\n",
    "us_bpc_lift = []\n",
    "grads_bpc_lift = []\n",
    "u_decomp_bpc_lift = []\n",
    "print('BPC LIFT')\n",
    "for t in tqdm.trange(T):\n",
    "    cost = cost_fn(x, control).item()\n",
    "    control = controller(cost)\n",
    "    x = sys.step(x, control) + disturbance(t, ds)\n",
    "    \n",
    "    xs_bpc_lift.append(x.numpy())\n",
    "    costs_bpc_lift.append(cost)\n",
    "    ws_bpc_lift.append(jnp.linalg.norm(controller.disturbance_history[-1]))\n",
    "    if du == 1: us_bpc_lift.append(control.item())\n",
    "    grads_bpc_lift.append(jnp.linalg.norm(controller.grads[0][1]))\n",
    "    u_decomp_bpc_lift.append((-controller.K @ controller.lifter.map(controller.cost_history, controller.control_history),\n",
    "                           jnp.tensordot(controller.M, controller.disturbance_history[-controller.h:], axes=([0, 2], [0, 1])),\n",
    "                           controller.M0))\n",
    "\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(5, 2, figsize=(16, 36))\n",
    "\n",
    "# plot lqr\n",
    "xs_lqr = np.array(xs_lqr).squeeze()\n",
    "ax[0, 0].plot(range(T), xs_lqr, label='lqr')\n",
    "ax[0, 1].plot(range(T), costs_lqr, label='lqr')\n",
    "ax[4, 0].plot(range(T), us_lqr, label='lqr')\n",
    "\n",
    "# plot gpc\n",
    "xs_gpc = np.array(xs_gpc).squeeze()\n",
    "ws_gpc = np.array(ws_gpc).squeeze()\n",
    "u_decomp_gpc = np.array(u_decomp_gpc).squeeze()\n",
    "ax[0, 0].plot(range(T), xs_gpc, label='gpc')\n",
    "ax[0, 1].plot(range(T), costs_gpc, label='gpc')\n",
    "ax[1, 0].plot(range(T), ws_gpc, label='gpc')\n",
    "# ax[1, 1].plot(range(T), grads_gpc, label='gpc')\n",
    "ax[2, 0].plot(range(T), u_decomp_gpc[:, 0], label='K @ state')\n",
    "ax[2, 0].plot(range(T), u_decomp_gpc[:, 1], label='M \\cdot w')\n",
    "ax[2, 0].plot(range(T), u_decomp_gpc[:, 2], label='M0')\n",
    "ax[4, 0].plot(range(T), us_gpc, label='gpc')\n",
    "\n",
    "# plot bpc\n",
    "xs_bpc = np.array(xs_bpc).squeeze()\n",
    "ws_bpc = np.array(ws_bpc).squeeze()\n",
    "u_decomp_bpc = np.array(u_decomp_bpc).squeeze()\n",
    "ax[0, 0].plot(range(T), xs_bpc, label='bpc')\n",
    "ax[0, 1].plot(range(T), costs_bpc, label='bpc')\n",
    "ax[1, 0].plot(range(T), ws_bpc, label='bpc')\n",
    "# ax[1, 1].plot(range(T), grads_bpc, label='bpc')\n",
    "ax[2, 1].plot(range(T), u_decomp_bpc[:, 0], label='K @ state')\n",
    "ax[2, 1].plot(range(T), u_decomp_bpc[:, 1], label='M \\cdot w')\n",
    "ax[2, 1].plot(range(T), u_decomp_bpc[:, 2], label='M0')\n",
    "ax[4, 0].plot(range(T), us_bpc, label='bpc')\n",
    "\n",
    "# plot bpc nolift\n",
    "xs_bpc_nolift = np.array(xs_bpc_nolift).squeeze()\n",
    "ws_bpc_nolift = np.array(ws_bpc_nolift).squeeze()\n",
    "u_decomp_bpc_nolift = np.array(u_decomp_bpc_nolift).squeeze()\n",
    "ax[0, 0].plot(range(T), xs_bpc_nolift, label='bpc nolift')\n",
    "ax[0, 1].plot(range(T), costs_bpc_nolift, label='bpc nolift')\n",
    "ax[1, 0].plot(range(T), ws_bpc_nolift, label='bpc nolift')\n",
    "ax[1, 1].plot(range(T), grads_bpc_nolift, label='bpc nolift')\n",
    "ax[3, 0].plot(range(T), u_decomp_bpc_nolift[:, 0], label='K @ state')\n",
    "ax[3, 0].plot(range(T), u_decomp_bpc_nolift[:, 1], label='M \\cdot w')\n",
    "ax[3, 0].plot(range(T), u_decomp_bpc_nolift[:, 2], label='M0')\n",
    "ax[4, 0].plot(range(T), us_bpc_nolift, label='bpc nolift')\n",
    "\n",
    "# plot bpc lift\n",
    "xs_bpc_lift = np.array(xs_bpc_lift).squeeze()\n",
    "ws_bpc_lift = np.array(ws_bpc_lift).squeeze()\n",
    "u_decomp_bpc_lift = np.array(u_decomp_bpc_lift).squeeze()\n",
    "ax[0, 0].plot(range(T), xs_bpc_lift, label='bpc lift')\n",
    "ax[0, 1].plot(range(T), costs_bpc_lift, label='bpc lift')\n",
    "ax[1, 0].plot(range(T), ws_bpc_lift, label='bpc lift')\n",
    "ax[1, 1].plot(range(T), grads_bpc_lift, label='bpc lift')\n",
    "ax[3, 1].plot(range(T), u_decomp_bpc_lift[:, 0], label='K @ state')\n",
    "ax[3, 1].plot(range(T), u_decomp_bpc_lift[:, 1], label='M \\cdot w')\n",
    "ax[3, 1].plot(range(T), u_decomp_bpc_lift[:, 2], label='M0')\n",
    "ax[4, 0].plot(range(T), us_bpc_lift, label='bpc lift')\n",
    "\n",
    "ax[0, 0].plot(range(T), [0. for _ in range(T)], label='opt')\n",
    "ax[0, 0].set_title('position'); ax[0, 0].legend(); ax[0, 0].scatter([0,], [x_init.item(),], marker=(5, 1));# ax[0, 0].set_ylim(-2, 2)\n",
    "ax[0, 1].set_title('cost'); ax[0, 1].legend(); #ax[0, 1].set_ylim(-2, 2)\n",
    "ax[1, 0].set_title('disturbances'); ax[1, 0].legend();# ax[1, 0].set_ylim(0, 2)\n",
    "ax[1, 1].set_title('M0 grads'); ax[1, 1].legend()\n",
    "ax[2, 0].set_title('K @ state, M \\cdot w, and M_0 for GPC'); ax[2, 0].legend()\n",
    "ax[2, 1].set_title('K @ state, M \\cdot w, and M_0 for BPC'); ax[2, 1].legend()\n",
    "ax[3, 0].set_title('K @ state, M \\cdot w, and M_0 for BPC NOLIFT'); ax[3, 0].legend(); #ax[3, 0].set_ylim(-2, 2)\n",
    "ax[3, 1].set_title('K @ state, M \\cdot w, and M_0 for BPC LIFT'); ax[3, 1].legend(); #ax[3, 1].set_ylim(-2, 2)\n",
    "ax[4, 0].set_title('controls'); ax[4, 0].legend(); #ax[4, 0].set_ylim(-2, 2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf68e50",
   "metadata": {},
   "source": [
    "# COCO BBOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d77ae41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dynamical_systems import COCO\n",
    "\n",
    "LIFT_LR = 0\n",
    "\n",
    "M_UPDATE_RESCALER = lambda : ADAM(0.00, betas=(0.9, 0.999))\n",
    "M0_UPDATE_RESCALER = lambda : ADAM(0.0, betas=(0.9, 0.999))\n",
    "K_UPDATE_RESCALER = lambda : ADAM(0.000, betas=(0.9, 0.999))\n",
    "\n",
    "W_RESCALER = lambda : IDENTITY()\n",
    "# W_RESCALER = lambda : EMA_RESCALE(beta=0.99)\n",
    "# W_RESCALER = lambda : ADAM(betas=(0.9, 0.9))\n",
    "\n",
    "T = 1000\n",
    "T0 = 500\n",
    "USE_K = True\n",
    "\n",
    "du = 1  # control dim\n",
    "ds = 1\n",
    "h = 5  # controller memory length (# of w's to use on inference)\n",
    "hh = 40  # history length of the cost/control histories\n",
    "lift_dim = 128  # dimension to lift to\n",
    "\n",
    "sysid_args = {\n",
    "    'method': 'REGRESSION',\n",
    "    'scale': 0.01,\n",
    "    'control_dim': du\n",
    "}\n",
    "\n",
    "controller_args = {\n",
    "    'h': h,\n",
    "    'method': 'FKM',\n",
    "    'initial_scales': (0.00, 0.00, 0.00),  # M, M0, K   (uses M0's scale for REINFORCE)\n",
    "    'T0': T0,\n",
    "    'bounds': (-1, 1),\n",
    "    'initial_u': jnp.zeros(du),\n",
    "    'decay_scales': False,\n",
    "    'use_sigmoid': True\n",
    "}\n",
    "\n",
    "method = 'NOLIFT'  # 'NOLIFT', 'LIFT'\n",
    "problem_number = 684# np.random.randint(2160) \n",
    "u_index = 0\n",
    "predict_differences = True\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "lifter = NoLift(hh, du) if method == 'NOLIFT' else LinearLift(hh, du, lift_dim, depth=2)\n",
    "sysid = SysID(state_dim=lifter.state_dim, **sysid_args)\n",
    "controller = Controller(lifter=lifter, sysid=sysid, **controller_args)\n",
    "sys = COCO(index=problem_number, u_index=u_index, predict_differences=predict_differences)\n",
    "cost = sys.interact(0.)\n",
    "\n",
    "xs = []\n",
    "costs = []\n",
    "ws = []\n",
    "grads = []\n",
    "u_decomp = []\n",
    "for t in tqdm.trange(T):\n",
    "    control = controller(cost)\n",
    "    cost = sys.interact(control)\n",
    "    \n",
    "    xs.append(sys.x[sys.u_index])\n",
    "    costs.append(cost)\n",
    "    ws.append(jnp.linalg.norm(controller.disturbance_history[-1]))\n",
    "    grads.append(jnp.linalg.norm(controller.grads[0][1]))\n",
    "    u_decomp.append((-controller.K @ controller.lifter.map(controller.cost_history, controller.control_history),\n",
    "                   jnp.tensordot(controller.M, controller.disturbance_history[-controller.h:], axes=([0, 2], [0, 1])),\n",
    "                   controller.M0))\n",
    "    \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(3, 2, figsize=(16, 10))\n",
    "u_decomp = np.array(u_decomp)\n",
    "\n",
    "# plot \n",
    "ax[0, 0].plot(range(T), xs, label=method)\n",
    "ax[0, 1].plot(range(T), costs, label=method)\n",
    "ax[1, 0].plot(range(T), ws, label=method)\n",
    "ax[1, 1].plot(range(T), grads, label=method)\n",
    "\n",
    "ax[0, 0].plot(range(T), [sys.stats['optimal_control']['value'] for _ in range(T)], label='opt')\n",
    "ax[0, 0].set_title('position'); ax[0, 0].legend()\n",
    "ax[0, 1].set_title('cost'); ax[0, 1].legend()\n",
    "ax[1, 0].set_title('disturbances'); ax[1, 0].legend()\n",
    "ax[1, 1].set_title('M0 grads'); ax[1, 1].legend()\n",
    "\n",
    "ax[2, 0].plot(range(T), u_decomp[:, 0], label='K @ state')\n",
    "ax[2, 0].plot(range(T), u_decomp[:, 1], label='M \\cdot w')\n",
    "ax[2, 0].plot(range(T), u_decomp[:, 2], label='M0')\n",
    "ax[2, 0].set_title('K @ state, M \\cdot w, and M_0'); ax[2, 0].legend()\n",
    "\n",
    "us, fs = sys.stats['gt_controls']['value'], sys.stats['gt_values']['value']\n",
    "ax[2, 1].plot(us, fs)\n",
    "ax[2, 1].set_title('objective')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "extravaganza",
   "language": "python",
   "name": "extravaganza"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
