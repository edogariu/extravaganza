{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debc61fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from extravaganza.sysid import SysID\n",
    "from extravaganza.lifters import Lifter\n",
    "from extravaganza.utils import set_seed, append, sample, jkey, rescale, d_rescale, inv_rescale, opnorm, exponential_linspace_int\n",
    "from extravaganza.models import MLP\n",
    "from typing import Tuple\n",
    "import torch\n",
    "import jax.numpy as jnp\n",
    "from collections import deque\n",
    "\n",
    "class LearnedLift(Lifter, SysID):\n",
    "    def __init__(self,\n",
    "                 hh: int,\n",
    "                 control_dim: int,\n",
    "                 state_dim: int,\n",
    "                 depth: int,\n",
    "                 scale: float = 0.1,\n",
    "                 lift_lr: float = 0.001,\n",
    "                 sysid_lr: float = 0.001,\n",
    "                 cost_lr: float = 0.001,\n",
    "                 buffer_maxlen: int = int(1e9),\n",
    "                 batch_size: int = 64,\n",
    "                 num_epochs: int = 20,  # number of epochs over the buffer to use when querying `sysid()` or `dynamics()` for first time\n",
    "                 seed: int = None):\n",
    "        \n",
    "        set_seed(seed)\n",
    "        super().__init__(hh, control_dim, state_dim, seed)\n",
    "        \n",
    "        self.hh = hh\n",
    "        self.control_dim = control_dim\n",
    "        self.state_dim = state_dim\n",
    "        self.scale = scale\n",
    "        \n",
    "        self.buffer = deque(maxlen=buffer_maxlen)\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # to compute lifted states which hopefully respond linearly to the controls\n",
    "        flat_dim = hh + control_dim * hh  # TODO could add control history as an input as well!\n",
    "#         self.lift_model = MLP(layer_dims=exponential_linspace_int(flat_dim, self.state_dim, depth), \n",
    "#                             #   normalization = lambda dim: torch.nn.LayerNorm(dim),\n",
    "#                               use_bias=False, \n",
    "#                               seed=seed).train().float()\n",
    "\n",
    "        assert flat_dim == self.state_dim\n",
    "        from INN.CouplingModels.RealNVP.linear import NonlinearRealNVP\n",
    "        self.lift_model = NonlinearRealNVP(flat_dim, k=50)\n",
    "        print('using invertible lifting model!')\n",
    "        \n",
    "        self.lift_opt = torch.optim.Adam(self.lift_model.parameters(), lr=lift_lr, weight_decay=0.0001)\n",
    "    \n",
    "        # to estimate linear dynamics of lifted states\n",
    "        self.A = torch.nn.Parameter(0.99 * torch.eye(self.state_dim, dtype=torch.float32))  # for stability purposes :)\n",
    "        self.B = torch.nn.Parameter(0.5 * torch.randn((self.state_dim, self.control_dim), dtype=torch.float32))\n",
    "        self.sysid_opt = torch.optim.Adam([self.A, self.B], lr=sysid_lr, weight_decay=0.0001)\n",
    "        \n",
    "#         # to learn \"inverse\" of lifing function\n",
    "#         depth = 3\n",
    "#         self.cost_model = MLP(layer_dims=exponential_linspace_int(self.state_dim + self.control_dim, 1, depth),\n",
    "#                               seed=seed, use_bias=True).train().float()\n",
    "#         print('using depth of {} for cost model'.format(depth))\n",
    "#         self.cost_opt = torch.optim.Adam(self.cost_model.parameters(), lr=cost_lr, weight_decay=0.0001)\n",
    "        \n",
    "        self.t = 1\n",
    "        self.trained = False\n",
    "        pass\n",
    "    \n",
    "    def get_cost(self, state, control):\n",
    "        if len(state.shape) == 1: state = state.unsqueeze(0)\n",
    "        histories = self.lift_model.inverse(state)\n",
    "        cost = histories[:, self.hh - 1]\n",
    "        return cost\n",
    "    \n",
    "    def forward(self, \n",
    "                cost_history: torch.Tensor,\n",
    "                control_history: torch.Tensor) -> torch.Tensor:  # so that we don't need to return a jnp.ndarray\n",
    "        inp = torch.cat((cost_history.reshape(-1, self.hh), control_history.reshape(-1, self.control_dim * self.hh)), dim=-1)\n",
    "#         state = self.lift_model(inp)\n",
    "\n",
    "        state = self.lift_model.forward(inp)[0]\n",
    "    \n",
    "        return state\n",
    "    \n",
    "    def map(self,\n",
    "            cost_history: jnp.ndarray,\n",
    "            control_history: jnp.ndarray) -> jnp.ndarray:\n",
    "        \"\"\"\n",
    "        maps histories to lifted states. it's in pytorch rn, but that can change\n",
    "        \"\"\"\n",
    "        assert cost_history.shape == (self.hh,)\n",
    "        assert control_history.shape == (self.hh, self.control_dim)\n",
    "        \n",
    "        # convert to pytorch tensors and back rq  # TODO remove this eventually, making everything in jax\n",
    "        with torch.no_grad():\n",
    "            cost_history, control_history = map(lambda j_arr: torch.from_numpy(np.array(j_arr)).unsqueeze(0), [cost_history, control_history])\n",
    "            state = jnp.array(self.forward(cost_history, control_history).squeeze(0).data.numpy())\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def perturb_control(self,\n",
    "                        state: jnp.ndarray,\n",
    "                        control: jnp.ndarray = None):\n",
    "        assert state.shape == (self.state_dim,)\n",
    "        eps = sample(jkey(), (self.control_dim,))  # random direction\n",
    "        control = self.scale * eps\n",
    "        self.t += 1\n",
    "        return control\n",
    "    \n",
    "    def train(self):\n",
    "            \n",
    "        # prepare dataloader\n",
    "        from torch.utils.data import DataLoader, TensorDataset\n",
    "        controls = []\n",
    "        prev_cost_history = []\n",
    "        prev_control_history = []\n",
    "        cost_history = []\n",
    "        control_history = []\n",
    "        for prev_histories, histories in self.buffer:  # append em all\n",
    "            lists = [controls, prev_cost_history, prev_control_history, cost_history, control_history]\n",
    "            vals = [histories[1][-1], *prev_histories, *histories]\n",
    "            for l, v in zip(lists, vals): l.append(torch.from_numpy(np.array(v)))\n",
    "        dataset = TensorDataset(*map(lambda l: torch.stack(l, dim=0), \n",
    "                                     [prev_cost_history, prev_control_history, cost_history, control_history, controls]))\n",
    "        dl = DataLoader(dataset, batch_size=self.batch_size, shuffle=True, drop_last=True)\n",
    "        \n",
    "        losses = []\n",
    "        print('training!')\n",
    "        for t in range(self.num_epochs):\n",
    "            for prev_cost_history, prev_control_history, cost_history, control_history, controls in dl:\n",
    "                \n",
    "                # compute disturbance\n",
    "                prev_state = self.forward(prev_cost_history, prev_control_history)\n",
    "                state = self.forward(cost_history, control_history)\n",
    "                pred = self.A.expand(self.batch_size, self.state_dim, self.state_dim) @ prev_state.unsqueeze(-1) + \\\n",
    "                       self.B.expand(self.batch_size, self.state_dim, self.control_dim) @ controls.unsqueeze(-1)\n",
    "                diff = state - pred.squeeze(-1)\n",
    "                \n",
    "                # update\n",
    "                self.lift_opt.zero_grad()\n",
    "                self.sysid_opt.zero_grad()\n",
    "                \n",
    "                # compute loss \n",
    "                LAMBDA_STATE_NORM, LAMBDA_STABILITY, LAMBDA_B_NORM = 1e-5, 0, 0\n",
    "                state_norm = (1 / (torch.norm(state) + 1e-8))\n",
    "                stability = opnorm(self.A - self.B @ dare_gain(self.A, self.B, torch.eye(self.state_dim), torch.eye(self.control_dim))) if LAMBDA_STABILITY > 0 else 0.\n",
    "                B_norm = 1 / (torch.norm(self.B) + 1e-8)\n",
    "                loss = torch.mean(diff ** 2) + LAMBDA_STATE_NORM * state_norm + LAMBDA_STABILITY * stability + LAMBDA_B_NORM * B_norm\n",
    "                loss.backward()\n",
    "                self.lift_opt.step()\n",
    "                self.sysid_opt.step()\n",
    "                \n",
    "                losses.append(loss.item())\n",
    "                \n",
    "            print_every = 25\n",
    "            if t % print_every == 0 or t == self.num_epochs - 1: print('mean loss for past {} epochs was {}'.format(print_every, np.mean(losses[-print_every:])))\n",
    "        \n",
    "        cost_losses = []\n",
    "        for t in range(self.num_epochs):\n",
    "            for prev_cost_history, prev_control_history, cost_history, control_history, controls in dl:\n",
    "#                 self.cost_opt.zero_grad()\n",
    "                state = self.forward(prev_cost_history, prev_control_history).reshape(self.batch_size, self.state_dim)\n",
    "#                 control = control_history[:, -1].reshape(self.batch_size, self.control_dim).detach()\n",
    "#                 inp = torch.cat((state, control), dim=-1)\n",
    "#                 fhat = self.cost_model(inp)  # predict cost from state and control we played from that state\n",
    "                fhat = self.get_cost(state, None)\n",
    "                f = cost_history[:, -1]\n",
    "                loss = torch.nn.functional.mse_loss(fhat.squeeze(), f.squeeze())\n",
    "#                 loss.backward()\n",
    "#                 self.cost_opt.step()\n",
    "                cost_losses.append(loss.item())\n",
    "            print_every = self.num_epochs\n",
    "            if t % print_every == 0 or t == self.num_epochs - 1: \n",
    "                print('mean cost loss for past {} epochs was {}'.format(print_every, np.mean(cost_losses[-print_every:])))\n",
    "#                 print(loss.item(), fhat, f)\n",
    "\n",
    "        self.trained = True\n",
    "        return losses\n",
    "    \n",
    "    def sysid(self):\n",
    "        if not self.trained:\n",
    "            self.losses = self.train()\n",
    "        \n",
    "        return jnp.array(self.A.data.numpy()), jnp.array(self.B.data.numpy())\n",
    "    \n",
    "    def update(self, \n",
    "               prev_histories: Tuple[jnp.ndarray, jnp.ndarray], \n",
    "               histories: Tuple[jnp.ndarray, jnp.ndarray]) -> float:\n",
    "        self.buffer.append((prev_histories, histories))  # add the transition\n",
    "        return 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba4c8452",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'jnp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mextravaganza\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Stats\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tuple\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLiftedBPC\u001b[39;00m(Controller):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m      8\u001b[0m                  h: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m      9\u001b[0m                  initial_u: jnp\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m                  seed: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     23\u001b[0m                  stats: Stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     25\u001b[0m         set_seed(seed)  \u001b[38;5;66;03m# for reproducibility\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m, in \u001b[0;36mLiftedBPC\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLiftedBPC\u001b[39;00m(Controller):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m      8\u001b[0m                  h: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m----> 9\u001b[0m                  initial_u: \u001b[43mjnp\u001b[49m\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[1;32m     10\u001b[0m                  rescalers,\n\u001b[1;32m     11\u001b[0m                  initial_scales: Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m],\n\u001b[1;32m     12\u001b[0m                  T0: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m     13\u001b[0m                  bounds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m                  method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mREINFORCE\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     15\u001b[0m                  lifter: Lifter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     16\u001b[0m                  sysid: SysID \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     17\u001b[0m                  K: jnp\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     18\u001b[0m                  step_every: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     19\u001b[0m                  use_sigmoid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     20\u001b[0m                  decay_scales \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     21\u001b[0m                  use_K_from_sysid: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     22\u001b[0m                  seed: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     23\u001b[0m                  stats: Stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     25\u001b[0m         set_seed(seed)  \u001b[38;5;66;03m# for reproducibility\u001b[39;00m\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;66;03m# check things make sense\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'jnp' is not defined"
     ]
    }
   ],
   "source": [
    "from extravaganza.controllers import Controller\n",
    "from extravaganza.stats import Stats\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "class LiftedBPC(Controller):\n",
    "    def __init__(self,\n",
    "                 h: int,\n",
    "                 initial_u: jnp.ndarray,\n",
    "                 rescalers,\n",
    "                 initial_scales: Tuple[float, float, float],\n",
    "                 T0: int,\n",
    "                 bounds = None,\n",
    "                 method = 'REINFORCE',\n",
    "                 lifter: Lifter = None,\n",
    "                 sysid: SysID = None,\n",
    "                 K: jnp.ndarray = None,\n",
    "                 step_every: int = 1,\n",
    "                 use_sigmoid = True,\n",
    "                 decay_scales = False,\n",
    "                 use_K_from_sysid: bool = False,\n",
    "                 seed: int = None,\n",
    "                 stats: Stats = None):\n",
    "\n",
    "        set_seed(seed)  # for reproducibility\n",
    "        \n",
    "        # check things make sense\n",
    "        if lifter is None and isinstance(sysid, Lifter): lifter = sysid\n",
    "        if sysid is None and isinstance(lifter, SysID): sysid = lifter\n",
    "        assert lifter.state_dim == sysid.state_dim\n",
    "        assert lifter.control_dim == sysid.control_dim and initial_u.shape[0] == lifter.control_dim\n",
    "        self.control_dim = lifter.control_dim\n",
    "        self.state_dim = lifter.state_dim\n",
    "        assert method in ['FKM', 'REINFORCE', 'ROLLOUT']\n",
    "        assert all(map(lambda i: i >= 0, initial_scales))\n",
    "        if bounds is not None:\n",
    "            bounds = jnp.array(bounds).reshape(2, -1)\n",
    "            assert len(bounds[0]) == len(bounds[1]) and len(bounds[0]) == self.control_dim, 'improper bounds'\n",
    "            assert all(map(lambda i: bounds[0, i] < bounds[1, i], range(self.control_dim))), 'improper bounds'\n",
    "        if K is not None:\n",
    "            assert K.shape == (self.control_dim, self.state_dim)\n",
    "        assert step_every < h, 'need to update at least every `h` steps'\n",
    "        \n",
    "        # hyperparams\n",
    "        self.h = h\n",
    "        self.hh = lifter.hh\n",
    "        self.lifter = lifter\n",
    "        self.sysid = sysid\n",
    "        self.T0 = T0\n",
    "        self.method = method\n",
    "        self.bounds = bounds\n",
    "        self.decay_scales = decay_scales\n",
    "        self.use_K_from_sysid = use_K_from_sysid\n",
    "        self.initial_control = initial_u\n",
    "        self.step_every = step_every\n",
    "\n",
    "        # for rescaling u\n",
    "        self.rescale_u = lambda u: rescale(u, self.bounds, use_sigmoid=use_sigmoid) if self.bounds is not None else u\n",
    "        self.inv_rescale_u = lambda ru: inv_rescale(ru, self.bounds, use_sigmoid=use_sigmoid) if self.bounds is not None else ru\n",
    "        self.d_rescale_u = lambda u: d_rescale(u, self.bounds, use_sigmoid=use_sigmoid) if self.bounds is not None else jnp.ones_like(u)        \n",
    "        \n",
    "        # controller params\n",
    "        self.M = jnp.zeros((self.h, self.control_dim, self.state_dim))\n",
    "        self.M0 = self.inv_rescale_u(initial_u)\n",
    "        self.K = K if K is not None else jnp.zeros((self.control_dim, self.state_dim)) # jax.random.normal(self.jkey(), shape=(self.control_dim, self.state_dim)) / (self.control_dim * self.state_dim)\n",
    "        self.M_scale, self.M0_scale, self.K_scale = initial_scales\n",
    "        \n",
    "        # histories are rightmost recent (increasing in time)\n",
    "        self.prev_cost = 0.\n",
    "        self.prev_control = jnp.zeros(self.control_dim)\n",
    "        self.prev_state = jnp.zeros(self.state_dim)\n",
    "        self.disturbance_history = jnp.zeros((2 * self.h, self.state_dim))  # past 2h disturbances, for controller\n",
    "        self.cost_history = jnp.zeros(self.hh)  # for sysid/lifting\n",
    "        self.control_history = jnp.zeros((self.hh, self.control_dim))  # for sysid/lifting\n",
    "        self.t = 1\n",
    "\n",
    "        # grad estimation stuff -- NOTE maybe `self.eps` should be divided by its variance?\n",
    "        if self.method == 'FKM':\n",
    "            self.eps_M = jnp.zeros((self.h, self.h, self.control_dim, self.state_dim))  # noise history of M perturbations\n",
    "            self.eps_M0 = jnp.zeros((self.h, self.control_dim))  # noise history of M0 perturbations\n",
    "            self.eps_K = jnp.zeros((self.h, self.control_dim, self.state_dim))  # noise history of K perturbations\n",
    "            \n",
    "            def grad_fn(f, d: float = 1):\n",
    "                grad_M = d.reshape(1, -1, 1) * f * jnp.sum(self.eps_M, axis=0) #* self.control_dim * self.state_dim * self.h\n",
    "                grad_M0 = d.reshape(-1) * f * jnp.sum(self.eps_M0, axis=0) #* self.control_dim * self.state_dim * self.h\n",
    "                grad_K = f * jnp.sum(self.eps_K, axis=0) #* self.control_dim * self.state_dim * self.h\n",
    "                return grad_M, grad_M0, grad_K\n",
    "            \n",
    "        elif self.method == 'REINFORCE':\n",
    "            self.eps = jnp.zeros((self.h + 1, self.control_dim))  # noise history of u perturbations\n",
    "            \n",
    "            def grad_fn(f, d: float = 1):\n",
    "                val = sum([jnp.transpose(jnp.einsum('ij,k->ijk', self.disturbance_history[i: self.h + i], self.eps[i]), axes=(0, 2, 1)) for i in range(self.h)])\n",
    "                grad_M = d.reshape(1, -1, 1) * f * val #* self.control_dim * self.state_dim * self.h\n",
    "                grad_M0 = d.reshape(-1) * f * self.eps[-1] #* self.control_dim * self.state_dim * self.h\n",
    "                val = self.eps[-1].reshape(self.control_dim, 1) @ self.prev_state.reshape(1, self.state_dim)\n",
    "                grad_K = f * val #* self.control_dim * self.state_dim * self.h\n",
    "                return grad_M, grad_M0, grad_K\n",
    "            \n",
    "        elif self.method == 'ROLLOUT':\n",
    "            if not isinstance(self.lifter, LearnedLift): raise Exception('{} can only be used with learned lifters'.format(self.method))\n",
    "            \n",
    "            def grad_fn(f, d: float = 1):\n",
    "                state = torch.from_numpy(np.array(self.initial_state))\n",
    "                M, M0, K = map(lambda arr: torch.tensor(np.array(arr), requires_grad=True), [self.M, self.M0, self.K])\n",
    "                act = lambda state, ws: self.rescale_u(self.inv_rescale_u(-K @ state) + M0 + torch.tensordot(M, ws, dims=([0, 2], [0, 1]))).reshape(self.control_dim)\n",
    "                W = torch.from_numpy(np.array(self.disturbance_history))\n",
    "                rollout_len = 5\n",
    "                for t in range(rollout_len):\n",
    "                    control = act(state, W[t: t + self.h])\n",
    "                    state = (self.lifter.A @ state + self.lifter.B @ control + W[t + self.h]).reshape(self.state_dim)\n",
    "#                 inp = torch.cat((state, act(state, W[rollout_len: rollout_len + self.h])), dim=0).unsqueeze(0)\n",
    "                cost = self.lifter.get_cost(state, None).squeeze()\n",
    "                cost.backward()\n",
    "                grad_M = d.reshape(1, -1, 1) * M.grad.data.detach().cpu().numpy()\n",
    "                grad_M0 = d.reshape(-1) * M0.grad.data.detach().cpu().numpy()\n",
    "                grad_K = K.grad.data.detach().cpu().numpy()\n",
    "                return grad_M, grad_M0, grad_K                    \n",
    "            \n",
    "        self.grads = deque([(jnp.zeros_like(self.M), jnp.zeros_like(self.M0), jnp.zeros_like(self.K))], maxlen=self.h)\n",
    "        self.grad_fn = grad_fn\n",
    "        \n",
    "        self.M_update_rescaler = rescalers[0]()\n",
    "        self.M0_update_rescaler = rescalers[1]()\n",
    "        self.K_update_rescaler = rescalers[2]()\n",
    "        \n",
    "        # stats\n",
    "        if stats is None:\n",
    "            print('WARNING: no `Stats` object provided, so the controller will make a new one.')\n",
    "            stats = Stats()\n",
    "        self.stats = stats\n",
    "        self.stats.register('||A-BK||_op', float, plottable=True)\n",
    "        self.stats.register('||A||_op', float, plottable=True)\n",
    "        self.stats.register('||B||_F', float, plottable=True)\n",
    "        self.stats.register('disturbances', float, plottable=True)\n",
    "        self.stats.register('lifter losses', float, plottable=True)\n",
    "        if self.control_dim == 1:\n",
    "            self.stats.register('K @ state', float, plottable=True)\n",
    "            self.stats.register('M \\cdot w', float, plottable=True)\n",
    "            self.stats.register('M0', float, plottable=True)\n",
    "        pass\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    def __call__(self, cost: float) -> jnp.ndarray:\n",
    "        \"\"\"\n",
    "        Returns the control based on current cost and internal parameters.\n",
    "        \"\"\"\n",
    "        # 1. observe next state and update histories\n",
    "        prev_histories = (self.cost_history, self.control_history)\n",
    "        self.cost_history = append(self.cost_history, cost)\n",
    "        self.control_history = append(self.control_history, self.prev_control)\n",
    "        self.t += 1\n",
    "        histories = (self.cost_history, self.control_history)\n",
    "        state = self.lifter.map(*histories)  # xhat_{t+1}\n",
    "        if self.t < self.T0: \n",
    "            lifter_loss = self.lifter.update(prev_histories, histories)  # update lifter, if needed\n",
    "        else:\n",
    "            if self.t == self.T0: \n",
    "                print('WARNING: note that we are only updating lifter during sysid phase')\n",
    "                self.initial_state = state.copy()\n",
    "            lifter_loss = 0.\n",
    "        \n",
    "        # 2. explore for sysid, and then get stabilizing controller\n",
    "        if self.t < self.T0:\n",
    "            control = self.sysid.perturb_control(state)\n",
    "            if self.bounds is not None: control = control.clip(*self.bounds)\n",
    "            self.prev_state = state\n",
    "            self.prev_control = control\n",
    "            return control\n",
    "        elif self.use_K_from_sysid and self.t == self.T0:  # get the K from sysid every so often\n",
    "            print('copying the K from {}'.format(self.sysid))\n",
    "            self.K = self.sysid.get_lqr()\n",
    "            pass\n",
    "    \n",
    "        # 3. compute disturbance\n",
    "        pred_state = self.sysid.dynamics(self.prev_state, self.prev_control)  # A @ xhat_t + B @ u_t\n",
    "        disturbance = state - pred_state  # xhat_{t+1} - (A @ xhat_t + B @ u_t)\n",
    "        self.disturbance_history = append(self.disturbance_history, disturbance)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
    "        \n",
    "        # compute change in cost, as well as the new scale\n",
    "        cost_diff = cost - self.prev_cost\n",
    "        M_scale, M0_scale, K_scale = map(lambda s: s / (self.t ** 0.25) if self.decay_scales else s, [self.M_scale, self.M0_scale, self.K_scale])\n",
    "\n",
    "        # 4. update controller\n",
    "        d = self.d_rescale_u(self.prev_control)\n",
    "        self.grads.append(self.grad_fn(cost_diff, d))\n",
    "        if len(self.grads) == self.grads.maxlen and self.t % self.step_every == 0:\n",
    "            grads = list(self.grads)[:self.step_every] if self.method != 'ROLLOUT' else list(self.grads)[-1:]  # use updates starting from h steps ago\n",
    "            self.M = self.M - self.M_update_rescaler.step(sum([g[0] for g in grads]), iterate=self.M)\n",
    "            self.M0 = self.M0 - self.M0_update_rescaler.step(sum([g[1] for g in grads]), iterate=self.M0)\n",
    "            self.K = self.K - self.K_update_rescaler.step(sum([g[2] for g in grads]), iterate=self.K)\n",
    "        \n",
    "        # 5. compute newest perturbed control\n",
    "        M_tilde, M0_tilde, K_tilde = self.M, self.M0, self.K\n",
    "\n",
    "        if self.method == 'FKM':  # perturb em all\n",
    "            eps_M = sample(jkey(), (self.h, self.control_dim, self.state_dim))\n",
    "            eps_M0 = sample(jkey(), (self.control_dim,))\n",
    "            eps_K = sample(jkey(), (self.control_dim, self.state_dim))\n",
    "            M_tilde = M_tilde + M_scale * eps_M\n",
    "            M0_tilde = M0_tilde + M0_scale * eps_M0\n",
    "            K_tilde = K_tilde + K_scale * eps_K\n",
    "            if M_scale > 0: self.eps_M = append(self.eps_M, eps_M)\n",
    "            if M0_scale > 0: self.eps_M0 = append(self.eps_M0, eps_M0)\n",
    "            if K_scale > 0: self.eps_K = append(self.eps_K, eps_K)\n",
    "            \n",
    "        elif self.method == 'REINFORCE':  # perturb output only\n",
    "            eps = sample(jkey(), (self.control_dim,))\n",
    "            M0_tilde = M0_tilde + M0_scale * eps\n",
    "            if M0_scale > 0: self.eps = append(self.eps, eps / M0_scale)\n",
    "        \n",
    "        elif self.method == 'ROLLOUT':  # don't perturb at all\n",
    "            pass\n",
    "            \n",
    "        # TODO this might not be the right thing to do with `K` when rescaling!!!!\n",
    "        control = self.inv_rescale_u(-K_tilde @ state) + M0_tilde + jnp.tensordot(M_tilde, self.disturbance_history[-self.h:], axes=([0, 2], [0, 1]))        \n",
    "        control = self.rescale_u(control)\n",
    "#         control = self.sysid.perturb_control(state, control=control)  # perturb for sysid purposes later than T0?\n",
    "\n",
    "        # cache it\n",
    "        self.prev_cost = cost\n",
    "        self.prev_state = state\n",
    "        self.prev_control = control \n",
    "            \n",
    "        # update stats\n",
    "        A, B = self.sysid.sysid()\n",
    "        self.stats.update('||A-BK||_op', opnorm(A - B @ self.K), t=self.t)\n",
    "        self.stats.update('||A||_op', opnorm(A), t=self.t)\n",
    "        self.stats.update('||B||_F', jnp.linalg.norm(B, 'fro').item(), t=self.t)\n",
    "        self.stats.update('disturbances', jnp.linalg.norm(disturbance).item(), t=self.t)\n",
    "        self.stats.update('lifter losses', lifter_loss, t=self.t)\n",
    "        if self.control_dim == 1:\n",
    "            self.stats.update('K @ state', (-self.K @ state).item(), t=self.t)\n",
    "            self.stats.update('M \\cdot w', (jnp.tensordot(self.M, self.disturbance_history[-self.h:], axes=([0, 2], [0, 1]))).item(), t=self.t)\n",
    "            self.stats.update('M0', self.M0.item(), t=self.t)\n",
    "            \n",
    "        return control\n",
    "    \n",
    "    def get_control(self, cost: float, state: jnp.ndarray) -> jnp.ndarray:\n",
    "        return self(cost)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "extravaganza",
   "language": "python",
   "name": "extravaganza"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
