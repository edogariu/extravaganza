{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca1b0fb6",
   "metadata": {},
   "source": [
    "# Neural Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29db997e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created a temporary directory at /var/folders/5m/0xr906c130vdqvkm3g21n6wr0000gn/T/tmpqwm15xbz\n",
      "INFO: Writing /var/folders/5m/0xr906c130vdqvkm3g21n6wr0000gn/T/tmpqwm15xbz/_remote_module_non_scriptable.py\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s: %(message)s', level=logging.INFO)  # set level to INFO for wordy\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import torch.optim as optim\n",
    "\n",
    "from extravaganza.dynamical_systems import LinearRegression, MNIST\n",
    "\n",
    "from extravaganza.observables import TimeDelayedObservation, FullObservation, Trajectory\n",
    "from extravaganza.sysid import Lifter, LiftedController, OfflineSysid\n",
    "from extravaganza.controllers import LQR, HINF, BPC, GPC, RBPC, EvanBPC, ConstantController\n",
    "from extravaganza.rescalers import ADAM, D_ADAM, DoWG, FIXED_RESCALE\n",
    "from extravaganza.stats import Stats\n",
    "from extravaganza.utils import ylim, render, append, opnorm, dare_gain, least_squares\n",
    "from extravaganza.experiments import Experiment\n",
    "\n",
    "# seeds for randomness. setting to `None` uses random seeds\n",
    "SYSTEM_SEED = 9  # need a seed so that we generate the same linreg dataset each time\n",
    "CONTROLLER_SEED = None\n",
    "SYSID_SEED = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f06917",
   "metadata": {},
   "source": [
    "## System\n",
    "Here, we tune the parameters of a gradient descent algorithm training a neural network. We can train either a linear regression or an MLP or CNN MNIST model. Any optimizer can be used, such as `SGD` or `Adam`, and any parameter can be tuned, such as `lr` or `momentum`. \n",
    "\n",
    "Which optimizer to use is specified in the `make_optimizer` argument, and how to update and tune things is specified in the `apply_control` argument.\n",
    "\n",
    "At the moment, we apply a 2-dimensional control $u = (u_0, u_1)$ that dictates 2 parameters of the learning rate schedule, given as\n",
    "$$\\eta_t := \\frac{u_0}{1 + u_1 \\cdot t},$$\n",
    "where $u_0$ is the initial learning rate and $u_1$ is a decay rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c16dbd",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ea3889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'nn'\n",
    "filename = '../logs/{}.pkl'.format(name)\n",
    "\n",
    "def get_experiment_args():\n",
    "    # --------------------------------------------------------------------------------------\n",
    "    # ------------------------    EXPERIMENT HYPERPARAMETERS    ----------------------------\n",
    "    # --------------------------------------------------------------------------------------\n",
    "\n",
    "    num_trials = 1\n",
    "    T = 10000  # total timesteps\n",
    "    T0 = 4000  # number of timesteps to just sysid for our methods\n",
    "    reset_condition = lambda t: t % 1000 == 0  # when to reset the system (which means fresh LR/MNIST model params)\n",
    "    use_multiprocessing = False  # unsure if this works in jupyter notebooks\n",
    "    render_every = None\n",
    "\n",
    "    # --------------------------------------------------------------------------------------\n",
    "    # --------------------------    SYSTEM HYPERPARAMETERS    ------------------------------\n",
    "    # --------------------------------------------------------------------------------------\n",
    "\n",
    "    initial_lr = 0.1\n",
    "    initial_decay = 0.2\n",
    "    initial_control = jnp.array([initial_lr, initial_decay])\n",
    "    du = initial_control.shape[0]\n",
    "    \n",
    "    make_optimizer = lambda model: optim.SGD(model.parameters(), lr=initial_lr)\n",
    "    def apply_control(control, system): system.opt.param_groups[0]['lr'] = control[0].item() / (1 + max(0., control[1].item()) * system.episode_t)\n",
    "\n",
    "#     make_system = lambda : LinearRegression(make_optimizer, apply_control,\n",
    "#                                             dataset = 'generated', \n",
    "#                                             repeat = 3,\n",
    "#                                             eval_every = 1, seed=SYSTEM_SEED)   \n",
    "\n",
    "    make_system = lambda : MNIST(make_optimizer, apply_control,\n",
    "                                 model_type = 'MLP', batch_size = 64,\n",
    "                                 repeat = 5,\n",
    "                                 eval_every=None, seed=SYSTEM_SEED)   # best is something like (0.5, 0.05) or (0.2, 0)\n",
    "    observable = TimeDelayedObservation(hh = 3, control_dim=du, time_embedding_dim=8,\n",
    "                                        use_states=False, use_costs=True, use_controls=True, use_time=True)\n",
    "    do = observable.obs_dim  # dimension of observations to lift from\n",
    "\n",
    "    # --------------------------------------------------------------------------------------\n",
    "    # ------------------------    LIFT/SYSID HYPERPARAMETERS    ----------------------------\n",
    "    # --------------------------------------------------------------------------------------\n",
    "\n",
    "    dl = 32  # dimension of state to lift to\n",
    "    \n",
    "    sysid_args = {\n",
    "        'obs_dim': do,\n",
    "        'control_dim': du,\n",
    "        'initial_control': initial_control,\n",
    "\n",
    "        'max_traj_len': int(1e6),\n",
    "        \n",
    "        # define the sysid search space, which is centered around `initial_control`\n",
    "        'exploration_scales': (0.4, 0.2),\n",
    "        'exploration_bounds': ((0., 1.),\n",
    "                               (0., 0.5)),\n",
    "\n",
    "        'depth': 6,\n",
    "        'sigma': 0,  # inject noise\n",
    "        'determinstic_encoder': True,  # whether to use VAE-type encoder or not\n",
    "        'num_epochs': 500,\n",
    "        'batch_size': 32,\n",
    "        'lifter_lr': 0.002,                                                           \n",
    "        'sysid_lr': 0.002,\n",
    "\n",
    "        'seed': SYSID_SEED,\n",
    "    }\n",
    "\n",
    "    # --------------------------------------------------------------------------------------\n",
    "    # ------------------------    CONTROLLER HYPERPARAMETERS    ----------------------------\n",
    "    # --------------------------------------------------------------------------------------\n",
    "\n",
    "    h = 5 # controller memory length (# of w's to use on inference)\n",
    "    m_update_rescaler = lambda : ADAM(alpha=0.00, betas=(0.9, 0.99), use_bias_correction=True)\n",
    "    m0_update_rescaler = lambda : ADAM(alpha=0.004, betas=(0.9, 0.99), use_bias_correction=True)\n",
    "    k_update_rescaler = lambda : ADAM(alpha=0.001, betas=(0.9, 0.99), use_bias_correction=True)\n",
    "#     m_update_rescaler = lambda : FIXED_RESCALE(alpha=0.0)\n",
    "#     m0_update_rescaler = lambda : FIXED_RESCALE(alpha=0.01)\n",
    "#     k_update_rescaler = lambda : FIXED_RESCALE(alpha=0.0)\n",
    "\n",
    "    bpc_args = {\n",
    "        'h': h,  \n",
    "        'method': 'REINFORCE',\n",
    "        'initial_scales': (0., 0.05, 0.),  # M, M0, K   (uses M0's scale for REINFORCE)\n",
    "        'rescalers': (m_update_rescaler, m0_update_rescaler, k_update_rescaler),\n",
    "#         'bounds': ([0, 0], [1, 1]),\n",
    "        'initial_u': initial_control,\n",
    "        'decay_scales': False,\n",
    "        'use_tanh': False,  # dont\n",
    "        'use_stabilizing_K': False,\n",
    "        'seed': CONTROLLER_SEED\n",
    "    }\n",
    "\n",
    "    # this is a bit of a mess at the minute, but here goes: \n",
    "    #         - `OfflineSysid` is a wrapper to do sysid phase followed by control,\n",
    "    #         - `LiftedController` is a wrapper that lifts states before passing to the controller, and \n",
    "    #         - `EvanBPC` is the controller (can be replaced with `extravaganza.controllers.RBPC` as well)\n",
    "    # I currently use lambdas as object generators to make them from scratch easily, but soon i will switch to actual\n",
    "    # generators or using deepcopies or something :)\n",
    "    \n",
    "    make_controllers = {\n",
    "#         '{}/{}'.format(*[round(v.item(), 2) for v in initial_control]): lambda sys: ConstantController(initial_control, do),\n",
    "#         'Linear': lambda sys: OfflineSysid(lambda sysid: LiftedController(EvanBPC(sysid.A, sysid.B, **bpc_args), sysid), \n",
    "#                                                                             sysid=Lifter(method='identity', state_dim=do, **sysid_args), T0=T0),\n",
    "#         'Koopman': lambda sys: OfflineSysid(lambda sysid: LiftedController(EvanBPC(sysid.A, sysid.B, **bpc_args), sysid), \n",
    "#                                                                             sysid=Lifter(method='fourier', state_dim=dl, **sysid_args), T0=T0),\n",
    "        'NN': lambda sys: OfflineSysid(lambda sysid: LiftedController(EvanBPC(sysid.A, sysid.B, **bpc_args), sysid), \n",
    "                                                                            sysid=Lifter(method='nn', state_dim=dl, **sysid_args), T0=T0),\n",
    "    }\n",
    "    experiment_args = {    \n",
    "        'make_system': make_system,\n",
    "        'make_controllers': make_controllers,\n",
    "        'observable': observable,\n",
    "        'num_trials': num_trials,\n",
    "        'T': T,\n",
    "        'reset_condition': reset_condition,\n",
    "        'reset_seed': None,\n",
    "        'use_multiprocessing': use_multiprocessing,\n",
    "        'render_every': render_every,\n",
    "    }\n",
    "    return experiment_args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dd3a0d",
   "metadata": {},
   "source": [
    "## actually run the thing :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c78c6b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "INFO: Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "INFO: Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
      "INFO: Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
      "INFO: (EXPERIMENT) --------------------------------------------------\n",
      "INFO: (EXPERIMENT) ----------------- TRIAL 0 -----------------------\n",
      "INFO: (EXPERIMENT) --------------------------------------------------\n",
      "\n",
      "INFO: (EXPERIMENT): testing NN\n",
      "  0%|                                                                                                                      | 0/10000 [00:00<?, ?it/s]INFO: (EXPERIMENT): reset at t=0!\n",
      " 10%|██████▍                                                         | 999/10000 [00:23<03:20, 44.78it/s, control=[0.41981208 0.17697094], cost=0.27]INFO: (EXPERIMENT): reset at t=1000!\n",
      " 20%|████████████▌                                                  | 1999/10000 [00:46<03:09, 42.15it/s, control=[0.2527911  0.18740214], cost=0.25]INFO: (EXPERIMENT): reset at t=2000!\n",
      " 30%|██████████████████▌                                           | 2996/10000 [01:10<02:54, 40.20it/s, control=[0.         0.12092427], cost=0.193]INFO: (EXPERIMENT): reset at t=3000!\n",
      " 40%|████████████████████████▊                                     | 3996/10000 [01:33<02:17, 43.72it/s, control=[0.08659356 0.03031711], cost=0.253]INFO: (SYSID WRAPPER) ending exploration at timestep 4000\n",
      "INFO: (LIFTER): ending sysid phase at step 3999\n",
      "INFO: training!\n",
      "INFO: mean loss for epochs -25 - 0 was {'linearization': 1.360399828480431e-05, 'cpc': 3.4934674693692114, 'consistency': 0.0, 'simplification': 0.12762842997307738, 'residual centeredness': 4.2746289811050034e-07, 'centeredness': 0.18442558755557373}\n",
      "INFO: mean loss for epochs 0 - 25 was {'linearization': 0.00016441345173593764, 'cpc': 1.3722812327069622, 'consistency': 0.0, 'simplification': 0.28055830090757344, 'residual centeredness': 7.735248477816427e-06, 'centeredness': 0.002684363731146262}\n",
      "INFO: mean loss for epochs 25 - 50 was {'linearization': 0.0002754990126648467, 'cpc': 0.8879847711516965, 'consistency': 0.0, 'simplification': 0.28186512443807815, 'residual centeredness': 1.4138080163588127e-05, 'centeredness': 0.00023111158849397105}\n",
      "INFO: mean loss for epochs 50 - 75 was {'linearization': 0.0004818296186943283, 'cpc': 0.7340603472148216, 'consistency': 0.0, 'simplification': 0.27993593050106885, 'residual centeredness': 3.8607887408283384e-05, 'centeredness': 0.00042068091773539743}\n",
      " 40%|████████████████████████▊                                     | 3996/10000 [01:50<02:17, 43.72it/s, control=[0.08659356 0.03031711], cost=0.253]INFO: mean loss for epochs 75 - 100 was {'linearization': 0.0005530145262450641, 'cpc': 0.6240697522701755, 'consistency': 0.0, 'simplification': 0.27781040056578576, 'residual centeredness': 4.826188649872166e-05, 'centeredness': 0.00041613938890511617}\n",
      "INFO: mean loss for epochs 100 - 125 was {'linearization': 0.0005486524162246966, 'cpc': 0.56222650082842, 'consistency': 0.0, 'simplification': 0.27725680975664047, 'residual centeredness': 4.780357414083189e-05, 'centeredness': 0.0003959056600392467}\n",
      "INFO: mean loss for epochs 125 - 150 was {'linearization': 0.0005728716785632704, 'cpc': 0.5316120978516917, 'consistency': 0.0, 'simplification': 0.27643727603939267, 'residual centeredness': 4.699030761076434e-05, 'centeredness': 0.0003944203667824988}\n",
      "INFO: mean loss for epochs 150 - 175 was {'linearization': 0.0006138229807895127, 'cpc': 0.5152620985527193, 'consistency': 0.0, 'simplification': 0.2759900730031152, 'residual centeredness': 6.345120498677247e-05, 'centeredness': 0.0004158483321941364}\n",
      "INFO: mean loss for epochs 175 - 200 was {'linearization': 0.0006724300260858371, 'cpc': 0.4963365511259725, 'consistency': 0.0, 'simplification': 0.2751106953957388, 'residual centeredness': 7.58699128533085e-05, 'centeredness': 0.0004431655748920738}\n",
      "INFO: mean loss for epochs 200 - 225 was {'linearization': 0.0007656560900373263, 'cpc': 0.47739057642798266, 'consistency': 0.0, 'simplification': 0.2737139802257861, 'residual centeredness': 9.553292737894391e-05, 'centeredness': 0.0004872359523564697}\n",
      "INFO: mean loss for epochs 225 - 250 was {'linearization': 0.0009398038869896422, 'cpc': 0.46994102291522494, 'consistency': 0.0, 'simplification': 0.2717193263865286, 'residual centeredness': 0.0001610011258627574, 'centeredness': 0.0005980518878453539}\n",
      "INFO: mean loss for epochs 250 - 275 was {'linearization': 0.0010282085236019243, 'cpc': 0.4471242587316421, 'consistency': 0.0, 'simplification': 0.2698334701022794, 'residual centeredness': 0.00014846068100499933, 'centeredness': 0.0006317805865089006}\n",
      "INFO: mean loss for epochs 275 - 300 was {'linearization': 0.0011452358403250635, 'cpc': 0.4369529560977412, 'consistency': 0.0, 'simplification': 0.2681306034470758, 'residual centeredness': 0.00017161247693300017, 'centeredness': 0.000694856550619607}\n",
      "INFO: mean loss for epochs 300 - 325 was {'linearization': 0.0011783013853146843, 'cpc': 0.4077998242695485, 'consistency': 0.0, 'simplification': 0.2673070506222786, 'residual centeredness': 0.00011598260923803184, 'centeredness': 0.0006901438764444103}\n",
      "INFO: mean loss for epochs 325 - 350 was {'linearization': 0.0012028571413153961, 'cpc': 0.3783929470806352, 'consistency': 0.0, 'simplification': 0.267651418875302, 'residual centeredness': 0.00012605227103750297, 'centeredness': 0.0006998469515670994}\n",
      "INFO: mean loss for epochs 350 - 375 was {'linearization': 0.0010489872990987234, 'cpc': 0.3379398164922192, 'consistency': 0.0, 'simplification': 0.2675175457664074, 'residual centeredness': 7.152468252101127e-05, 'centeredness': 0.0006675567744006853}\n",
      "INFO: mean loss for epochs 375 - 400 was {'linearization': 0.0010840651124242632, 'cpc': 0.3262028135311219, 'consistency': 0.0, 'simplification': 0.26610638732871705, 'residual centeredness': 0.00013064753881493626, 'centeredness': 0.0007464024345291921}\n",
      "INFO: mean loss for epochs 400 - 425 was {'linearization': 0.0009753806613415721, 'cpc': 0.29814882023680594, 'consistency': 0.0, 'simplification': 0.2660606574723797, 'residual centeredness': 7.162719539682286e-05, 'centeredness': 0.0007153272912816442}\n",
      "INFO: mean loss for epochs 425 - 450 was {'linearization': 0.0010318226414176848, 'cpc': 0.28813203071874954, 'consistency': 0.0, 'simplification': 0.26564451449340387, 'residual centeredness': 0.00013184788877637282, 'centeredness': 0.0007369513265368896}\n",
      "INFO: mean loss for epochs 450 - 475 was {'linearization': 0.0009809364551385386, 'cpc': 0.2674570410578482, 'consistency': 0.0, 'simplification': 0.265227049565123, 'residual centeredness': 8.298726645622786e-05, 'centeredness': 0.000724042320116789}\n",
      "INFO: mean loss for epochs 474 - 499 was {'linearization': 0.001115548185114148, 'cpc': 0.2667025466215226, 'consistency': 0.0, 'simplification': 0.2647309984699372, 'residual centeredness': 0.00018943004539690472, 'centeredness': 0.0008033248695362764}\n",
      " 40%|████████████████████████                                    | 4000/10000 [03:06<9:56:58,  5.97s/it, control=[0.06388667 0.23275638], cost=0.351]INFO: (EXPERIMENT): reset at t=4000!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||A||_op = 0.994439959526062\n",
      "||B||_F = 0.4898119270801544\n",
      "||A-BK||_op = 0.9899886846542358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████████████████████████████▌                                 | 4601/10000 [03:25<02:50, 31.63it/s, control=[0.19755816 0.11054642], cost=0.321]"
     ]
    }
   ],
   "source": [
    "# run\n",
    "experiment = Experiment(name)\n",
    "stats = experiment(get_experiment_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044a38b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save args and stats!  --  note that to save the args, we actually save the `get_args` function. we can print the \n",
    "#                           source code later to see the hyperparameters we chose\n",
    "# experiment.save(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8107c23",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "We keep track of the useful information through `Stats` objects, which can `register()` a variable to keep track of (which it does via calls to `update()`) and which can be aggregated via `Stats.aggregate()` for mean and variance statistics. \n",
    "\n",
    "We define below a plotting arrangement that plots all the desired quantities from both the system and controller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185524c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(experiment: Experiment):\n",
    "    assert experiment.stats is not None, 'cannot plot the results of an experiment that hasnt been run'\n",
    "    all_stats = experiment.stats\n",
    "    \n",
    "    # clear plot and calc nrows\n",
    "    plt.clf()\n",
    "    n = 4\n",
    "    nrows = n + (len(all_stats) + 1) // 2\n",
    "    fig, ax = plt.subplots(nrows, 2, figsize=(16, 6 * nrows))\n",
    "\n",
    "    # plot stats\n",
    "    for i, (method, stats) in enumerate(all_stats.items()):\n",
    "        if stats is None: \n",
    "            logging.warning('{} had no stats'.format(method))\n",
    "            continue\n",
    "            \n",
    "        stats.plot(ax[0, 0], 'lrs', label=method)\n",
    "        stats.plot(ax[1, 0], 'costs', label=method)\n",
    "        stats.plot(ax[1, 1], 'costs', label=method, plot_cummean=True)\n",
    "        stats.plot(ax[2, 0], 'avg train losses since reset', label=method)\n",
    "        stats.plot(ax[2, 1], 'avg val losses since reset', label=method)        \n",
    "        \n",
    "        stats.plot(ax[3, 0], 'states', label=method, plot_norm=True)  # norm of the \"state\"\n",
    "        from extravaganza.sysid import LOSS_WEIGHTS\n",
    "        for k in LOSS_WEIGHTS.keys(): stats.plot(ax[3, 1], k, label=k)  # various nn losses\n",
    "            \n",
    "        i_ax = ax[n + i // 2, i % 2]\n",
    "        stats.plot(ax[0, 1], 'disturbances', label=method, plot_norm=True)\n",
    "        idx = 1\n",
    "        stats.plot(i_ax, '-K @ state', label='-K @ state', plot_idx=idx)\n",
    "        stats.plot(i_ax, 'M \\cdot w', label='M \\cdot w', plot_idx=idx)\n",
    "        stats.plot(i_ax, 'M0', label='M0', plot_idx=idx)\n",
    "        i_ax.set_title('u decomp for {}'.format(method))\n",
    "        i_ax.legend()\n",
    "\n",
    "    # set titles and legends and limits and such\n",
    "    # (note: `ylim()` is so useful! because sometimes one thing blows up and then autoscale messes up all plots)\n",
    "    _ax = ax[0, 0]; _ax.set_title('learning rate'); _ax.legend()\n",
    "    _ax = ax[0, 1]; _ax.set_title('disturbances'); _ax.legend()\n",
    "    _ax = ax[1, 0]; _ax.set_title('instantaneous costs'); _ax.legend()\n",
    "    _ax = ax[1, 1]; _ax.set_title('avg costs'); _ax.legend(); ylim(_ax, 0, 10000)\n",
    "    _ax = ax[2, 0]; _ax.set_title('avg train losses since reset'); _ax.legend()\n",
    "    _ax = ax[2, 1]; _ax.set_title('avg val losses since reset'); _ax.legend()\n",
    "    _ax = ax[3, 0]; _ax.set_title('reconstructed states'); _ax.legend()\n",
    "    _ax = ax[3, 1]; _ax.set_title('nn losses'); _ax.legend()  \n",
    "    pass\n",
    "plot(experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6c615b",
   "metadata": {},
   "source": [
    "### Dynamic Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504ced57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynamic plot\n",
    "anim = render(experiment, 'lrs', 'train losses', sliderkey='lrs', save_path=None, duration=5)\n",
    "vid = anim.to_html5_video()\n",
    "HTML(vid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "extravaganza",
   "language": "python",
   "name": "extravaganza"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
