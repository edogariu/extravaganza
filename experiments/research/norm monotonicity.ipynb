{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75766e83",
   "metadata": {},
   "source": [
    "# Definitions\n",
    "As per the remark at the end of the writeup (I think its remark 10), we can enforce that our function $f: \\mathcal{H}_1 \\rightarrow \\mathcal{H}_2$ is both $L$-subhomogenous and rotationally symmetric (w.r.t. the norm on $\\mathcal{H}_2$) by constructing it according to the following decomposition:\n",
    "$$f(v) = \\varphi(||v||) \\cdot \\phi(v),$$\n",
    "where $\\varphi: \\mathbb{R}_+ \\rightarrow \\mathbb{R}$ is an $L$-subhomogenous one-dimensional function and $\\phi: \\mathcal{H}_1 \\rightarrow \\mathbb{S}_{\\mathcal{H}_2}$ is any function whose image is contained on the sphere in $\\mathcal{H}_2$. In particular, we can represent $\\varphi$ with a ReLU neural network with no biases! Importantly, $\\phi$ can be as complex as we want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4c7ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from extravaganza.models import MLP\n",
    "from extravaganza.utils import set_seed\n",
    "\n",
    "def exponential_linspace_int(start, end, num, divisible_by=1):\n",
    "    \"\"\"Exponentially increasing values of integers.\"\"\"\n",
    "    base = np.exp(np.log(end / start) / (num - 1))\n",
    "    return [int(np.round(start * base**i / divisible_by) * divisible_by) for i in range(num)]\n",
    "\n",
    "def uhoh(message):\n",
    "    print('!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "    print('!!!!!!!!! UH OH !!!!!!!!!!!!')\n",
    "    print('!!!!!!!!!! {} !!!!!!!!!!'.format(message))\n",
    "    print('!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "    pass\n",
    "\n",
    "class NM(nn.Module):\n",
    "    def __init__(self, in_dim: int, out_dim: int, depth: int, seed: int = None, activation = nn.ReLU):\n",
    "        set_seed(seed)\n",
    "        super().__init__()\n",
    "        \n",
    "        self.normnet = MLP(layer_dims=[1, 10, 10, 1],   # \\varphi in the writeup \n",
    "                           activation=activation, \n",
    "                           use_bias=False)  # for homogeneity\n",
    "        layer_dims = exponential_linspace_int(in_dim, out_dim, depth)\n",
    "        self.dirnet = MLP(layer_dims=layer_dims,    # \\phi in the writeup\n",
    "                          activation=activation)\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        assert x.ndim == 2, x.shape\n",
    "        x_norm = torch.norm(x, dim=-1).unsqueeze(-1).type(x.dtype)\n",
    "        varphi = self.normnet(x_norm)\n",
    "        phi = self.dirnet(x)\n",
    "        phi = phi / torch.norm(phi, dim=-1).unsqueeze(-1).type(x.dtype)  # ensure that phi lies on the unit sphere\n",
    "        return varphi * phi\n",
    "\n",
    "\n",
    "# make sure shapes appear ok\n",
    "IN_DIM = 5\n",
    "OUT_DIM = 128\n",
    "DEPTH = 3\n",
    "nm = NM(IN_DIM, OUT_DIM, DEPTH)\n",
    "BATCH_SIZE = 17\n",
    "x = torch.randn(BATCH_SIZE, IN_DIM)\n",
    "\n",
    "if nm(x).shape == (BATCH_SIZE, OUT_DIM): print('seems ok')\n",
    "else: uhoh('incorrect shape')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699294d2",
   "metadata": {},
   "source": [
    "# Testing for *L*-(sub)homogeneity\n",
    "The definition we use is as follows:\n",
    "### Definition\n",
    "#### (Positive) Homogeneity and Subhomogeneity\n",
    "We say that $f: \\mathcal{H}_1 \\rightarrow \\mathcal{H}_2$ is (postiive) **$L$-homogenous** or **homogenous of order $L$** if for all $\\gamma > 0$ and all $v \\in \\mathcal{H}_1$, we know that\n",
    "    $$||f(\\gamma v)||_{\\mathcal{H}_2} = \\gamma^L ||f(v)||_{\\mathcal{H}_2}$$\n",
    "    If instead all we know is that \n",
    "$$||f(\\gamma v)||_{\\mathcal{H}_2} \\leq \\gamma^L ||f(v)||_{\\mathcal{H}_2},$$\n",
    "we refer to $f$ as (positive) **$L$-subhomogeneous**. Note that this definition is weaker than the usual definition of positive homogeneity in that it only needs to hold w.r.t. the norm, allowing for arbitrary rotation and still allowing rich expressitivity.\n",
    "\n",
    "We expect our neural network above to have the above property with $L = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdc7b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 10000\n",
    "L = 1.  # true value for L\n",
    "\n",
    "# run trials\n",
    "vals = []\n",
    "for _ in tqdm.trange(num_trials):\n",
    "    nm = NM(IN_DIM, OUT_DIM, DEPTH)\n",
    "\n",
    "    # sample a random vector v\n",
    "    x = torch.randn(1, IN_DIM)\n",
    "\n",
    "    # sample a random positive scalar \\gamma\n",
    "    gamma = torch.rand(1) * torch.randint(50, size=(1,)) + 1e-8\n",
    "\n",
    "    lhs = torch.norm(nm(gamma * x), dim=-1)\n",
    "    norm_fv = torch.norm(nm(x), dim=-1)\n",
    "    \n",
    "    # change of base formula -- log_gamma(a) = ln(a) / ln(gamma)\n",
    "    a = (lhs / norm_fv).squeeze()\n",
    "    L_estimate = torch.log(a) / torch.log(gamma)\n",
    "    vals.append(L_estimate.item())\n",
    "\n",
    "# confirm\n",
    "vals = np.array(vals)\n",
    "vals = vals[~np.isnan(vals)]\n",
    "mean, std = np.mean(vals), np.std(vals)\n",
    "if abs(mean - L) < 1e-4 and std < 1e-2:\n",
    "    print('seems ok')\n",
    "elif std >= 1e-2:\n",
    "    uhoh('fluctuating estimates for L, std={}'.format(std))\n",
    "else:\n",
    "    uhoh('we were pretty sure L={}'.format(mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1566e021",
   "metadata": {},
   "source": [
    "# Testing for Rotational Symmetry\n",
    "The definition we follow is as follows:\n",
    "### Definition\n",
    "#### Rotational Symmetry\n",
    "We say that $f$ is rotationally symmetric in the sense that $||f \\circ U||_{\\mathcal{H}_2} \\equiv ||f||_{\\mathcal{H}_2}$ everywhere for all unitary transformations $U : \\mathcal{H}_1 \\rightarrow \\mathcal{H}_1$ Note that this definition is weaker than the usual definition of rotational symmetry in that it doesn't require commuting with unitary operators, but instead it basically requires mapping spheres to spheres with arbitrary deformity, allowing rich expressitivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32ffb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ortho_group\n",
    "\n",
    "num_trials = 10000\n",
    "\n",
    "# run trials\n",
    "vals = []\n",
    "for _ in tqdm.trange(num_trials):\n",
    "    nm = NM(IN_DIM, OUT_DIM, DEPTH)\n",
    "\n",
    "    # sample a random vector v\n",
    "    x = torch.randn(IN_DIM)\n",
    "\n",
    "    # sample a random unitary transformation U\n",
    "    U = torch.tensor(ortho_group.rvs(IN_DIM)).type(torch.float32)\n",
    "\n",
    "    lhs = torch.norm(nm((U @ x).unsqueeze(0)), dim=-1)\n",
    "    rhs = torch.norm(nm(x.unsqueeze(0)), dim=-1)\n",
    "    \n",
    "    vals.append(abs((lhs - rhs).item()))\n",
    "\n",
    "# confirm\n",
    "mean, std = np.mean(vals), np.std(vals)\n",
    "if mean < 1e-4 and std < 1e-4:\n",
    "    print('seems ok')\n",
    "elif std >= 1e-4:\n",
    "    uhoh('fluctuating stuff, std={}'.format(std))\n",
    "else:\n",
    "    uhoh('we were pretty sure LHS-RHS={}'.format(mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f4132b",
   "metadata": {},
   "source": [
    "# Testing Norm Monotonicity\n",
    "\n",
    "The following proposition is the reason for this notebook:\n",
    "\n",
    "### Proposition: \n",
    "#### *Suppose that $f$ is $L$-subhomogenous for some $L > 0$ and rotationally symmetric in the way described above. Then, $f$ is norm-monotonic.*\n",
    "\n",
    "#### Proof\n",
    "Let $x, y \\in \\mathcal{H}_1$ be arbitrary. Note that we can certainly find some unitary operator $U$ for which\n",
    "        $$y = \\left(\\frac{||y||_{\\mathcal{H}_1}}{||x||_{\\mathcal{H}_1}}\\right)U(x)$$\n",
    "Let $\\gamma :=\\frac{||y||_{\\mathcal{H}_1}}{||x||_{\\mathcal{H}_1}} > 0$. Then, $y = \\gamma  U(x)$. Since $f$ is $L$-subhomogenous, we can readily see that\n",
    "$$\\frac{||f(y)||_{\\mathcal{H}_2}}{||f(x)||_{\\mathcal{H}_2}} = \\frac{||f(\\gamma U(x)||_{\\mathcal{H}_2}}{||f(x)||_{\\mathcal{H}_2}} \\leq \\frac{\\gamma^L ||f(U(x))||_{\\mathcal{H}_2}}{||f(x)||_{\\mathcal{H}_2}}$$ \n",
    "By the rotational symmetry of $f$ we know that $||f(U(x))||_{\\mathcal{H}_2} = ||f(x)||_{\\mathcal{H}_2}$, from which we find\n",
    "    $$\\frac{||f(y)||_{\\mathcal{H}_2}}{||f(x)||_{\\mathcal{H}_2}} \\leq \\gamma^L = \\left(\\frac{||y||_{\\mathcal{H}_1}}{||x||_{\\mathcal{H}_1}}\\right)^L$$\n",
    "This means that if $||f(y)||_{\\mathcal{H}_2} > ||f(x)||_{\\mathcal{H}_2}$, we immediately see that $\\gamma > 1$ and therefore that $||y||_1 > ||x||_{\\mathcal{H}_1}$. This is the condition for strict norm-monotonicity. $\\blacksquare$\n",
    "\n",
    "\n",
    "Since our function $f$ in this notebook satisfies the two above properties ($L$-subhomogeneity and rotational symmetry) w.r.t. the $||\\cdot||_{\\mathcal{H}_2}$ norm, the proposition should guarantee strict norm monotonicity. This is what we will test for below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd668a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 10000\n",
    "\n",
    "# run trials\n",
    "for _ in tqdm.trange(num_trials):\n",
    "    nm = NM(IN_DIM, OUT_DIM, DEPTH)\n",
    "\n",
    "    # sample two random vectors x and y\n",
    "    x = torch.randn(1, IN_DIM)\n",
    "    y = torch.randn(1, IN_DIM)\n",
    "    fx = nm(x).squeeze(0)\n",
    "    fy = nm(y).squeeze(0)\n",
    "    \n",
    "    n_x, n_y, n_fx, n_fy = map(lambda t: torch.norm(t).item(), [x, y, fx, fy])\n",
    "    if n_fx < n_fy: assert n_x < n_y, (n_x, n_y, n_fx, n_fy)\n",
    "    elif n_fx > n_fy: assert n_x > n_y, (n_x, n_y, n_fx, n_fy)\n",
    "    elif n_fx > 0 or n_fy > 0: assert abs(n_x - n_y) < 1e-4, (n_x, n_y, n_fx, n_fy)\n",
    "        \n",
    "print('seems ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e250f181",
   "metadata": {},
   "source": [
    "# But is it a good NN?\n",
    "Ok, so we have crafted a general and strictly norm-monotonic neural network architecture. But is it expressive enough to learn?\n",
    "\n",
    "To answer this question we will use it to do some good ol' contrastive learning on MNIST and compare it with an equivalently-sized regular MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b99a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from pytorch_metric_learning.losses import SupConLoss\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "loss_fn = SupConLoss()\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Lambda(lambda t: torch.flatten(t, start_dim=0))\n",
    "])\n",
    "train_dataset = torchvision.datasets.MNIST('../../data', train=False, transform=transform)\n",
    "val_dataset = torchvision.datasets.MNIST('../../data', train=False, transform=transform)\n",
    "train_dl = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "val_dl = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "def train(model, opt, num_epochs: int):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for _ in tqdm.trange(num_epochs):\n",
    "        # train\n",
    "        epoch_train_losses = []\n",
    "        for x, y in train_dl:\n",
    "            opt.zero_grad()\n",
    "            emb = model(x)\n",
    "            loss = loss_fn(emb, y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            epoch_train_losses.append(loss.item())\n",
    "        train_losses.append(np.mean(epoch_train_losses))\n",
    "        \n",
    "        # val\n",
    "        epoch_val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_dl:\n",
    "                emb = model(x)\n",
    "                loss = loss_fn(emb, y)\n",
    "                epoch_val_losses.append(loss.item())\n",
    "            val_losses.append(np.mean(epoch_val_losses))\n",
    "            \n",
    "    return train_losses, val_losses\n",
    "         \n",
    "    \n",
    "seed = None\n",
    "dim = 128\n",
    "depth = 8\n",
    "num_epochs = 30\n",
    "activation = nn.ReLU\n",
    "set_seed(seed)\n",
    "\n",
    "# make models\n",
    "models = {\n",
    "    'ours ReLU': NM(in_dim=28 * 28, out_dim=dim, depth=depth, activation=nn.ReLU),\n",
    "    'ours Leaky': NM(in_dim=28 * 28, out_dim=dim, depth=depth, activation=nn.LeakyReLU),\n",
    "    'default ReLU': MLP(layer_dims=exponential_linspace_int(28 * 28, dim, depth), activation=nn.ReLU),\n",
    "    'default Leaky': MLP(layer_dims=exponential_linspace_int(28 * 28, dim, depth), activation=nn.LeakyReLU)\n",
    "}\n",
    "\n",
    "# run trials\n",
    "results = {'train_losses': {}, 'val_losses': {}}\n",
    "for k, model in models.items():\n",
    "    print(k)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=0.004)\n",
    "    t, v = train(model, opt, num_epochs)\n",
    "    results['train_losses'][k] = t\n",
    "    results['val_losses'][k] = v\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "for k in models.keys():\n",
    "    t, v = results['train_losses'][k], results['val_losses'][k]\n",
    "    ax[0].plot(range(len(t)), t, label=k)\n",
    "    ax[1].plot(range(len(v)), v, label=k)\n",
    "    \n",
    "_ax = ax[0]; _ax.legend(); _ax.set_xlabel('epoch'); _ax.set_ylabel('loss'); _ax.set_title('train losses');\n",
    "_ax = ax[1]; _ax.legend(); _ax.set_xlabel('epoch'); _ax.set_ylabel('loss'); _ax.set_title('val losses');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc972fd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# check embedding spaces and t-SNE plot what's going on\n",
    "fig, ax = plt.subplots(len(models), 1, figsize=(8, 8 * len(models)))\n",
    "N = 1000 # how many points to actually plot in each fig, val dataset is 10k full\n",
    "\n",
    "for _ax, (k, model) in zip(ax, models.items()):\n",
    "    print(k)\n",
    "    embs = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_dl:\n",
    "            embs.append(model(x).squeeze(0).data.numpy())\n",
    "            labels.append(y.item())\n",
    "    X = np.stack(embs, axis=0)\n",
    "    idxs = np.random.permutation(len(X))[:N] \n",
    "    X = X[idxs]\n",
    "    labels = [labels[i] for i in idxs]\n",
    "\n",
    "    tsne = TSNE(n_components=2, learning_rate='auto',\n",
    "                       init='random').fit_transform(X)\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df['tsne-one'] = tsne[:, 0]\n",
    "    df['tsne-two'] = tsne[:, 1]\n",
    "    df['label'] = labels\n",
    "    sns.scatterplot(\n",
    "        x=\"tsne-one\", y=\"tsne-two\",\n",
    "        hue=\"label\",\n",
    "        palette=sns.color_palette(\"hls\", 10),  # 21 different speakers in the first 10k data points\n",
    "        data=df,\n",
    "        legend=\"full\",\n",
    "        alpha=0.3,\n",
    "        ax=_ax\n",
    "    )\n",
    "    _ax.set_title('{} Embedding Space for MNIST Val Dataset'.format(k))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1fce2e",
   "metadata": {},
   "source": [
    "# Testing LQR and H_inf Controllers with Norm-Monotonic Lifter\n",
    "The entire reason we were interested in neural nets with this property is to ensure that minimizing state norm of the lifted state corresponds to minimizing norm of the inputs!. Lifters with this property technically form an LDS with quadratic costs, lending themselves to provable control via LQR (optimal control, $H_{\\infty}$ (robust control), and perhaps even GPC!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576b5ca4",
   "metadata": {},
   "source": [
    "### Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6be8c730",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created a temporary directory at /var/folders/5m/0xr906c130vdqvkm3g21n6wr0000gn/T/tmpb4k2_zho\n",
      "INFO: Writing /var/folders/5m/0xr906c130vdqvkm3g21n6wr0000gn/T/tmpb4k2_zho/_remote_module_non_scriptable.py\n",
      "INFO: Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "INFO: Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "INFO: Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
      "INFO: Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
      "WARNING: (Predictor): using epsilon=0, so there is no point in training. so, we will not train :)\n",
      "INFO: (LDS): initial state is [1.0642822]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lifted :  ||A||=0.837    ||B||=1.368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                      | 0/10000 [00:00<?, ?it/s]INFO: (EXPERIMENT): reset!\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:21<00:00, 463.81it/s]\n",
      "INFO: (LIFTER): ending sysid phase at step 10000\n",
      "INFO: training!\n",
      "INFO: mean loss for past 25 epochs was {'linearization': 3.9753585775138665e-05, 'simplification': 0.7121675345632764, 'reconstruction': 19.350510703192818, 'controllability': 0.0}\n",
      "INFO: mean loss for past 25 epochs was {'linearization': 56959.022390726364, 'simplification': 0.6933339738845824, 'reconstruction': 14.321412764655221, 'controllability': 0.0}\n",
      "INFO: mean loss for past 25 epochs was {'linearization': 59598.70874046124, 'simplification': 0.6956968262460498, 'reconstruction': 14.094584566752117, 'controllability': 0.0}\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s: %(message)s', level=logging.INFO)  # set level to INFO for wordy\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from extravaganza.dynamical_systems import LDS\n",
    "\n",
    "from extravaganza.controllers import NonlinearBPC, LambdaController\n",
    "from extravaganza.observables import TimeDelayedObservation, FullObservation\n",
    "from extravaganza.sysid import Predictor, Lifter\n",
    "from extravaganza.controllers import LQR, HINF, BPC, GPC, RBPC\n",
    "from extravaganza.rescalers import ADAM, D_ADAM, DoWG\n",
    "from extravaganza.utils import ylim, render, append, opnorm, dare_gain, least_squares\n",
    "from extravaganza.experiments import Experiment\n",
    "\n",
    "# seeds for randomness. setting to `None` uses random seeds\n",
    "SYSTEM_SEED = 5\n",
    "CONTROLLER_SEED = None\n",
    "SYSID_SEED = None\n",
    "\n",
    "name = 'research_test'\n",
    "filename = '../logs/{}.pkl'.format(name)\n",
    "\n",
    "def get_experiment_args():\n",
    "    # --------------------------------------------------------------------------------------\n",
    "    # ------------------------    EXPERIMENT HYPERPARAMETERS    ----------------------------\n",
    "    # --------------------------------------------------------------------------------------\n",
    "\n",
    "    num_trials = 1\n",
    "    T = 5000  # total timesteps\n",
    "    T0 = 10000  # number of timesteps to just sysid for our methods\n",
    "    reset_condition = lambda t: t == 0  # how often to reset the system\n",
    "    use_multiprocessing = False\n",
    "    render_every = None\n",
    "\n",
    "    # --------------------------------------------------------------------------------------\n",
    "    # --------------------------    SYSTEM HYPERPARAMETERS    ------------------------------\n",
    "    # --------------------------------------------------------------------------------------\n",
    "\n",
    "    du = 1  # control dim\n",
    "    ds = 1  # state dim\n",
    "    initial_control = jnp.zeros(du)\n",
    "\n",
    "    disturbance_type = 'gaussian'\n",
    "    cost_fn = 'quad'\n",
    "\n",
    "    make_system = lambda : LDS(ds, du, disturbance_type, cost_fn, seed=SYSTEM_SEED)\n",
    "    hh = 8\n",
    "#     observable = FullObservation(); do = ds\n",
    "    observable = TimeDelayedObservation(hh=hh, control_dim=du, use_states=False, use_costs=True, use_controls=True, use_time=True); assert not observable.use_states; do = observable.hh * (1 * observable.use_costs + du * observable.use_controls) + 8 * observable.use_time\n",
    "\n",
    "    # --------------------------------------------------------------------------------------\n",
    "    # ------------------------    LIFT/SYSID HYPERPARAMETERS    ----------------------------\n",
    "    # --------------------------------------------------------------------------------------\n",
    "\n",
    "    dl = 12\n",
    "    \n",
    "    sysid_args = {\n",
    "        'obs_dim': do,\n",
    "        'control_dim': du,\n",
    "        \n",
    "        'max_traj_len': int(1e6),\n",
    "        'exploration_scale': 0.6,\n",
    "\n",
    "        'depth': 10,\n",
    "        'num_epochs': 500,\n",
    "        'batch_size': min(1024, T0 - 20),\n",
    "        'lr': 0.004,\n",
    "        \n",
    "        'seed': SYSID_SEED,\n",
    "    }\n",
    "\n",
    "    linear = Predictor(epsilon=0, norm_fn=observable.norm_fn, **sysid_args)\n",
    "    lifted = Lifter(method='nn', h=5, state_dim=dl, **sysid_args)\n",
    "\n",
    "    if do == ds:\n",
    "        dynamics = {'g.t.': (None, None)}\n",
    "        sysids = {}\n",
    "    else:\n",
    "        dynamics = {}\n",
    "        sysids = {\n",
    "#             'Linear': linear,\n",
    "            'Lifted': lifted,\n",
    "        }\n",
    "\n",
    "    for k, sysid in sysids.items(): # interact in order to perform sysid\n",
    "        # make system and get initial control\n",
    "        system = make_system()\n",
    "        control = initial_control\n",
    "        print(k, ':  ||A||={}    ||B||={}'.format(round(opnorm(system.A), 3), round(jnp.linalg.norm(system.B).item(), 3)))\n",
    "        traj = []\n",
    "        for t in tqdm.trange(T0):\n",
    "            if reset_condition(t):\n",
    "                logging.info('(EXPERIMENT): reset!')\n",
    "                system.reset(None)\n",
    "                sysid.end_trajectory()\n",
    "                traj = []\n",
    "                pass\n",
    "                    \n",
    "            cost, state = system.interact(control)  # state will be `None` for unobservable systems\n",
    "            traj.extend([state, cost])\n",
    "            obs = observable(traj)\n",
    "            control = sysid.explore(cost, obs) + initial_control\n",
    "            traj.append(control)\n",
    "                    \n",
    "            if (isinstance(state, jnp.ndarray) and jnp.any(jnp.isnan(state))) or (cost > 1e20):\n",
    "                logging.error('(EXPERIMENT): state {} or cost {} diverged'.format(state, cost))\n",
    "                assert False\n",
    "                \n",
    "        sysid.end_exploration()\n",
    "        dynamics[k] = (sysid.A, sysid.B)\n",
    "\n",
    "    # test \n",
    "    make_controllers = {}\n",
    "    for k, (A, B) in dynamics.items():\n",
    "        \n",
    "        def init(controller):\n",
    "            controller.sysid = sysids[k]\n",
    "            pass\n",
    "        \n",
    "        def get_control(controller, cost, obs):\n",
    "            state = controller.sysid.get_state(obs)\n",
    "            control = controller._controller.get_control(cost, state)\n",
    "            return control\n",
    "        \n",
    "        def get_controller(key, controller_class):\n",
    "            def _func(sys):\n",
    "                A, B = dynamics[key]\n",
    "                if A is None: A = sys.A\n",
    "                if B is None: B = sys.B\n",
    "                Q = jnp.eye(A.shape[0])\n",
    "                R = jnp.eye(B.shape[1])\n",
    "                controller = controller_class(A=A, B=B, Q=Q, R=R, seed=CONTROLLER_SEED)\n",
    "                if 'g.t.' not in k: return LambdaController(controller=controller, init_fn=init, get_control=get_control)\n",
    "                else: return controller\n",
    "            return _func\n",
    "        \n",
    "        make_controllers.update({\n",
    "            k + ' LQR': get_controller(k, LQR),\n",
    "            k + ' HINF': get_controller(k, HINF),\n",
    "            k + ' GPC': get_controller(k, GPC),\n",
    "#             k + ' BPC': get_controller(k, BPC),\n",
    "#             k + ' RBPC': get_controller(k, RBPC),\n",
    "        })\n",
    "    experiment_args = {\n",
    "        'make_system': make_system,\n",
    "        'make_controllers': make_controllers,\n",
    "        'num_trials': num_trials,\n",
    "        'observable': observable,\n",
    "        'T': T, \n",
    "        'reset_condition': reset_condition,\n",
    "        'reset_seed': SYSTEM_SEED,\n",
    "        'use_multiprocessing': use_multiprocessing,\n",
    "        'render_every': render_every,\n",
    "    }   \n",
    "    return experiment_args\n",
    "\n",
    "experiment = Experiment(name)\n",
    "stats = experiment(get_experiment_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6b10b1",
   "metadata": {},
   "source": [
    "### Save Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3364ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save args and stats!  --  note that to save the args, we actually save the `get_args` function. we can print the \n",
    "# #                           source code later to see the hyperparameters we chose\n",
    "# experiment.save(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db978f0c",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956f422f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lds(experiment: Experiment):\n",
    "    assert experiment.stats is not None, 'cannot plot the results of an experiment that hasnt been run'\n",
    "    all_stats = experiment.stats\n",
    "    \n",
    "    # clear plot and calc nrows\n",
    "    plt.clf()\n",
    "    n = 5\n",
    "    nrows = n + (len(all_stats) + 1) // 2\n",
    "    fig, ax = plt.subplots(nrows, 2, figsize=(16, 6 * nrows))\n",
    "\n",
    "    # plot stats\n",
    "    for i, (method, stats) in enumerate(all_stats.items()):\n",
    "#         if 'g.t.' not in method: continue\n",
    "        if stats is None: \n",
    "            logging.warning('{} had no stats'.format(method))\n",
    "            continue\n",
    "        stats.plot(ax[0, 0], 'xs', label=method)\n",
    "#         stats.plot(ax[0, 1], 'ws', label=method)\n",
    "        stats.plot(ax[3, 1], 'us', label=method)\n",
    "        stats.plot(ax[4, 0], 'state_norm', label=method)\n",
    "        if 'costs' in stats:\n",
    "            stats.plot(ax[1, 0], 'avg costs', label=method)\n",
    "            stats.plot(ax[1, 1], 'costs', label=method)\n",
    "        else:\n",
    "            stats.plot(ax[1, 0], 'avg fs', label=method)\n",
    "            stats.plot(ax[1, 1], 'fs', label=method)\n",
    "    \n",
    "        stats.plot(ax[2, 0], '||A||_op', label=method)\n",
    "        stats.plot(ax[2, 1], '||B||_F', label=method)\n",
    "        stats.plot(ax[3, 0], '||A-BK||_op', label=method)\n",
    "        i_ax = ax[n + i // 2, i % 2]\n",
    "        stats.plot(ax[0, 1], 'disturbance norms', label=method)\n",
    "        stats.plot(i_ax, 'K @ state', label='K @ state')\n",
    "        stats.plot(i_ax, 'M \\cdot w', label='M \\cdot w')\n",
    "        stats.plot(i_ax, 'M0', label='M0')\n",
    "        i_ax.set_title('u decomp for {}'.format(method))\n",
    "        i_ax.legend()\n",
    "\n",
    "    # set titles and legends and limits and such\n",
    "    # (note: `ylim()` is so useful! because sometimes one thing blows up and then autoscale messes up all plots)\n",
    "    _ax = ax[0, 0]; _ax.set_title('position'); _ax.legend()\n",
    "    _ax = ax[0, 1]; _ax.set_title('disturbances'); _ax.legend()\n",
    "    _ax = ax[1, 0]; _ax.set_title('avg costs'); _ax.legend()\n",
    "    _ax = ax[1, 1]; _ax.set_title('costs'); _ax.legend()\n",
    "    \n",
    "    _ax = ax[2, 0]; _ax.set_title('||A||_op'); _ax.legend()\n",
    "    _ax = ax[2, 1]; _ax.set_title('||B||_F'); _ax.legend()\n",
    "    \n",
    "    _ax = ax[3, 0]; _ax.set_title('||A-BK||_op'); _ax.legend()\n",
    "    _ax = ax[3, 1]; _ax.set_title('controls'); _ax.legend()\n",
    "    \n",
    "    _ax = ax[4, 0]; _ax.set_title('state norm'); _ax.legend()\n",
    "    pass\n",
    "\n",
    "plot_lds(experiment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "extravaganza",
   "language": "python",
   "name": "extravaganza"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
