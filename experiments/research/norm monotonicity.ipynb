{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75766e83",
   "metadata": {},
   "source": [
    "# Definitions\n",
    "As per the remark at the end of the writeup (I think its remark 10), we can enforce that our function $f: \\mathcal{H}_1 \\rightarrow \\mathcal{H}_2$ is both $L$-subhomogenous and rotationally symmetric (w.r.t. the norm on $\\mathcal{H}_2$) by constructing it according to the following decomposition:\n",
    "$$f(v) = \\varphi(||v||) \\cdot \\phi(v),$$\n",
    "where $\\varphi: \\mathbb{R}_+ \\rightarrow \\mathbb{R}$ is an $L$-subhomogenous one-dimensional function and $\\phi: \\mathcal{H}_1 \\rightarrow \\mathbb{S}_{\\mathcal{H}_2}$ is any function whose image is contained on the sphere in $\\mathcal{H}_2$. In particular, we can represent $\\varphi$ with a ReLU neural network with no biases! Importantly, $\\phi$ can be as complex as we want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac4c7ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seems ok\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from extravaganza.models import MLP\n",
    "from extravaganza.utils import set_seed\n",
    "\n",
    "def exponential_linspace_int(start, end, num, divisible_by=1):\n",
    "    \"\"\"Exponentially increasing values of integers.\"\"\"\n",
    "    base = np.exp(np.log(end / start) / (num - 1))\n",
    "    return [int(np.round(start * base**i / divisible_by) * divisible_by) for i in range(num)]\n",
    "\n",
    "def uhoh(message):\n",
    "    print('!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "    print('!!!!!!!!! UH OH !!!!!!!!!!!!')\n",
    "    print('!!!!!!!!!! {} !!!!!!!!!!'.format(message))\n",
    "    print('!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "    pass\n",
    "\n",
    "class NM(nn.Module):\n",
    "    def __init__(self, in_dim: int, out_dim: int, depth: int, seed: int = None, activation = nn.ReLU):\n",
    "        set_seed(seed)\n",
    "        super().__init__()\n",
    "        \n",
    "        self.normnet = MLP(layer_dims=[1, 10, 10, 1],   # \\varphi in the writeup \n",
    "                           activation=activation, \n",
    "                           use_bias=False)  # for homogeneity\n",
    "        layer_dims = exponential_linspace_int(in_dim, out_dim, depth)\n",
    "        self.dirnet = MLP(layer_dims=layer_dims,    # \\phi in the writeup\n",
    "                          activation=activation)\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        assert x.ndim == 2, x.shape\n",
    "        x_norm = torch.norm(x, dim=-1).unsqueeze(-1).type(x.dtype)\n",
    "        varphi = self.normnet(x_norm)\n",
    "        phi = self.dirnet(x)\n",
    "        phi = phi / torch.norm(phi, dim=-1).unsqueeze(-1).type(x.dtype)  # ensure that phi lies on the unit sphere\n",
    "        return varphi * phi\n",
    "\n",
    "\n",
    "# make sure shapes appear ok\n",
    "IN_DIM = 5\n",
    "OUT_DIM = 128\n",
    "DEPTH = 3\n",
    "nm = NM(IN_DIM, OUT_DIM, DEPTH)\n",
    "BATCH_SIZE = 17\n",
    "x = torch.randn(BATCH_SIZE, IN_DIM)\n",
    "\n",
    "if nm(x).shape == (BATCH_SIZE, OUT_DIM): print('seems ok')\n",
    "else: uhoh('incorrect shape')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699294d2",
   "metadata": {},
   "source": [
    "# Testing for *L*-(sub)homogeneity\n",
    "The definition we use is as follows:\n",
    "### Definition\n",
    "#### (Positive) Homogeneity and Subhomogeneity\n",
    "We say that $f: \\mathcal{H}_1 \\rightarrow \\mathcal{H}_2$ is (postiive) **$L$-homogenous** or **homogenous of order $L$** if for all $\\gamma > 0$ and all $v \\in \\mathcal{H}_1$, we know that\n",
    "    $$||f(\\gamma v)||_{\\mathcal{H}_2} = \\gamma^L ||f(v)||_{\\mathcal{H}_2}$$\n",
    "    If instead all we know is that \n",
    "$$||f(\\gamma v)||_{\\mathcal{H}_2} \\leq \\gamma^L ||f(v)||_{\\mathcal{H}_2},$$\n",
    "we refer to $f$ as (positive) **$L$-subhomogeneous**. Note that this definition is weaker than the usual definition of positive homogeneity in that it only needs to hold w.r.t. the norm, allowing for arbitrary rotation and still allowing rich expressitivity.\n",
    "\n",
    "We expect our neural network above to have the above property with $L = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdc7b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 10000\n",
    "L = 1.  # true value for L\n",
    "\n",
    "# run trials\n",
    "vals = []\n",
    "for _ in tqdm.trange(num_trials):\n",
    "    nm = NM(IN_DIM, OUT_DIM, DEPTH)\n",
    "\n",
    "    # sample a random vector v\n",
    "    x = torch.randn(1, IN_DIM)\n",
    "\n",
    "    # sample a random positive scalar \\gamma\n",
    "    gamma = torch.rand(1) * torch.randint(50, size=(1,)) + 1e-8\n",
    "\n",
    "    lhs = torch.norm(nm(gamma * x), dim=-1)\n",
    "    norm_fv = torch.norm(nm(x), dim=-1)\n",
    "    \n",
    "    # change of base formula -- log_gamma(a) = ln(a) / ln(gamma)\n",
    "    a = (lhs / norm_fv).squeeze()\n",
    "    L_estimate = torch.log(a) / torch.log(gamma)\n",
    "    vals.append(L_estimate.item())\n",
    "\n",
    "# confirm\n",
    "vals = np.array(vals)\n",
    "vals = vals[~np.isnan(vals)]\n",
    "mean, std = np.mean(vals), np.std(vals)\n",
    "if abs(mean - L) < 1e-4 and std < 1e-2:\n",
    "    print('seems ok')\n",
    "elif std >= 1e-2:\n",
    "    uhoh('fluctuating estimates for L, std={}'.format(std))\n",
    "else:\n",
    "    uhoh('we were pretty sure L={}'.format(mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1566e021",
   "metadata": {},
   "source": [
    "# Testing for Rotational Symmetry\n",
    "The definition we follow is as follows:\n",
    "### Definition\n",
    "#### Rotational Symmetry\n",
    "We say that $f$ is rotationally symmetric in the sense that $||f \\circ U||_{\\mathcal{H}_2} \\equiv ||f||_{\\mathcal{H}_2}$ everywhere for all unitary transformations $U : \\mathcal{H}_1 \\rightarrow \\mathcal{H}_1$ Note that this definition is weaker than the usual definition of rotational symmetry in that it doesn't require commuting with unitary operators, but instead it basically requires mapping spheres to spheres with arbitrary deformity, allowing rich expressitivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32ffb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ortho_group\n",
    "\n",
    "num_trials = 10000\n",
    "\n",
    "# run trials\n",
    "vals = []\n",
    "for _ in tqdm.trange(num_trials):\n",
    "    nm = NM(IN_DIM, OUT_DIM, DEPTH)\n",
    "\n",
    "    # sample a random vector v\n",
    "    x = torch.randn(IN_DIM)\n",
    "\n",
    "    # sample a random unitary transformation U\n",
    "    U = torch.tensor(ortho_group.rvs(IN_DIM)).type(torch.float32)\n",
    "\n",
    "    lhs = torch.norm(nm((U @ x).unsqueeze(0)), dim=-1)\n",
    "    rhs = torch.norm(nm(x.unsqueeze(0)), dim=-1)\n",
    "    \n",
    "    vals.append(abs((lhs - rhs).item()))\n",
    "\n",
    "# confirm\n",
    "mean, std = np.mean(vals), np.std(vals)\n",
    "if mean < 1e-4 and std < 1e-4:\n",
    "    print('seems ok')\n",
    "elif std >= 1e-4:\n",
    "    uhoh('fluctuating stuff, std={}'.format(std))\n",
    "else:\n",
    "    uhoh('we were pretty sure LHS-RHS={}'.format(mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f4132b",
   "metadata": {},
   "source": [
    "# Testing Norm Monotonicity\n",
    "\n",
    "The following proposition is the reason for this notebook:\n",
    "\n",
    "### Proposition: \n",
    "#### *Suppose that $f$ is $L$-subhomogenous for some $L > 0$ and rotationally symmetric in the way described above. Then, $f$ is norm-monotonic.*\n",
    "\n",
    "#### Proof\n",
    "Let $x, y \\in \\mathcal{H}_1$ be arbitrary. Note that we can certainly find some unitary operator $U$ for which\n",
    "        $$y = \\left(\\frac{||y||_{\\mathcal{H}_1}}{||x||_{\\mathcal{H}_1}}\\right)U(x)$$\n",
    "Let $\\gamma :=\\frac{||y||_{\\mathcal{H}_1}}{||x||_{\\mathcal{H}_1}} > 0$. Then, $y = \\gamma  U(x)$. Since $f$ is $L$-subhomogenous, we can readily see that\n",
    "$$\\frac{||f(y)||_{\\mathcal{H}_2}}{||f(x)||_{\\mathcal{H}_2}} = \\frac{||f(\\gamma U(x)||_{\\mathcal{H}_2}}{||f(x)||_{\\mathcal{H}_2}} \\leq \\frac{\\gamma^L ||f(U(x))||_{\\mathcal{H}_2}}{||f(x)||_{\\mathcal{H}_2}}$$ \n",
    "By the rotational symmetry of $f$ we know that $||f(U(x))||_{\\mathcal{H}_2} = ||f(x)||_{\\mathcal{H}_2}$, from which we find\n",
    "    $$\\frac{||f(y)||_{\\mathcal{H}_2}}{||f(x)||_{\\mathcal{H}_2}} \\leq \\gamma^L = \\left(\\frac{||y||_{\\mathcal{H}_1}}{||x||_{\\mathcal{H}_1}}\\right)^L$$\n",
    "This means that if $||f(y)||_{\\mathcal{H}_2} > ||f(x)||_{\\mathcal{H}_2}$, we immediately see that $\\gamma > 1$ and therefore that $||y||_1 > ||x||_{\\mathcal{H}_1}$. This is the condition for strict norm-monotonicity. $\\blacksquare$\n",
    "\n",
    "\n",
    "Since our function $f$ in this notebook satisfies the two above properties ($L$-subhomogeneity and rotational symmetry) w.r.t. the $||\\cdot||_{\\mathcal{H}_2}$ norm, the proposition should guarantee strict norm monotonicity. This is what we will test for below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dd668a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:09<00:00, 1051.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seems ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_trials = 10000\n",
    "\n",
    "# run trials\n",
    "for _ in tqdm.trange(num_trials):\n",
    "    nm = NM(IN_DIM, OUT_DIM, DEPTH)\n",
    "\n",
    "    # sample two random vectors x and y\n",
    "    x = torch.randn(1, IN_DIM)\n",
    "    y = torch.randn(1, IN_DIM)\n",
    "    fx = nm(x).squeeze(0)\n",
    "    fy = nm(y).squeeze(0)\n",
    "    \n",
    "    n_x, n_y, n_fx, n_fy = map(lambda t: torch.norm(t).item(), [x, y, fx, fy])\n",
    "    if n_fx < n_fy: assert n_x < n_y, (n_x, n_y, n_fx, n_fy)\n",
    "    elif n_fx > n_fy: assert n_x > n_y, (n_x, n_y, n_fx, n_fy)\n",
    "    elif n_fx > 0 or n_fy > 0: assert abs(n_x - n_y) < 1e-4, (n_x, n_y, n_fx, n_fy)\n",
    "        \n",
    "print('seems ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e250f181",
   "metadata": {},
   "source": [
    "# But is it a good NN?\n",
    "Ok, so we have crafted a general and strictly norm-monotonic neural network architecture. But is it expressive enough to learn?\n",
    "\n",
    "To answer this question we will use it to do some good ol' contrastive learning on MNIST and compare it with an equivalently-sized regular MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b99a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from pytorch_metric_learning.losses import SupConLoss\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "loss_fn = SupConLoss()\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Lambda(lambda t: torch.flatten(t, start_dim=0))\n",
    "])\n",
    "train_dataset = torchvision.datasets.MNIST('../../data', train=False, transform=transform)\n",
    "val_dataset = torchvision.datasets.MNIST('../../data', train=False, transform=transform)\n",
    "train_dl = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "val_dl = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "def train(model, opt, num_epochs: int):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for _ in tqdm.trange(num_epochs):\n",
    "        # train\n",
    "        epoch_train_losses = []\n",
    "        for x, y in train_dl:\n",
    "            opt.zero_grad()\n",
    "            emb = model(x)\n",
    "            loss = loss_fn(emb, y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            epoch_train_losses.append(loss.item())\n",
    "        train_losses.append(np.mean(epoch_train_losses))\n",
    "        \n",
    "        # val\n",
    "        epoch_val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_dl:\n",
    "                emb = model(x)\n",
    "                loss = loss_fn(emb, y)\n",
    "                epoch_val_losses.append(loss.item())\n",
    "            val_losses.append(np.mean(epoch_val_losses))\n",
    "            \n",
    "    return train_losses, val_losses\n",
    "         \n",
    "    \n",
    "seed = None\n",
    "dim = 128\n",
    "depth = 8\n",
    "num_epochs = 30\n",
    "activation = nn.ReLU\n",
    "set_seed(seed)\n",
    "\n",
    "# make models\n",
    "models = {\n",
    "    'ours ReLU': NM(in_dim=28 * 28, out_dim=dim, depth=depth, activation=nn.ReLU),\n",
    "    'ours Leaky': NM(in_dim=28 * 28, out_dim=dim, depth=depth, activation=nn.LeakyReLU),\n",
    "    'default ReLU': MLP(layer_dims=exponential_linspace_int(28 * 28, dim, depth), activation=nn.ReLU),\n",
    "    'default Leaky': MLP(layer_dims=exponential_linspace_int(28 * 28, dim, depth), activation=nn.LeakyReLU)\n",
    "}\n",
    "\n",
    "# run trials\n",
    "results = {'train_losses': {}, 'val_losses': {}}\n",
    "for k, model in models.items():\n",
    "    print(k)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=0.004)\n",
    "    t, v = train(model, opt, num_epochs)\n",
    "    results['train_losses'][k] = t\n",
    "    results['val_losses'][k] = v\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "for k in models.keys():\n",
    "    t, v = results['train_losses'][k], results['val_losses'][k]\n",
    "    ax[0].plot(range(len(t)), t, label=k)\n",
    "    ax[1].plot(range(len(v)), v, label=k)\n",
    "    \n",
    "_ax = ax[0]; _ax.legend(); _ax.set_xlabel('epoch'); _ax.set_ylabel('loss'); _ax.set_title('train losses');\n",
    "_ax = ax[1]; _ax.legend(); _ax.set_xlabel('epoch'); _ax.set_ylabel('loss'); _ax.set_title('val losses');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc972fd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# check embedding spaces and t-SNE plot what's going on\n",
    "fig, ax = plt.subplots(len(models), 1, figsize=(8, 8 * len(models)))\n",
    "N = 1000 # how many points to actually plot in each fig, val dataset is 10k full\n",
    "\n",
    "for _ax, (k, model) in zip(ax, models.items()):\n",
    "    print(k)\n",
    "    embs = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_dl:\n",
    "            embs.append(model(x).squeeze(0).data.numpy())\n",
    "            labels.append(y.item())\n",
    "    X = np.stack(embs, axis=0)\n",
    "    idxs = np.random.permutation(len(X))[:N] \n",
    "    X = X[idxs]\n",
    "    labels = [labels[i] for i in idxs]\n",
    "\n",
    "    tsne = TSNE(n_components=2, learning_rate='auto',\n",
    "                       init='random').fit_transform(X)\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df['tsne-one'] = tsne[:, 0]\n",
    "    df['tsne-two'] = tsne[:, 1]\n",
    "    df['label'] = labels\n",
    "    sns.scatterplot(\n",
    "        x=\"tsne-one\", y=\"tsne-two\",\n",
    "        hue=\"label\",\n",
    "        palette=sns.color_palette(\"hls\", 10),  # 21 different speakers in the first 10k data points\n",
    "        data=df,\n",
    "        legend=\"full\",\n",
    "        alpha=0.3,\n",
    "        ax=_ax\n",
    "    )\n",
    "    _ax.set_title('{} Embedding Space for MNIST Val Dataset'.format(k))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1fce2e",
   "metadata": {},
   "source": [
    "# Testing LQR and H_inf Controllers with Norm-Monotonic Lifter\n",
    "The entire reason we were interested in neural nets with this property is to ensure that minimizing state norm of the lifted state corresponds to minimizing norm of the inputs!. Lifters with this property technically form an LDS with quadratic costs, lending themselves to provable control via LQR (optimal control, $H_{\\infty}$ (robust control), and perhaps even GPC!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fcdc101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from abc import abstractmethod\n",
    "import inspect\n",
    "from typing import Tuple\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from extravaganza.models import MLP\n",
    "from extravaganza.lifters import Lifter\n",
    "from extravaganza.sysid import SysID\n",
    "from extravaganza.utils import exponential_linspace_int, sample, set_seed, jkey, opnorm, dare_gain, get_classname, least_squares\n",
    "\n",
    "class NMLift(Lifter, SysID):\n",
    "    def __init__(self,\n",
    "                 hh: int,\n",
    "                 control_dim: int,\n",
    "                 state_dim: int,\n",
    "                 depth: int,\n",
    "                 scale: float = 0.1,\n",
    "                 lift_lr: float = 0.001,\n",
    "                 sysid_lr: float = 0.001,\n",
    "                 buffer_maxlen: int = int(1e9),\n",
    "                 batch_size: int = 64,\n",
    "                 num_epochs: int = 20,  # number of epochs over the buffer to use when querying `sysid()` or `dynamics()` for first time\n",
    "                 seed: int = None):\n",
    "        \n",
    "        set_seed(seed)\n",
    "        super().__init__(hh, control_dim, state_dim, seed)\n",
    "        \n",
    "        self.hh = hh\n",
    "        self.control_dim = control_dim\n",
    "        self.state_dim = state_dim\n",
    "        self.scale = scale\n",
    "        \n",
    "        self.buffer = deque(maxlen=buffer_maxlen)\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # to compute lifted states which hopefully respond linearly to the controls\n",
    "        flat_dim = hh + control_dim * hh\n",
    "        self.lift_model = NM(flat_dim, self.state_dim, depth, seed=seed).train().float() # TODO add layernorm to NM\n",
    "        self.lift_opt = torch.optim.Adam(self.lift_model.parameters(), lr=lift_lr)\n",
    "    \n",
    "        # to estimate linear dynamics of lifted states\n",
    "        self.A = torch.nn.Parameter(0.99 * torch.eye(self.state_dim, dtype=torch.float32))  # for stability purposes :)\n",
    "        self.B = torch.nn.Parameter(torch.from_numpy(np.array(sample(jkey(), shape=(self.state_dim, self.control_dim), sampling_method='sphere'))))\n",
    "        self.sysid_opt = torch.optim.Adam([self.A, self.B], lr=sysid_lr)\n",
    "#         logging.warning('(LIFTER): note that right now we are NOT LEARNING B!!')\n",
    "        \n",
    "        self.t = 1\n",
    "        self.trained = False\n",
    "        pass\n",
    "    \n",
    "    def forward(self, \n",
    "                cost_history: torch.Tensor,\n",
    "                control_history: torch.Tensor) -> torch.Tensor:  # so that we don't need to return a jnp.ndarray\n",
    "        inp = torch.cat((cost_history.reshape(-1, self.hh), control_history.reshape(-1, self.control_dim * self.hh)), dim=-1)\n",
    "        state = self.lift_model(inp)\n",
    "        return state\n",
    "    \n",
    "    def map(self,\n",
    "            cost_history: jnp.ndarray,\n",
    "            control_history: jnp.ndarray) -> jnp.ndarray:\n",
    "        \"\"\"\n",
    "        maps histories to lifted states. it's in pytorch rn, but that can change\n",
    "        \"\"\"\n",
    "        assert cost_history.shape == (self.hh,)\n",
    "        assert control_history.shape == (self.hh, self.control_dim)\n",
    "        \n",
    "        # convert to pytorch tensors and back rq  # TODO remove this eventually, making everything in jax\n",
    "        with torch.no_grad():\n",
    "            cost_history, control_history = map(lambda j_arr: torch.from_numpy(np.array(j_arr)).unsqueeze(0), [cost_history, control_history])\n",
    "            state = jnp.array(self.forward(cost_history, control_history).squeeze(0).data.numpy())\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def perturb_control(self,\n",
    "                        state: jnp.ndarray,\n",
    "                        control: jnp.ndarray = None):\n",
    "        # assert state.shape == (self.state_dim,)\n",
    "        eps = sample(jkey(), (self.control_dim,))  # random direction\n",
    "        control = self.scale * eps\n",
    "        self.t += 1\n",
    "        return control\n",
    "    \n",
    "    def train(self):\n",
    "        logging.info('({}): training!'.format(get_classname(self)))\n",
    "            \n",
    "        # prepare dataloader\n",
    "        from torch.utils.data import DataLoader, TensorDataset\n",
    "        controls = []\n",
    "        prev_cost_history = []\n",
    "        prev_control_history = []\n",
    "        cost_history = []\n",
    "        control_history = []\n",
    "        for prev_histories, histories in self.buffer:  # append em all\n",
    "            lists = [controls, prev_cost_history, prev_control_history, cost_history, control_history]\n",
    "            vals = [histories[1][-1], *prev_histories, *histories]\n",
    "            for l, v in zip(lists, vals): l.append(torch.from_numpy(np.array(v)))\n",
    "        dataset = TensorDataset(*map(lambda l: torch.stack(l, dim=0), \n",
    "                                     [prev_cost_history, prev_control_history, cost_history, control_history, controls]))\n",
    "        dl = DataLoader(dataset, batch_size=self.batch_size, shuffle=True, drop_last=True)\n",
    "        \n",
    "        # learn lifter\n",
    "        losses = []\n",
    "        for t in tqdm.trange(self.num_epochs):\n",
    "            for prev_cost_history, prev_control_history, cost_history, control_history, controls in dl:\n",
    "                \n",
    "                # compute disturbance\n",
    "                prev_state = self.forward(prev_cost_history, prev_control_history)\n",
    "                state = self.forward(cost_history, control_history)\n",
    "                pred = self.A.expand(self.batch_size, self.state_dim, self.state_dim) @ prev_state.unsqueeze(-1) + \\\n",
    "                       self.B.expand(self.batch_size, self.state_dim, self.control_dim) @ controls.unsqueeze(-1)\n",
    "                diff = state - pred.squeeze(-1)\n",
    "                \n",
    "                # update\n",
    "                self.lift_opt.zero_grad()\n",
    "                self.sysid_opt.zero_grad()\n",
    "                \n",
    "                # compute loss\n",
    "                LAMBDA_STATE_NORM, LAMBDA_STABILITY, LAMBDA_B_NORM = 1e-5, 1e-5, 1e-5\n",
    "                norm = torch.norm(state)\n",
    "                state_norm = (1 / (norm + 1e-8)) + 1e-2 * norm if LAMBDA_STATE_NORM > 0 else 0.\n",
    "                stability = opnorm(self.A - self.B @ dare_gain(self.A, self.B, torch.eye(self.state_dim), torch.eye(self.control_dim))) if LAMBDA_STABILITY > 0 else 0.\n",
    "                B_norm = 1 / (torch.norm(self.B) + 1e-8) if LAMBDA_B_NORM > 0 else 0.\n",
    "                loss = torch.norm(diff) + LAMBDA_STATE_NORM * state_norm + LAMBDA_STABILITY * stability + LAMBDA_B_NORM * B_norm\n",
    "                loss.backward()\n",
    "                self.lift_opt.step()\n",
    "                self.sysid_opt.step()\n",
    "                \n",
    "                losses.append(loss.item())\n",
    "                \n",
    "            print_every = 25\n",
    "            if t % print_every == 0 or t == self.num_epochs - 1: \n",
    "                logging.info('({}) \\tmean loss for past {} epochs was {}'.format(get_classname(self), print_every, np.mean(losses[-print_every:])))\n",
    "        \n",
    "        # sysid\n",
    "        states = []\n",
    "        controls = []\n",
    "        for prev_histories, histories in self.buffer:\n",
    "            states.append(self.map(*prev_histories))\n",
    "            controls.append(histories[1][-1])\n",
    "        states, controls = np.array(states), np.array(controls)\n",
    "\n",
    "        A, B = least_squares(states, controls)\n",
    "        A, B = torch.from_numpy(np.array(A)), torch.from_numpy(np.array(B))\n",
    "        \n",
    "        def calc_loss(A, B):\n",
    "            with torch.no_grad():\n",
    "                l = 0.\n",
    "                for prev_cost_history, prev_control_history, cost_history, control_history, controls in dl:\n",
    "                    # compute disturbance\n",
    "                    prev_state = self.forward(prev_cost_history, prev_control_history)\n",
    "                    state = self.forward(cost_history, control_history)\n",
    "                    pred = A.expand(self.batch_size, self.state_dim, self.state_dim) @ prev_state.unsqueeze(-1) + \\\n",
    "                        B.expand(self.batch_size, self.state_dim, self.control_dim) @ controls.unsqueeze(-1)\n",
    "                    diff = state - pred.squeeze(-1)\n",
    "                    \n",
    "                    # compute loss\n",
    "                    loss = torch.norm(diff)\n",
    "                    l += loss.item()\n",
    "                l /= len(dl)\n",
    "                return l\n",
    "\n",
    "            print('|A|', opnorm(self.A), \n",
    "              opnorm(A))\n",
    "        print('|A-BK|', opnorm(self.A - self.B @ dare_gain(self.A, self.B, torch.eye(self.state_dim), torch.eye(self.control_dim))), \n",
    "              opnorm(A - B @ dare_gain(A, B, torch.eye(self.state_dim), torch.eye(self.control_dim))))\n",
    "        print('|B|', torch.norm(self.B), torch.norm(B))\n",
    "        print('old', calc_loss(self.A, self.B), 'new', calc_loss(A, B))\n",
    "#         self.A, self.B = A, B\n",
    "        logging.warning('(LIFTER): note that right now we are USING THE OLD SYSID FOR A, B!!')\n",
    "            \n",
    "        self.trained = True\n",
    "        return losses\n",
    "    \n",
    "    def sysid(self):\n",
    "        if not self.trained:\n",
    "            self.losses = self.train()\n",
    "            A, B = jnp.array(self.A.data.numpy()), jnp.array(self.B.data.numpy())\n",
    "            logging.info('({}) ||A||_op = {}     ||B||_F {}'.format(get_classname(self), opnorm(A), jnp.linalg.norm(B, 'fro')))\n",
    "            return A, B\n",
    "        \n",
    "        A, B = jnp.array(self.A.data.numpy()), jnp.array(self.B.data.numpy())\n",
    "        return A, B\n",
    "    \n",
    "    def update(self, \n",
    "               prev_histories: Tuple[jnp.ndarray, jnp.ndarray], \n",
    "               histories: Tuple[jnp.ndarray, jnp.ndarray]) -> float:\n",
    "        self.buffer.append((prev_histories, histories))  # add the transition\n",
    "        return 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576b5ca4",
   "metadata": {},
   "source": [
    "### Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6be8c730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▊| 9987/10000 [00:28<00:00, 338.25it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▊| 9987/10000 [00:41<00:00, 338.25it/s]\u001b[A\n",
      " 17%|██████████████████▊                                                                                              | 1/6 [04:48<24:00, 288.15s/it]\u001b[A\n",
      " 33%|█████████████████████████████████████▋                                                                           | 2/6 [09:58<20:04, 301.06s/it]\u001b[A\n",
      " 50%|████████████████████████████████████████████████████████▌                                                        | 3/6 [15:33<15:33, 311.31s/it]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 9998/10000 [16:05<00:00, 10.35it/s]\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "Failed to find a finite solution.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 169\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m experiment_args\n\u001b[1;32m    168\u001b[0m experiment \u001b[38;5;241m=\u001b[39m Experiment(name)\n\u001b[0;32m--> 169\u001b[0m stats \u001b[38;5;241m=\u001b[39m \u001b[43mexperiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_experiment_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/extravaganza/env/lib/python3.10/site-packages/extravaganza/experiments.py:50\u001b[0m, in \u001b[0;36mExperiment.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_args \u001b[38;5;241m=\u001b[39m get_args\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 50\u001b[0m     experiment_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_experiment(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mexperiment_args)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[12], line 110\u001b[0m, in \u001b[0;36mget_experiment_args\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m     system\u001b[38;5;241m.\u001b[39mreset(reset_seed)\n\u001b[1;32m    109\u001b[0m cost, state \u001b[38;5;241m=\u001b[39m system\u001b[38;5;241m.\u001b[39minteract(control)  \u001b[38;5;66;03m# state will be `None` for unobservable systems\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m control \u001b[38;5;241m=\u001b[39m \u001b[43mcontroller\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_control\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(state, jnp\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39many(jnp\u001b[38;5;241m.\u001b[39misnan(state))) \u001b[38;5;129;01mor\u001b[39;00m (cost \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1e20\u001b[39m):\n\u001b[1;32m    113\u001b[0m     logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(EXPERIMENT): state \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m or cost \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m diverged\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(state, cost))\n",
      "File \u001b[0;32m~/Desktop/extravaganza/env/lib/python3.10/site-packages/extravaganza/controllers.py:270\u001b[0m, in \u001b[0;36mLiftedBPC.get_control\u001b[0;34m(self, cost, state)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_control\u001b[39m(\u001b[38;5;28mself\u001b[39m, cost: \u001b[38;5;28mfloat\u001b[39m, state: jnp\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m jnp\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m--> 270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcost\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/extravaganza/env/lib/python3.10/site-packages/extravaganza/controllers.py:189\u001b[0m, in \u001b[0;36mLiftedBPC.__call__\u001b[0;34m(self, cost)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_K_from_sysid \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mT0:  \u001b[38;5;66;03m# get the K from sysid every so often\u001b[39;00m\n\u001b[1;32m    188\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) copying the K from \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(get_classname(\u001b[38;5;28mself\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msysid))\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mK \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msysid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_lqr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m# 3. compute disturbance\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/extravaganza/env/lib/python3.10/site-packages/extravaganza/sysid.py:100\u001b[0m, in \u001b[0;36mSysID.get_lqr\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_lqr\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 100\u001b[0m     A, B \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msysid\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# make sure we have an estimate first\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;66;03m# compute stabilizing controller for squared costs\u001b[39;00m\n\u001b[1;32m    103\u001b[0m     Q \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39meye(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_dim); logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) solving DARE with unconstrained Q\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(get_classname(\u001b[38;5;28mself\u001b[39m)))\n",
      "Cell \u001b[0;32mIn[2], line 181\u001b[0m, in \u001b[0;36mNMLift.sysid\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msysid\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrained:\n\u001b[0;32m--> 181\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m         A, B \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mA\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnumpy()), jnp\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mB\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m    183\u001b[0m         logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) ||A||_op = \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m     ||B||_F \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(get_classname(\u001b[38;5;28mself\u001b[39m), opnorm(A), jnp\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(B, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfro\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n",
      "Cell \u001b[0;32mIn[2], line 126\u001b[0m, in \u001b[0;36mNMLift.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    124\u001b[0m norm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnorm(state)\n\u001b[1;32m    125\u001b[0m state_norm \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (norm \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-8\u001b[39m)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-2\u001b[39m \u001b[38;5;241m*\u001b[39m norm \u001b[38;5;28;01mif\u001b[39;00m LAMBDA_STATE_NORM \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.\u001b[39m\n\u001b[0;32m--> 126\u001b[0m stability \u001b[38;5;241m=\u001b[39m opnorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mA \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mB \u001b[38;5;241m@\u001b[39m \u001b[43mdare_gain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meye\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meye\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontrol_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mif\u001b[39;00m LAMBDA_STABILITY \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.\u001b[39m\n\u001b[1;32m    127\u001b[0m B_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (torch\u001b[38;5;241m.\u001b[39mnorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mB) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-8\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m LAMBDA_B_NORM \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.\u001b[39m\n\u001b[1;32m    128\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnorm(diff) \u001b[38;5;241m+\u001b[39m LAMBDA_STATE_NORM \u001b[38;5;241m*\u001b[39m state_norm \u001b[38;5;241m+\u001b[39m LAMBDA_STABILITY \u001b[38;5;241m*\u001b[39m stability \u001b[38;5;241m+\u001b[39m LAMBDA_B_NORM \u001b[38;5;241m*\u001b[39m B_norm\n",
      "File \u001b[0;32m~/Desktop/extravaganza/env/lib/python3.10/site-packages/extravaganza/utils.py:118\u001b[0m, in \u001b[0;36mdare_gain\u001b[0;34m(A, B, Q, R)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdare_gain\u001b[39m(A, B, Q, R):\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(A, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 118\u001b[0m         P \u001b[38;5;241m=\u001b[39m \u001b[43mRiccati\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m         K \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39minv(B\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m P \u001b[38;5;241m@\u001b[39m B \u001b[38;5;241m+\u001b[39m R) \u001b[38;5;241m@\u001b[39m (B\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m P \u001b[38;5;241m@\u001b[39m A)  \u001b[38;5;66;03m# compute LQR gain\u001b[39;00m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(A, np\u001b[38;5;241m.\u001b[39mndarray):\n",
      "File \u001b[0;32m~/Desktop/extravaganza/env/lib/python3.10/site-packages/torch/autograd/function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/extravaganza/env/lib/python3.10/site-packages/extravaganza/utils.py:356\u001b[0m, in \u001b[0;36mRiccati.forward\u001b[0;34m(ctx, A, B, Q, R)\u001b[0m\n\u001b[1;32m    353\u001b[0m R \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m (R \u001b[38;5;241m+\u001b[39m R\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    354\u001b[0m Rtemp \u001b[38;5;241m=\u001b[39m R\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m--> 356\u001b[0m P \u001b[38;5;241m=\u001b[39m \u001b[43msolve_discrete_are\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAtemp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBtemp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQtemp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRtemp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m P \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(P)\u001b[38;5;241m.\u001b[39mtype(A\u001b[38;5;241m.\u001b[39mtype())\n\u001b[1;32m    359\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(P, A, B, Q, R) \u001b[38;5;66;03m#Save variables for backwards pass\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/extravaganza/env/lib/python3.10/site-packages/scipy/linalg/_solvers.py:716\u001b[0m, in \u001b[0;36msolve_discrete_are\u001b[0;34m(a, b, q, r, e, s, balanced)\u001b[0m\n\u001b[1;32m    713\u001b[0m up, ul, uu \u001b[38;5;241m=\u001b[39m lu(u00)\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mcond(uu) \u001b[38;5;241m<\u001b[39m np\u001b[38;5;241m.\u001b[39mspacing(\u001b[38;5;241m1.\u001b[39m):\n\u001b[0;32m--> 716\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed to find a finite solution.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    718\u001b[0m \u001b[38;5;66;03m# Exploit the triangular structure\u001b[39;00m\n\u001b[1;32m    719\u001b[0m x \u001b[38;5;241m=\u001b[39m solve_triangular(ul\u001b[38;5;241m.\u001b[39mconj()\u001b[38;5;241m.\u001b[39mT,\n\u001b[1;32m    720\u001b[0m                      solve_triangular(uu\u001b[38;5;241m.\u001b[39mconj()\u001b[38;5;241m.\u001b[39mT,\n\u001b[1;32m    721\u001b[0m                                       u10\u001b[38;5;241m.\u001b[39mconj()\u001b[38;5;241m.\u001b[39mT,\n\u001b[1;32m    722\u001b[0m                                       lower\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m    723\u001b[0m                      unit_diagonal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    724\u001b[0m                      )\u001b[38;5;241m.\u001b[39mconj()\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mdot(up\u001b[38;5;241m.\u001b[39mconj()\u001b[38;5;241m.\u001b[39mT)\n",
      "\u001b[0;31mLinAlgError\u001b[0m: Failed to find a finite solution."
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s: %(message)s', level=logging.INFO)  # set level to INFO for wordy\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from extravaganza.dynamical_systems import LDS\n",
    "\n",
    "from extravaganza.controllers import LiftedBPC, LambdaController\n",
    "from extravaganza.lifters import NoLift, RandomLift, LearnedLift\n",
    "from extravaganza.sysid import SysID\n",
    "from extravaganza.controllers import LQR, HINF, BPC, GPC, RBPC\n",
    "from extravaganza.rescalers import ADAM, D_ADAM, DoWG\n",
    "from extravaganza.utils import ylim, render, append, opnorm, dare_gain, least_squares\n",
    "from extravaganza.experiments import Experiment\n",
    "\n",
    "# seeds for randomness. setting to `None` uses random seeds\n",
    "SYSTEM_SEED = 23\n",
    "CONTROLLER_SEED = None\n",
    "LIFTER_AND_SYSID_SEED = None\n",
    "\n",
    "name = 'lds_constant'\n",
    "filename = '../logs/{}.pkl'.format(name)\n",
    "\n",
    "def get_experiment_args():\n",
    "    # --------------------------------------------------------------------------------------\n",
    "    # ------------------------    EXPERIMENT HYPERPARAMETERS    ----------------------------\n",
    "    # --------------------------------------------------------------------------------------\n",
    "\n",
    "    num_trials = 1\n",
    "    T = 5000  # total timesteps\n",
    "    T0 = 10000  # number of timesteps to just sysid for our methods\n",
    "    reset_condition = lambda t: False  # how often to reset the system\n",
    "    use_multiprocessing = False\n",
    "    render_every = None\n",
    "\n",
    "    # --------------------------------------------------------------------------------------\n",
    "    # --------------------------    SYSTEM HYPERPARAMETERS    ------------------------------\n",
    "    # --------------------------------------------------------------------------------------\n",
    "\n",
    "    du = 1  # control dim\n",
    "    ds = 1  # state dim\n",
    "\n",
    "    disturbance_type = 'constant'\n",
    "    cost_fn = 'quad'\n",
    "\n",
    "    make_system = lambda : LDS(ds, du, disturbance_type, cost_fn, seed=SYSTEM_SEED)\n",
    "\n",
    "    # --------------------------------------------------------------------------------------\n",
    "    # ------------------------    LIFT/SYSID HYPERPARAMETERS    ----------------------------\n",
    "    # --------------------------------------------------------------------------------------\n",
    "\n",
    "    sysid_method = 'regression'\n",
    "    sysid_scale = 1.\n",
    "\n",
    "    learned_lift_args = {\n",
    "        'lift_lr': 0.004,\n",
    "        'sysid_lr': 0.004,\n",
    "        'depth': 12,\n",
    "        'buffer_maxlen': int(1e6),\n",
    "        'num_epochs': 6,\n",
    "        'batch_size': 64,\n",
    "        'seed': LIFTER_AND_SYSID_SEED\n",
    "    }\n",
    "\n",
    "    # --------------------------------------------------------------------------------------\n",
    "    # ------------------------    CONTROLLER HYPERPARAMETERS    ----------------------------\n",
    "    # --------------------------------------------------------------------------------------\n",
    "\n",
    "    h = 5  # controller memory length (# of w's to use on inference)\n",
    "    hh = 15  # history length of the cost/control histories\n",
    "    lift_dim = 64  # dimension to lift to\n",
    "\n",
    "    lifted_bpc_args = {\n",
    "        'h': h,\n",
    "        'method': 'REINFORCE',\n",
    "        'initial_scales': (0.0, 0., 0.0),  # M, M0, K   (uses M0's scale for REINFORCE)\n",
    "        'rescalers': (lambda : 0., lambda : 0., lambda : 0.),\n",
    "        'T0': T0,\n",
    "    #     'bounds': None,\n",
    "        'initial_u': jnp.zeros(du),\n",
    "        'decay_scales': False,\n",
    "        'use_tanh': False,\n",
    "        'use_K_from_sysid': True,\n",
    "        'seed': CONTROLLER_SEED\n",
    "    }\n",
    "    none = LiftedBPC(lifter=NoLift(hh, du, LIFTER_AND_SYSID_SEED), sysid=SysID(sysid_method, du, hh, sysid_scale, LIFTER_AND_SYSID_SEED), **lifted_bpc_args)\n",
    "    learned = LiftedBPC(lifter=LearnedLift(hh, du, lift_dim, scale=sysid_scale, **learned_lift_args), **lifted_bpc_args)\n",
    "    nm = LiftedBPC(lifter=NMLift(hh, du, lift_dim, scale=sysid_scale, **learned_lift_args), **lifted_bpc_args)\n",
    "    controllers = {\n",
    "#         'None': none, \n",
    "#         'Learned': learned, \n",
    "        'NM': nm\n",
    "    }\n",
    "\n",
    "    dynamics = {'g.t.': (None, None)}\n",
    "    for k, controller in controllers.items(): # interact in order to perform sysid\n",
    "        # make system and get initial control\n",
    "        system = make_system()\n",
    "        control = controller.initial_control if hasattr(controller, 'initial_control') else jnp.zeros(du) \n",
    "        print(k)\n",
    "        for t in tqdm.trange(T0):\n",
    "            if reset_condition(t):\n",
    "                logging.info('(EXPERIMENT): reset!')\n",
    "                system.reset(reset_seed)\n",
    "                    \n",
    "            cost, state = system.interact(control)  # state will be `None` for unobservable systems\n",
    "            control = controller.get_control(cost, state)\n",
    "                    \n",
    "            if (isinstance(state, jnp.ndarray) and jnp.any(jnp.isnan(state))) or (cost > 1e20):\n",
    "                logging.error('(EXPERIMENT): state {} or cost {} diverged'.format(state, cost))\n",
    "                assert False\n",
    "                \n",
    "        A, B = controller.sysid.sysid()\n",
    "        dynamics[k] = (A, B)\n",
    "\n",
    "    # test \n",
    "    make_controllers = {}\n",
    "    for k, (A, B) in dynamics.items():\n",
    "        \n",
    "        def init(controller):\n",
    "            controller._cost_history = jnp.zeros(hh,)\n",
    "            controller._control_history = jnp.zeros((hh, du))\n",
    "            pass\n",
    "        \n",
    "        def get_control(key):\n",
    "            def _func(controller, cost, state):\n",
    "                controller._cost_history = append(controller._cost_history, cost)\n",
    "                if key.split()[0] != 'g.t.': state = controllers[key.split()[0]].lifter.map(controller._cost_history, controller._control_history)\n",
    "                controller.stats.update('state_norm', jnp.linalg.norm(state).item(), t=controller.t)\n",
    "                control = controller._controller.get_control(cost, state)\n",
    "                controller._control_history = append(controller._control_history, control)\n",
    "                return control\n",
    "            return _func\n",
    "        \n",
    "        def get_controller(key, controller_class):\n",
    "            def _func(sys):\n",
    "                A, B = dynamics[key]\n",
    "                if A is None: A = sys.A\n",
    "                if B is None: B = sys.B\n",
    "                Q = jnp.eye(A.shape[0])\n",
    "                R = jnp.eye(du)\n",
    "                controller = controller_class(A=A, B=B, Q=Q, R=R, seed=CONTROLLER_SEED)\n",
    "                return LambdaController(controller, init_fn=init, get_control=get_control(key))\n",
    "            return _func\n",
    "        \n",
    "        make_controllers.update({\n",
    "            k + ' LQR': get_controller(k, LQR),\n",
    "            k + ' HINF': get_controller(k, HINF),\n",
    "            k + ' GPC': get_controller(k, GPC),\n",
    "#             k + ' BPC': get_controller(k, BPC),\n",
    "#             k + ' RBPC': get_controller(k, RBPC),\n",
    "        })\n",
    "    experiment_args = {\n",
    "        'make_system': make_system,\n",
    "        'make_controllers': make_controllers,\n",
    "        'num_trials': num_trials,\n",
    "        'T': T, \n",
    "        'reset_condition': reset_condition,\n",
    "        'reset_seed': SYSTEM_SEED,\n",
    "        'use_multiprocessing': use_multiprocessing,\n",
    "        'render_every': render_every,\n",
    "    }   \n",
    "    return experiment_args\n",
    "\n",
    "experiment = Experiment(name)\n",
    "stats = experiment(get_experiment_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6b10b1",
   "metadata": {},
   "source": [
    "### Save Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3364ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save args and stats!  --  note that to save the args, we actually save the `get_args` function. we can print the \n",
    "# #                           source code later to see the hyperparameters we chose\n",
    "# experiment.save(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db978f0c",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956f422f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lds(experiment: Experiment):\n",
    "    assert experiment.stats is not None, 'cannot plot the results of an experiment that hasnt been run'\n",
    "    all_stats = experiment.stats\n",
    "    \n",
    "    # clear plot and calc nrows\n",
    "    plt.clf()\n",
    "    n = 5\n",
    "    nrows = n + (len(all_stats) + 1) // 2\n",
    "    fig, ax = plt.subplots(nrows, 2, figsize=(16, 6 * nrows))\n",
    "\n",
    "    # plot stats\n",
    "    for i, (method, stats) in enumerate(all_stats.items()):\n",
    "#         if 'g.t.' not in method: continue\n",
    "        if stats is None: \n",
    "            logging.warning('{} had no stats'.format(method))\n",
    "            continue\n",
    "        stats.plot(ax[0, 0], 'xs', label=method)\n",
    "#         stats.plot(ax[0, 1], 'ws', label=method)\n",
    "        stats.plot(ax[3, 1], 'us', label=method)\n",
    "        stats.plot(ax[4, 0], 'state_norm', label=method)\n",
    "        if 'costs' in stats:\n",
    "            stats.plot(ax[1, 0], 'avg costs', label=method)\n",
    "            stats.plot(ax[1, 1], 'costs', label=method)\n",
    "        else:\n",
    "            stats.plot(ax[1, 0], 'avg fs', label=method)\n",
    "            stats.plot(ax[1, 1], 'fs', label=method)\n",
    "    \n",
    "        stats.plot(ax[2, 0], '||A||_op', label=method)\n",
    "        stats.plot(ax[2, 1], '||B||_F', label=method)\n",
    "        stats.plot(ax[3, 0], '||A-BK||_op', label=method)\n",
    "        i_ax = ax[n + i // 2, i % 2]\n",
    "        stats.plot(ax[0, 1], 'disturbances', label=method)\n",
    "        stats.plot(i_ax, 'K @ state', label='K @ state')\n",
    "        stats.plot(i_ax, 'M \\cdot w', label='M \\cdot w')\n",
    "        stats.plot(i_ax, 'M0', label='M0')\n",
    "        i_ax.set_title('u decomp for {}'.format(method))\n",
    "        i_ax.legend()\n",
    "\n",
    "    # set titles and legends and limits and such\n",
    "    # (note: `ylim()` is so useful! because sometimes one thing blows up and then autoscale messes up all plots)\n",
    "    _ax = ax[0, 0]; _ax.set_title('position'); _ax.legend()\n",
    "    _ax = ax[0, 1]; _ax.set_title('disturbances'); _ax.legend()\n",
    "    _ax = ax[1, 0]; _ax.set_title('avg costs'); _ax.legend()\n",
    "    _ax = ax[1, 1]; _ax.set_title('costs'); _ax.legend()\n",
    "    \n",
    "    _ax = ax[2, 0]; _ax.set_title('||A||_op'); _ax.legend()\n",
    "    _ax = ax[2, 1]; _ax.set_title('||B||_F'); _ax.legend()\n",
    "    \n",
    "    _ax = ax[3, 0]; _ax.set_title('||A-BK||_op'); _ax.legend()\n",
    "    _ax = ax[3, 1]; _ax.set_title('controls'); _ax.legend()\n",
    "    \n",
    "    _ax = ax[4, 0]; _ax.set_title('state norm'); _ax.legend()\n",
    "    pass\n",
    "\n",
    "plot_lds(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a461ac4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "extravaganza",
   "language": "python",
   "name": "extravaganza"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
