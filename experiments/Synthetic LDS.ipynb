{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "921a8433",
   "metadata": {},
   "source": [
    "# Nonlinearly-Observed LDS\n",
    "\n",
    "## Some background\n",
    "For notation, we write $\\mathcal{S} \\subset \\mathbb{R}^{d_s}, \\mathcal{O} \\subset \\mathbb{R}^{d_o}, \\mathcal{U} \\subset \\mathbb{R}^{d_u}, \\mathcal{Z} \\subset \\mathbb{R}^{d_l}$ to be the state, observation, control, and embedding vector spaces, respectively. In this notebook, we will experiment under the following model: \n",
    "\n",
    "We suppose that there is a ground truth linear dynamical system with states $x_t \\in \\mathcal{S}$, controls $u_t \\in \\mathcal{U}$, and dynamics\n",
    "$$x_{t+1} = A x_t + B u_t$$\n",
    "for some linear transformations $A : \\mathcal{S} \\rightarrow \\mathcal{S}$ and $B: \\mathcal{U} \\rightarrow \\mathcal{S}$. As usual, we require $\\|A\\|_{op} < 1$ for Lyapunov stability. We will be observing this LDS with a nonlinear (_and injective!_) observation function $g: \\mathcal{S} \\rightarrow \\mathcal{O}$. The costs are the usual quadratic $\\ell(x) = \\|x\\|^2$.\n",
    "\n",
    "Of course, this induces (nonlinear) dynamics on the iterates $g(x_t)$ in $\\mathcal{O}$, but we know that there is a ground truth LDS to be recovered. We seek lifting maps $f: \\mathcal{O} \\rightarrow \\mathcal{Z}$ to recover the linear dynamics; morally, we want $f \\circ g$ to be the identity. In fact, it can be a bit more general, see below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ded719",
   "metadata": {},
   "source": [
    "#### <u>Proposition</u>: A lifting map $f$ yields a faithful, linearizing, and cost-preserving representation of the original dynamical system $(A, B)$ <u>if and only if</u> $f \\circ g$ is a linear isometry over $range(A) \\oplus range(B)$.\n",
    "\n",
    "#### <u>Proof</u>: \n",
    "Check the overleaf for the proof, but it goes something like the following. Let $T: \\mathcal{S} \\rightarrow \\mathcal{Z}$ denote $f \\circ g$.\n",
    "\n",
    "($\\implies$) Suppose $f$ yields a faithful, linearizing, and cost-preserving representation. Then, $\\ell(x) = \\ell(Tx)$ for all reachable $x \\in \\mathcal{S}$ implies that $\\|x\\|^2 = \\|Tx\\|^2$, and so $T$ is an isometry where it matters. This also means that $T(0) = 0$. Next, since it's linearizing, there are some $\\tilde{A}: \\mathcal{Z} \\rightarrow \\mathcal{Z}$ and $\\tilde{B}: \\mathcal{U} \\rightarrow \\mathcal{Z}$ for which $$\\tilde{A}Tx_t + \\tilde{B}u_t = Tx_{t+1} = T(Ax_t + Bu_t)$$\n",
    "Since this must hold for all states and controls, we find that $\\tilde{A} T = TA$ and $\\tilde{B} = TB$. Immediately, we see that $T$ must be linear over $range(A) \\oplus range(B)$.\n",
    "\n",
    "($\\impliedby$) Suppose $f$ is a linear isometry over $range(A) \\oplus range(B)$. Then, $\\|x\\|^2 = \\|Tx\\|^2$ for all reachable $x \\in \\mathcal{S}$ implies that $\\ell(x) = \\ell((f \\circ g)x)$, and so $f$ preserves costs. Since $T$ is a linear isometry, then $T$ has trivial kernel and so is injective, meaning the representation is faithful. Lastly, we can confirm that the system is linearized with any dynamics $(\\tilde{A}, \\tilde{B})$ satisfying $T^{-1}\\tilde{A}T = A$ and $\\tilde{B} = TB$. If $d_s = d_l$ and we are lifting to the correct dimension, then $T$ is automatically invertible and these dynamics $(\\tilde{A}, \\tilde{B})$ are unique. $\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ea9d31",
   "metadata": {},
   "source": [
    "## Defining our tests\n",
    "It seems that we get exactly what we want if and only if $f \\circ g$ is a linear isometry over the reachable states. So, it makes sense to test if we can produce a linear isometry under more and more complex/interesting choices of $g$. All that really changes in the different experiments in this notebook is our selection of $g$, the dimensions $d_s, d_u, d_l$, and the system matrices $A, B$. So, we will write the code the rest below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55b17c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created a temporary directory at /var/folders/5m/0xr906c130vdqvkm3g21n6wr0000gn/T/tmphpjii69q\n",
      "INFO: Writing /var/folders/5m/0xr906c130vdqvkm3g21n6wr0000gn/T/tmphpjii69q/_remote_module_non_scriptable.py\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s: %(message)s', level=logging.INFO)  # set level to INFO for wordy\n",
    "import tqdm\n",
    "\n",
    "from mpl_toolkits import mplot3d\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from extravaganza.dynamical_systems import LDS, Gym\n",
    "from extravaganza.observables import Observable, TimeDelayedObservation, Trajectory\n",
    "from extravaganza.sysid import Lifter\n",
    "from extravaganza.utils import summarize_lds, sample, jkey, opnorm, SAMPLING_METHOD\n",
    "\n",
    "# seeds for randomness. setting to `None` uses random seeds\n",
    "SYSTEM_SEED = None\n",
    "SYSID_SEED = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a64d8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(observable, ds: int, du: int, dl: int,\n",
    "        T0: int, reset_every: int, sysid_args):\n",
    "    \"\"\"\n",
    "    fn to gather data and train a sysid.\n",
    "    \"\"\"\n",
    "    # make system\n",
    "    done = False\n",
    "    while not done:\n",
    "        system = LDS(ds, du, 'none', 'quad', seed=SYSTEM_SEED)\n",
    "        A, B = system.A, system.B\n",
    "        done = all(jnp.linalg.eigvals(A) > 0.3) and all(jnp.linalg.svd(B)[1] > 0.3)\n",
    "    print(summarize_lds(A, B))\n",
    "\n",
    "    # make sysid\n",
    "    sysid = Lifter(state_dim=dl, **sysid_args)\n",
    "\n",
    "    # interaction loop\n",
    "    control = jnp.zeros(du)\n",
    "    max_sq_norm = 0.\n",
    "    traj = Trajectory()\n",
    "    for t in tqdm.trange(T0):\n",
    "        if t % reset_every == 0: \n",
    "            system.reset()\n",
    "            system.state = sample(jkey(), (ds,), sampling_method='ball')\n",
    "            sysid.end_trajectory()\n",
    "            traj = Trajectory()\n",
    "        cost, state = system.interact(control)\n",
    "        traj.add_state(cost, state)\n",
    "        obs = observable(traj)\n",
    "        control = sysid.explore(cost, obs)\n",
    "        traj.add_control(control)\n",
    "        max_sq_norm = max(max_sq_norm, jnp.linalg.norm(state) ** 2)\n",
    "    sysid.end_exploration(wordy=True)\n",
    "    return A, B, sysid, max_sq_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "169dafde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(observable, A, B, sysid, max_sq_norm, sampling_method='ball', \n",
    "             N=1000, reset_every=10, hh=10, use_pbar=True):\n",
    "    ds, du = B.shape\n",
    "    do, dl = sysid.obs_dim, sysid.state_dim\n",
    "    states = []\n",
    "    observations = []\n",
    "    embeddings = []\n",
    "    if use_pbar: pbar = tqdm.tqdm(total=N)\n",
    "    while len(embeddings) < N:\n",
    "        if len(embeddings) % reset_every == 0:\n",
    "            traj = Trajectory()\n",
    "            x = (max_sq_norm ** 0.5) * sample(jkey(), (ds,), sampling_method=sampling_method)\n",
    "            for _ in range(hh):\n",
    "                u = sysid.explorer.exploration_scales * sample(jkey(), (du,), sampling_method=SAMPLING_METHOD)\n",
    "                traj.add_control(u)\n",
    "                x = A @ x + B @ u\n",
    "                traj.add_state(jnp.linalg.norm(x) ** 2, x)\n",
    "                \n",
    "        u = sysid.explorer.exploration_scales * sample(jkey(), (du,), sampling_method=SAMPLING_METHOD)\n",
    "        traj.add_control(u)\n",
    "        x = A @ x + B @ u\n",
    "        cost = jnp.linalg.norm(x) ** 2\n",
    "        traj.add_state(cost, x)\n",
    "        obs = observable(traj)\n",
    "        emb = sysid.get_state(obs, cost)        \n",
    "        states.append(x)\n",
    "        observations.append(obs)\n",
    "        embeddings.append(emb)\n",
    "        if use_pbar: pbar.update(1)\n",
    "    if use_pbar: pbar.close()\n",
    "    states, observations, embeddings = map(lambda arr: jnp.stack(arr, axis=0), (states, observations, embeddings))\n",
    "    states, observations, embeddings = states.reshape(N, ds), observations.reshape(N, do), embeddings.reshape(N, dl)\n",
    "    return states, observations, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31961282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(observable, A, B, sysid, max_sq_norm, sampling_method='ball', N=1000, reset_every=10, hh=10):\n",
    "    \"\"\"\n",
    "    We know that if we have succeeded, our embedding composed with our observation function is a linear transformation of the original state.\n",
    "    Below, we regress this transformation, C, and compute some errors with it.\n",
    "    \"\"\"\n",
    "    Ahat, Bhat = sysid.A, sysid.B\n",
    "    ds, du = B.shape\n",
    "    assert du == sysid.control_dim, (du, sysid.control_dim)\n",
    "    do, dl = sysid.obs_dim, sysid.state_dim, \n",
    "    \n",
    "    states, _, embeddings = generate(observable, A, B, sysid, max_sq_norm, sampling_method, N, reset_every, hh)\n",
    "\n",
    "    # -------- FORWARD DIRECTION, showing that T = f \\circ g linearizes ------------------------\n",
    "    print('forward direction:')\n",
    "    C_forward = jnp.linalg.lstsq(states, embeddings, rcond=-1)[0].T\n",
    "    print('\\t||Ahat @ C - C @ A||^2: \\t{}'.format(jnp.linalg.norm(Ahat @ C_forward - C_forward @ A) ** 2))\n",
    "    print('\\t||Bhat - C @ B||^2: \\t\\t{}'.format(jnp.linalg.norm(Bhat - C_forward @ B) ** 2))\n",
    "\n",
    "#     residuals = (C_forward[None] @ states[:, :, None]).squeeze() - embeddings\n",
    "#     residual_norms = jnp.linalg.norm(residuals, axis=-1) ** 2\n",
    "#     state_norms = jnp.linalg.norm(embeddings, axis=-1) ** 2\n",
    "#     print('\\tAvg % error: \\t\\t\\t{}%'.format(100 * jnp.mean(residual_norms / state_norms)))\n",
    "#     plt.scatter(state_norms, residual_norms)\n",
    "#     plt.show()\n",
    "\n",
    "    # -------- REVERSE DIRECTION, showing that T^{-1} = (f \\circ g)^{-1} linearizes -----------------------\n",
    "    print('reverse direction:')\n",
    "    C_reverse = jnp.linalg.lstsq(embeddings, states, rcond=-1)[0].T\n",
    "    print('\\t||A @ C - C @ Ahat||^2: \\t{}'.format(jnp.linalg.norm(A @ C_reverse - C_reverse @ Ahat) ** 2))\n",
    "    print('\\t||B - C @ Bhat||^2: \\t\\t{}'.format(jnp.linalg.norm(B - C_reverse @ Bhat) ** 2))\n",
    "\n",
    "#     residuals = (C_reverse[None] @ embeddings[:, :, None]).squeeze() - states\n",
    "#     residual_norms = jnp.linalg.norm(residuals, axis=-1) ** 2\n",
    "#     state_norms = jnp.linalg.norm(states, axis=-1) ** 2\n",
    "#     print('\\tAvg % error: \\t\\t\\t{}%'.format(100 * jnp.mean(residual_norms / state_norms)))\n",
    "#     plt.scatter(state_norms, residual_norms)\n",
    "#     plt.show()\n",
    "\n",
    "    print()\n",
    "    print('injectivity: \\t||C_reverse @ C_forward - I_ds||^2 / ds^2 = {}'.format(jnp.linalg.norm(C_reverse @ C_forward - jnp.eye(ds)) ** 2 / ds ** 2))\n",
    "    print('surjectivity: \\t||C_forward @ C_reverse - I_dl||^2 / dl^2 = {}'.format(jnp.linalg.norm(C_forward @ C_reverse - jnp.eye(dl)) ** 2 / dl ** 2))\n",
    "    \n",
    "    print()\n",
    "    print('singular values of C_forward: \\t{}'.format(jnp.linalg.svd(C_forward)[1]))\n",
    "    print('singular values of C_reverse: \\t{}'.format(jnp.linalg.svd(C_reverse)[1]))\n",
    "    if hasattr(sysid, 'fmean'): print('sqrt(fmean): \\t\\t{}'.format(sysid.fmean ** 0.5))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e1790c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_prediction(observable, A, B, sysid, max_sq_norm, k=1, sampling_method='ball', hh=10):\n",
    "    \"\"\"\n",
    "    See some prediction instances\n",
    "    Here, 'original' means (f \\circ g)(x_t), 'gt' means (f \\circ g)(x_{t+k}), and \n",
    "        'pred' is the predicted embedding at t+k using linear dynamics given by (Ahat, Bhat).\n",
    "    All norms are in the embedding space\n",
    "    \"\"\"\n",
    "    Ahat, Bhat = sysid.A, sysid.B\n",
    "    \n",
    "    traj = Trajectory()\n",
    "    x = (max_sq_norm ** 0.5) * sample(jkey(), (ds,), sampling_method=sampling_method)\n",
    "    for _ in range(hh):\n",
    "        u = sysid.explorer.exploration_scales * sample(jkey(), (du,), sampling_method=SAMPLING_METHOD)\n",
    "        traj.add_control(u)\n",
    "        x = A @ x + B @ u\n",
    "        traj.add_state(jnp.linalg.norm(x) ** 2, x)\n",
    "    \n",
    "    s = sysid.get_state(observable(traj), jnp.linalg.norm(x) ** 2)\n",
    "    original = s.copy()\n",
    "    for _ in range(k):\n",
    "        u = sysid.explorer.exploration_scales * sample(jkey(), (du,), sampling_method=SAMPLING_METHOD)\n",
    "        traj.add_control(u)\n",
    "        x = A @ x + B @ u\n",
    "        traj.add_state(jnp.linalg.norm(x) ** 2, x)\n",
    "        s = Ahat @ s + Bhat @ u\n",
    "    gt = sysid.get_state(observable(traj), jnp.linalg.norm(x) ** 2)\n",
    "    pred = s.copy()\n",
    "    print(f'||gt - original||={jnp.linalg.norm(gt - original)}\\t||pred - original||={jnp.linalg.norm(pred - original)}\\t||gt - pred||={jnp.linalg.norm(gt - pred)}')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed29a371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_1d(observable, A, B, sysid, max_sq_norm, sampling_method='ball', N=2000, reset_every=10, hh=10):\n",
    "    assert A.shape[0] == 1 and sysid.state_dim == 1\n",
    "\n",
    "    xs, os, zs = generate(observable, A, B, sysid, max_sq_norm, sampling_method, N, reset_every, hh)\n",
    "    idxs = jnp.argsort(xs.squeeze())\n",
    "    xs, os, zs = xs[idxs].reshape(-1), os[idxs].reshape(*os.shape), zs[idxs].reshape(-1)\n",
    "#     zs = zs * (sysid.fmean ** 0.5)\n",
    "\n",
    "    do = os.shape[1]\n",
    "    fig, ax = plt.subplots(nrows=do, ncols=2, figsize=(12, do * 5))\n",
    "    ax = ax.reshape(do, 2)\n",
    "    _xs, _zs = xs, zs\n",
    "    for i in range(do):\n",
    "        _os = os[:, i]\n",
    "        ax[i, 0].plot(_xs, _os, color='r', label='obs(x)')\n",
    "        ax[i, 0].scatter(_xs, _zs, s=4, color='b', label='(embedding \\circ obs)(x)')\n",
    "        ax[i, 1].plot(_os, _xs, color='c', label='x(obs)')\n",
    "        ax[i, 1].scatter(_os, _zs, s=4, color='purple', label='embedding(obs)')\n",
    "\n",
    "        ax[i, 0].set_title('state space')\n",
    "        ax[i, 0].set_xlabel('state')\n",
    "        ax[i, 1].set_xlabel('observation_{}'.format(i))\n",
    "        ax[i, 1].set_title('{}th coord of observation space'.format(i))\n",
    "        ax[i, 0].legend(); ax[i, 1].legend()\n",
    "    plt.show()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d95f0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d(observable, A, B, sysid, max_sq_norm, sampling_method='ball', N=2000, reset_every=10, hh=10):\n",
    "    assert A.shape[0] == 2\n",
    "    assert isinstance(observable, TimeDelayedObservation) and observable.obs_dim == sysid.obs_dim and sysid.obs_dim == hh * (B.shape[1] + 1)\n",
    "\n",
    "    xs, os, zs = generate(observable, A, B, sysid, max_sq_norm, sampling_method, N, reset_every, hh)\n",
    "    #     zs = zs * (sysid.fmean ** 0.5)\n",
    "\n",
    "    C_reverse = jnp.linalg.lstsq(zs, xs, rcond=-1)[0].T\n",
    "\n",
    "    do = os.shape[1]\n",
    "    _xs, _zs = xs, (C_reverse @ zs.reshape(-1, sysid.state_dim, 1)).reshape(-1, 2)\n",
    "    _os = os[:, -1]\n",
    "    \n",
    "    nrows = 3\n",
    "    fig = plt.figure(figsize=(12, nrows * 5))\n",
    "    ax = []\n",
    "    \n",
    "    ax.append(fig.add_subplot(nrows, 2, len(ax) + 1, projection='3d'))\n",
    "    surf = ax[-1].plot_trisurf(_xs[:, 0], _xs[:, 1], _os, cmap=cm.coolwarm)\n",
    "#     fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "    ax[-1].set_title('obs(state)')\n",
    "    ax[-1].set_xlabel('state_0')\n",
    "    ax[-1].set_ylabel('state_1')\n",
    "    ax[-1].set_zlabel('cost')\n",
    "    \n",
    "    ax.append(fig.add_subplot(nrows, 2, len(ax) + 1, projection='3d'))\n",
    "    surf = ax[-1].plot_trisurf(_xs[:, 1], -_xs[:, 0], _os, cmap=cm.coolwarm)\n",
    "    fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "    ax[-1].set_title('obs(state)')\n",
    "    ax[-1].set_xlabel('state_1')\n",
    "    ax[-1].set_ylabel('-state_0')\n",
    "    ax[-1].set_zlabel('cost')\n",
    "    \n",
    "    ax.append(fig.add_subplot(nrows, 2, len(ax) + 1, projection='3d'))\n",
    "    surf = ax[-1].plot_trisurf(_xs[:, 0], _xs[:, 1], _zs[:, 0], cmap=cm.coolwarm)\n",
    "    ax[-1].set_title('embedding(obs(state))_0')\n",
    "    ax[-1].set_xlabel('state_0')\n",
    "    ax[-1].set_ylabel('state_1')\n",
    "    \n",
    "    ax.append(fig.add_subplot(nrows, 2, len(ax) + 1, projection='3d'))\n",
    "    surf = ax[-1].plot_trisurf(_xs[:, 1], -_xs[:, 0], _zs[:, 0], cmap=cm.coolwarm)\n",
    "    fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "    ax[-1].set_title('embedding(obs(state))_0')\n",
    "    ax[-1].set_xlabel('state_1')\n",
    "    ax[-1].set_ylabel('-state_0')\n",
    "    \n",
    "    ax.append(fig.add_subplot(nrows, 2, len(ax) + 1, projection='3d'))\n",
    "    surf = ax[-1].plot_trisurf(_xs[:, 0], _xs[:, 1], _zs[:, 1], cmap=cm.coolwarm)\n",
    "    fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "    ax[-1].set_title('embedding(obs(state))_1')\n",
    "    ax[-1].set_xlabel('state_0')\n",
    "    ax[-1].set_ylabel('state_1')\n",
    "\n",
    "    ax.append(fig.add_subplot(nrows, 2, len(ax) + 1, projection='3d'))\n",
    "    surf = ax[-1].plot_trisurf(_xs[:, 1], -_xs[:, 0], _zs[:, 1], cmap=cm.coolwarm)\n",
    "    fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "    ax[-1].set_title('embedding(obs(state))_1')\n",
    "    ax[-1].set_xlabel('state_1')\n",
    "    ax[-1].set_ylabel('-state_0')\n",
    "    plt.show()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af574251",
   "metadata": {},
   "source": [
    "# (1) Identity observation (as a sanity check)\n",
    "We consider the map $g$ sending $x \\mapsto x$. The dynamics are linear w.r.t. these observations, and so our neural network really only needs to learn the identity map (or any linear map). Let's see how it does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e38487b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # dimensions\n",
    "# ds, du, dl = 4, 4, 4\n",
    "\n",
    "# # define observation fn\n",
    "# g = lambda x: x\n",
    "# observable = lambda traj: g(traj.x[-1])\n",
    "# do = ds\n",
    "\n",
    "# # send it\n",
    "# T0 = 2000\n",
    "# reset_every = 20\n",
    "# exploration_args = {'scales': 0.5, 'avg_len': 5}\n",
    "# sysid_args = {\n",
    "#     'obs_dim': do,\n",
    "#     'control_dim': du,\n",
    "\n",
    "#     'exploration_args': {'random 1.0': exploration_args,\n",
    "# #                          'impulse 0.25': exploration_args,\n",
    "#                         },\n",
    "\n",
    "#     'method': 'nn',\n",
    "#     'AB_method': 'learned',\n",
    "\n",
    "#     'sigma': 0.01,\n",
    "#     'deterministic_encoder': False,\n",
    "#     'depth': 6,\n",
    "#     'num_iters': 24000,\n",
    "#     'batch_size': 128,\n",
    "#     'lifter_lr': 0.001,\n",
    "\n",
    "#     'seed': SYSID_SEED,\n",
    "# }\n",
    "# A, B, sysid, max_sq_norm = run(observable, ds, du, dl, T0, reset_every, sysid_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac7bac3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # check things\n",
    "# summarize(observable, A, B, sysid, max_sq_norm, sampling_method='ball', reset_every=reset_every)\n",
    "# print('\\npredictions:')\n",
    "# for _ in range(5):\n",
    "#     print_prediction(observable, A, B, sysid, max_sq_norm, sampling_method='ball', k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becf1353",
   "metadata": {},
   "source": [
    "# (2) Nonlinear injective observation with $d_s=d_o=d_l=1$\n",
    "Here, we play with systems with 1-dimensional states for plotting and interpretation convenience. In particular, $A$ is a scalar, $B$ is a $1 \\times d_u$ column vector, and we use the observation function $g: \\mathbb{R} \\rightarrow \\mathbb{R}$ given by\n",
    "$$g(x) := sign(x) \\cdot \\sqrt{|x|}$$\n",
    "As we see, this is nonlinear (looks sorta sigmoidal), but it is invertible. Let's see what happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "075e2e0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # dimensions\n",
    "# ds, du, dl = 1, 1, 1\n",
    "\n",
    "# # define observation fn\n",
    "# g = lambda t: jnp.sign(t) * (jnp.abs(t) ** 0.5)\n",
    "# observable = lambda traj: g(traj.x[-1])\n",
    "# do = ds\n",
    "\n",
    "# # send it\n",
    "# T0 = 5000\n",
    "# reset_every = 20\n",
    "# exploration_args = {'scales': 0.5, 'avg_len': 5}\n",
    "# sysid_args = {\n",
    "#     'obs_dim': do,\n",
    "#     'control_dim': du,\n",
    "\n",
    "#     'exploration_args': {'random 1.0': exploration_args,\n",
    "# #                          'impulse 0.25': exploration_args,\n",
    "#                         },\n",
    "\n",
    "#     'method': 'nn',\n",
    "#     'AB_method': 'learned',\n",
    "\n",
    "#     'sigma': 0.01,\n",
    "#     'deterministic_encoder': False,\n",
    "#     'depth': 6,\n",
    "#     'num_iters': 24000,\n",
    "#     'batch_size': 128,\n",
    "#     'lifter_lr': 0.001,\n",
    "\n",
    "#     'seed': SYSID_SEED,\n",
    "# }\n",
    "# A, B, sysid, max_sq_norm = run(observable, ds, du, dl, T0, reset_every, sysid_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19632d05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # check linearization and plot some stuff\n",
    "# summarize(observable, A, B, sysid, max_sq_norm, sampling_method='ball', reset_every=reset_every)\n",
    "# plot_1d(observable, A, B, sysid, max_sq_norm, sampling_method='ball', reset_every=reset_every, N=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e2db2b",
   "metadata": {},
   "source": [
    "# (3) Nonlinear 2-to-1 observation with $d_s=d_l=1$\n",
    "Again, we use 1-dimensional stuff here; we will look into how this scales as we increase dimension later. Now, our observation function $g: \\mathbb{R} \\rightarrow \\mathbb{R}$ is given by\n",
    "$$g(x) := x^2$$\n",
    "This is nonlinear and **not injective**. Two distinct states ($x$ and $-x$) will produce the same observations, and yet they will evolve differently. For example, if we knew $g(x_{t})$, $u_t$, and $g(x_{t-1})$, then we can differentiate between the two states that produce the same observation. We will investigate the efficacy of such ***time-delayed observations*** below.\n",
    "\n",
    "As a clarifying example, suppose $d_u=1$ and $A=B=1$ (i know its only semistable, but simple constants). Then, while the map $x \\mapsto x^2$ is not injective, the map $x_{t} \\mapsto (x_{t-1}^2, u_t, x_t^2)$ is. To see this, let us construct a left inverse. Note that $$x_t = x_{t-1} + u_t \\implies x_t^2 = x_{t-1}^2 + u_t^2 + 2u_tx_{t-1} \\implies x_{t-1} = \\frac{x_t^2 - x_{t-1}^2}{2u_t} - \\frac{u_t}{2}$$\n",
    "Therefore, using only the variables $(x_{t-1}^2, u_t, x_t^2)$ we may write\n",
    "$$x_t = \\frac{x_t^2 - x_{t-1}^2}{2u_t} + \\frac{u_t}{2}$$\n",
    "This time-delayed observation with $d_o = 3$ is enough to faithfully represent the system, which realizes the upper bound $d_o \\leq 3$ given by the Takens embedding (more on this later). (In fact, in this case we can do it with only $d_o=2$ using the variables $(x_t^2 - x_{t-1}^2, u_t) = (g(x_t) - g(x_{t-1}), u_t)$, though this only works here because of the properties of quadratics).\n",
    "\n",
    "We attempt to have our method learn a linearization to this representation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abe38a33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # dimensions\n",
    "# ds, du, dl = 1, 1, 1\n",
    "\n",
    "# # define observation fn\n",
    "# hh = 2\n",
    "# observable = TimeDelayedObservation(hh=hh, control_dim=du, state_dim=ds,\n",
    "#                                     use_states=False, use_controls=True, \n",
    "#                                     use_costs=True, use_cost_diffs=False,\n",
    "#                                     use_time=False)\n",
    "# do = observable.obs_dim\n",
    "\n",
    "# # send it\n",
    "# T0 = 5000\n",
    "# reset_every = 20\n",
    "# exploration_args = {'scales': 1., 'avg_len': 4}\n",
    "# sysid_args = {\n",
    "#     'obs_dim': do,\n",
    "#     'control_dim': du,\n",
    "\n",
    "#     'exploration_args': {'random 1.0': exploration_args,\n",
    "# #                          'impulse 0.25': exploration_args,\n",
    "#                         },\n",
    "\n",
    "#     'method': 'nn',\n",
    "#     'AB_method': 'learned',\n",
    "    \n",
    "#     'sigma': 0.01,\n",
    "#     'deterministic_encoder': False,\n",
    "\n",
    "#     'depth': 6,\n",
    "#     'num_iters': 32000,\n",
    "#     'batch_size': 128,\n",
    "#     'lifter_lr': 0.001,\n",
    "#     'hh': hh,\n",
    "\n",
    "#     'seed': SYSID_SEED,\n",
    "# }\n",
    "# A, B, sysid, max_sq_norm = run(observable, ds, du, dl, T0, reset_every, sysid_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73c3c2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # you know the drill\n",
    "# summarize(observable, A, B, sysid, max_sq_norm, sampling_method='ball', reset_every=reset_every, hh=hh)\n",
    "# plot_1d(observable, A, B, sysid, max_sq_norm, sampling_method='ball', reset_every=reset_every, N=2000, hh=hh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9c6d70",
   "metadata": {},
   "source": [
    "# (4) Nonlinear many-to-1 observation with $d_s=d_l=n$\n",
    "We next seek to understand how the proposed method of time-delay embeddings scales with dimension. Our observation function $g: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is now \n",
    "$$g(x) := \\|x\\|^2$$\n",
    "This is nonlinear and **super-duper not injective**. Each sphere in $\\mathbb{R}^n$ gets condensed into a single observation value. Can we hope to recover the initial states? We appeal to the following theorem (see https://en.wikipedia.org/wiki/Takens%27s_theorem):\n",
    "#### <u>Takens' Theorem (informal)</u>:  Given a dynamical system on $X = \\mathbb{R}^n$ with smooth and determinstic dynamics and a smooth observation function $g: X \\rightarrow \\mathbb{R}$ that is coupled to all dimensions of $X$, the following map is injective:\n",
    "$$x_t \\mapsto (g(x_{t}), g(x_{t+1}), ..., g(x_{t+2n}))$$\n",
    "\n",
    "We saw a special case of this before when $n=1$. In general, we expect that our time-delayed observation should contain at least $2n + 1$ measurements to guarantee injectivity. We try this below.\n",
    "\n",
    "<!-- Just to prove it, the general case for a 1-dimensional LDS would be \n",
    "$$x_{t} = Ax_{t-1} + Bu_t \\implies x_t^2 = A^2x_{t-1}^2 + B^2u_t^2 + 2ABx_{t-1}u_t \\implies x_{t-1} = \\frac{x_t^2 - A^2x_{t-1}^2}{2ABu_t} - \\frac{Bu_t}{2A} \\implies x_t = \\frac{x_t^2 - A^2x_{t-1}^2}{2Bu_t} + \\frac{Bu_t}{2}$$ -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f074e2cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "INFO: Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "INFO: Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
      "INFO: Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
      "INFO: (EXPLORER) generating exploration control sequences using ['random'] w.p. [1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||A||_op = 0.9999998807907104\n",
      "||B||_F = 6.523845672607422\n",
      "||A-BK||_op = 0.5193030834197998\n",
      "eig(A) = [0.9999998  0.59712136 0.59712136 0.5832602  0.58325994 0.5504368 ]\n",
      "svd(B) = [5.4346347  2.6926978  1.8585099  1.2568535  0.79585314 0.32798216]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:06<00:00, 715.41it/s]\n",
      "INFO: (LIFTER): we are imposing simplification as a hard constraint via isometric NN\n",
      "INFO: (LIFTER): we will be linearizing in dimension 7 and linearly project down to dimension 6\n",
      "INFO: (LIFTER): using \"learned\" method to get the AB matrices during each training step\n",
      " 50%|████████████████████████████████████████████████████▏                                                    | 4973/10000 [00:02<00:02, 2407.41it/s]INFO: (EXPLORER) generating exploration control sequences using ['random'] w.p. [1.]\n",
      "\n",
      "  0%|                                                                                                                       | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|█▋                                                                                                           | 80/5000 [00:00<00:06, 793.28it/s]\u001b[A\n",
      "  3%|███▍                                                                                                        | 161/5000 [00:00<00:06, 802.64it/s]\u001b[A\n",
      "  5%|█████▏                                                                                                      | 243/5000 [00:00<00:05, 808.89it/s]\u001b[A\n",
      "  7%|███████                                                                                                     | 326/5000 [00:00<00:05, 813.90it/s]\u001b[A\n",
      "  8%|████████▊                                                                                                   | 410/5000 [00:00<00:05, 819.74it/s]\u001b[A\n",
      " 10%|██████████▋                                                                                                 | 492/5000 [00:00<00:05, 818.49it/s]\u001b[A\n",
      " 11%|████████████▍                                                                                               | 574/5000 [00:00<00:05, 818.76it/s]\u001b[A\n",
      " 13%|██████████████▏                                                                                             | 656/5000 [00:00<00:05, 815.63it/s]\u001b[A\n",
      " 15%|███████████████▉                                                                                            | 738/5000 [00:00<00:05, 814.97it/s]\u001b[A\n",
      " 16%|█████████████████▋                                                                                          | 820/5000 [00:01<00:05, 815.12it/s]\u001b[A\n",
      " 18%|███████████████████▌                                                                                        | 904/5000 [00:01<00:04, 820.52it/s]\u001b[A\n",
      " 20%|█████████████████████▎                                                                                      | 988/5000 [00:01<00:04, 823.43it/s]\u001b[A\n",
      " 21%|██████████████████████▉                                                                                    | 1071/5000 [00:01<00:04, 818.68it/s]\u001b[A\n",
      " 23%|████████████████████████▋                                                                                  | 1153/5000 [00:01<00:04, 818.06it/s]\u001b[A\n",
      " 25%|██████████████████████████▍                                                                                | 1235/5000 [00:01<00:04, 817.77it/s]\u001b[A\n",
      " 26%|████████████████████████████▏                                                                              | 1317/5000 [00:01<00:04, 817.66it/s]\u001b[A\n",
      " 28%|█████████████████████████████▉                                                                             | 1399/5000 [00:01<00:04, 817.32it/s]\u001b[A\n",
      " 30%|███████████████████████████████▋                                                                           | 1481/5000 [00:01<00:04, 816.19it/s]\u001b[A\n",
      " 31%|█████████████████████████████████▍                                                                         | 1563/5000 [00:01<00:04, 814.19it/s]\u001b[A\n",
      " 33%|███████████████████████████████████▏                                                                       | 1645/5000 [00:02<00:04, 766.01it/s]\u001b[A\n",
      " 34%|████████████████████████████████████▊                                                                      | 1723/5000 [00:02<00:04, 763.57it/s]\u001b[A\n",
      " 36%|██████████████████████████████████████▌                                                                    | 1800/5000 [00:02<00:04, 757.83it/s]\u001b[A\n",
      " 38%|████████████████████████████████████████▏                                                                  | 1879/5000 [00:02<00:04, 766.22it/s]\u001b[A\n",
      " 39%|█████████████████████████████████████████▉                                                                 | 1959/5000 [00:02<00:03, 775.21it/s]\u001b[A\n",
      " 41%|███████████████████████████████████████████▋                                                               | 2040/5000 [00:02<00:03, 783.17it/s]\u001b[A\n",
      " 42%|█████████████████████████████████████████████▍                                                             | 2122/5000 [00:02<00:03, 792.19it/s]\u001b[A\n",
      " 44%|███████████████████████████████████████████████▏                                                           | 2203/5000 [00:02<00:03, 796.36it/s]\u001b[A\n",
      " 46%|████████████████████████████████████████████████▊                                                          | 2283/5000 [00:02<00:03, 795.18it/s]\u001b[A\n",
      " 47%|██████████████████████████████████████████████████▌                                                        | 2364/5000 [00:02<00:03, 796.37it/s]\u001b[A\n",
      " 49%|████████████████████████████████████████████████████▎                                                      | 2444/5000 [00:03<00:03, 792.48it/s]\u001b[A\n",
      " 50%|██████████████████████████████████████████████████████                                                     | 2525/5000 [00:03<00:03, 795.56it/s]\u001b[A\n",
      " 52%|███████████████████████████████████████████████████████▊                                                   | 2607/5000 [00:03<00:02, 800.20it/s]\u001b[A\n",
      " 54%|█████████████████████████████████████████████████████████▌                                                 | 2688/5000 [00:03<00:02, 794.58it/s]\u001b[A\n",
      " 55%|███████████████████████████████████████████████████████████▏                                               | 2768/5000 [00:03<00:02, 792.31it/s]\u001b[A\n",
      " 57%|████████████████████████████████████████████████████████████▉                                              | 2848/5000 [00:03<00:02, 793.65it/s]\u001b[A\n",
      " 59%|██████████████████████████████████████████████████████████████▋                                            | 2931/5000 [00:03<00:02, 804.09it/s]\u001b[A\n",
      " 60%|████████████████████████████████████████████████████████████████▍                                          | 3012/5000 [00:03<00:02, 791.28it/s]\u001b[A\n",
      " 62%|██████████████████████████████████████████████████████████████████▏                                        | 3092/5000 [00:03<00:02, 781.24it/s]\u001b[A\n",
      " 63%|███████████████████████████████████████████████████████████████████▊                                       | 3171/5000 [00:03<00:02, 783.47it/s]\u001b[A\n",
      " 65%|█████████████████████████████████████████████████████████████████████▌                                     | 3250/5000 [00:04<00:02, 761.76it/s]\u001b[A\n",
      " 67%|███████████████████████████████████████████████████████████████████████▏                                   | 3329/5000 [00:04<00:02, 769.87it/s]\u001b[A\n",
      " 68%|████████████████████████████████████████████████████████████████████████▉                                  | 3408/5000 [00:04<00:02, 774.77it/s]\u001b[A\n",
      " 70%|██████████████████████████████████████████████████████████████████████████▋                                | 3489/5000 [00:04<00:01, 783.57it/s]\u001b[A\n",
      " 71%|████████████████████████████████████████████████████████████████████████████▍                              | 3572/5000 [00:04<00:01, 796.48it/s]\u001b[A\n",
      " 73%|██████████████████████████████████████████████████████████████████████████████▏                            | 3655/5000 [00:04<00:01, 805.29it/s]\u001b[A\n",
      " 75%|███████████████████████████████████████████████████████████████████████████████▉                           | 3737/5000 [00:04<00:01, 809.52it/s]\u001b[A\n",
      " 76%|█████████████████████████████████████████████████████████████████████████████████▋                         | 3818/5000 [00:04<00:01, 769.56it/s]\u001b[A\n",
      " 78%|███████████████████████████████████████████████████████████████████████████████████▎                       | 3896/5000 [00:04<00:01, 752.27it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|█████████████████████████████████████████████████████████████████████████████████████                      | 3972/5000 [00:05<00:01, 712.03it/s]\u001b[A\n",
      " 81%|██████████████████████████████████████████████████████████████████████████████████████▌                    | 4047/5000 [00:05<00:01, 722.16it/s]\u001b[A\n",
      " 82%|████████████████████████████████████████████████████████████████████████████████████████▏                  | 4120/5000 [00:05<00:01, 714.80it/s]\u001b[A\n",
      " 84%|█████████████████████████████████████████████████████████████████████████████████████████▊                 | 4197/5000 [00:05<00:01, 730.16it/s]\u001b[A\n",
      " 85%|███████████████████████████████████████████████████████████████████████████████████████████▍               | 4271/5000 [00:05<00:01, 723.88it/s]\u001b[A\n",
      " 87%|████████████████████████████████████████████████████████████████████████████████████████████▉              | 4344/5000 [00:05<00:01, 646.05it/s]\u001b[A\n",
      " 88%|██████████████████████████████████████████████████████████████████████████████████████████████▌            | 4421/5000 [00:05<00:00, 678.88it/s]\u001b[A\n",
      " 90%|████████████████████████████████████████████████████████████████████████████████████████████████▎          | 4500/5000 [00:05<00:00, 708.23it/s]\u001b[A\n",
      " 92%|██████████████████████████████████████████████████████████████████████████████████████████████████         | 4581/5000 [00:05<00:00, 736.90it/s]\u001b[A\n",
      " 93%|███████████████████████████████████████████████████████████████████████████████████████████████████▊       | 4663/5000 [00:05<00:00, 759.15it/s]\u001b[A\n",
      " 95%|█████████████████████████████████████████████████████████████████████████████████████████████████████▌     | 4743/5000 [00:06<00:00, 768.59it/s]\u001b[A\n",
      " 97%|███████████████████████████████████████████████████████████████████████████████████████████████████████▎   | 4826/5000 [00:06<00:00, 785.66it/s]\u001b[A\n",
      " 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████  | 4908/5000 [00:06<00:00, 795.53it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:06<00:00, 780.82it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:10<00:00, 938.10it/s]\n",
      "INFO: (LIFTER): ending sysid phase at step 10000\n",
      "INFO: (LIFTER): NOTE THAT WE ARE USING FMEAN TO RESCALE EMBEDDINGS DURING INFERENCE (so outputs of `get_state()` will have their true norms instead of their normalized ones)!!!\n",
      "INFO: training!\n",
      "INFO: mean loss for iters -4800 - 0:\n",
      "INFO: \t\tconsistency: 35.53804397583008\n",
      "INFO: \t\tproj_isometry: 3.4694817066192627\n",
      "INFO: \t\tproj_error: 4.288298435426406e-14\n",
      "INFO: \t\tl2 linearization: 4.2070770263671875\n",
      "INFO: \t\tguess the control: 0.5092071890830994\n",
      "INFO: \t\tsimplification: 16.111858367919922\n",
      "INFO: \t\tsimplification 0: 771.620849609375\n",
      "INFO: \t\tsimplification 1: 3339.053466796875\n",
      "INFO: \t\tsimplification 2: 7694.68994140625\n",
      "INFO: \t\tsimplification 3: 12962.7099609375\n",
      "INFO: mean loss for iters 0 - 4800:\n",
      "INFO: \t\tconsistency: 22.96955012480418\n",
      "INFO: \t\tproj_isometry: 2.266588520358006\n",
      "INFO: \t\tproj_error: 1.946753280097336e-05\n",
      "INFO: \t\tl2 linearization: 0.6117927403872212\n",
      "INFO: \t\tguess the control: 0.5483341627754271\n",
      "INFO: \t\tsimplification: 2.472262511390339\n",
      "INFO: \t\tsimplification 0: 38.776937394316\n",
      "INFO: \t\tsimplification 1: 123.92101293737069\n",
      "INFO: \t\tsimplification 2: 228.7932253505538\n",
      "INFO: \t\tsimplification 3: 343.4598977361371\n",
      "INFO: mean loss for iters 4800 - 9600:\n",
      "INFO: \t\tconsistency: 10.476729260534048\n",
      "INFO: \t\tproj_isometry: 1.9694188830753168\n",
      "INFO: \t\tproj_error: 0.00018858251110638472\n",
      "INFO: \t\tl2 linearization: 0.12119028416772683\n",
      "INFO: \t\tguess the control: 0.4868381539421777\n",
      "INFO: \t\tsimplification: 0.19631920896198912\n",
      "INFO: \t\tsimplification 0: 0.3521781197337744\n",
      "INFO: \t\tsimplification 1: 0.5099783467656622\n",
      "INFO: \t\tsimplification 2: 0.6471454669193675\n",
      "INFO: \t\tsimplification 3: 0.7869601822900586\n",
      "INFO: mean loss for iters 9600 - 14400:\n",
      "INFO: \t\tconsistency: 2.9093244682004054\n",
      "INFO: \t\tproj_isometry: 0.9663256026307742\n",
      "INFO: \t\tproj_error: 0.000427706591082521\n",
      "INFO: \t\tl2 linearization: 0.047660667463205755\n",
      "INFO: \t\tguess the control: 0.35289318598185976\n",
      "INFO: \t\tsimplification: 0.06049227304436499\n",
      "INFO: \t\tsimplification 0: 0.11124912878265604\n",
      "INFO: \t\tsimplification 1: 0.1566167168666531\n",
      "INFO: \t\tsimplification 2: 0.20693935354279044\n",
      "INFO: \t\tsimplification 3: 0.2626601366539641\n",
      "INFO: mean loss for iters 14400 - 19200:\n",
      "INFO: \t\tconsistency: 0.45920282349921765\n",
      "INFO: \t\tproj_isometry: 0.09028212435659952\n",
      "INFO: \t\tproj_error: 0.00018096768135023923\n",
      "INFO: \t\tl2 linearization: 0.014028201160836034\n",
      "INFO: \t\tguess the control: 0.23990471036949507\n",
      "INFO: \t\tsimplification: 0.0102969467120541\n",
      "INFO: \t\tsimplification 0: 0.05284397376235574\n",
      "INFO: \t\tsimplification 1: 0.09297871349495836\n",
      "INFO: \t\tsimplification 2: 0.133542634358552\n",
      "INFO: \t\tsimplification 3: 0.17290551874631396\n",
      "INFO: mean loss for iters 19200 - 24000:\n",
      "INFO: \t\tconsistency: 0.20050006633624434\n",
      "INFO: \t\tproj_isometry: 0.002068140835790473\n",
      "INFO: \t\tproj_error: 1.7489685242821905e-05\n",
      "INFO: \t\tl2 linearization: 0.004781708451143156\n",
      "INFO: \t\tguess the control: 0.14417578539811074\n",
      "INFO: \t\tsimplification: 0.000992949727180985\n",
      "INFO: \t\tsimplification 0: 0.0399444374605082\n",
      "INFO: \t\tsimplification 1: 0.07636759005923523\n",
      "INFO: \t\tsimplification 2: 0.11332567376327157\n",
      "INFO: \t\tsimplification 3: 0.14978002386293762\n",
      "INFO: mean loss for iters 24000 - 28800:\n",
      "INFO: \t\tconsistency: 0.12488013605742405\n",
      "INFO: \t\tproj_isometry: 0.0009056282903353955\n",
      "INFO: \t\tproj_error: 7.203434358444838e-06\n",
      "INFO: \t\tl2 linearization: 0.004698883453287029\n",
      "INFO: \t\tguess the control: 0.08633510948857293\n",
      "INFO: \t\tsimplification: 0.0009946652866604684\n",
      "INFO: \t\tsimplification 0: 0.032970278419476626\n",
      "INFO: \t\tsimplification 1: 0.06062483187386533\n",
      "INFO: \t\tsimplification 2: 0.08968651488112907\n",
      "INFO: \t\tsimplification 3: 0.12042805581906578\n",
      "INFO: mean loss for iters 28800 - 33600:\n",
      "INFO: \t\tconsistency: 0.11544288673670962\n",
      "INFO: \t\tproj_isometry: 0.0005122508013543362\n",
      "INFO: \t\tproj_error: 5.86590044560816e-07\n",
      "INFO: \t\tl2 linearization: 0.005116123332845746\n",
      "INFO: \t\tguess the control: 0.07382197480998003\n",
      "INFO: \t\tsimplification: 0.0009578128230638565\n",
      "INFO: \t\tsimplification 0: 0.02266181448978993\n",
      "INFO: \t\tsimplification 1: 0.039635389966521564\n",
      "INFO: \t\tsimplification 2: 0.05814619396919928\n",
      "INFO: \t\tsimplification 3: 0.07809953902838364\n",
      "INFO: mean loss for iters 33600 - 38400:\n",
      "INFO: \t\tconsistency: 0.11634571952357267\n",
      "INFO: \t\tproj_isometry: 0.000441226521719121\n",
      "INFO: \t\tproj_error: 1.3637609315655016e-07\n",
      "INFO: \t\tl2 linearization: 0.0052323006209432305\n",
      "INFO: \t\tguess the control: 0.069313604265141\n",
      "INFO: \t\tsimplification: 0.0009742519124029059\n",
      "INFO: \t\tsimplification 0: 0.016174443780376652\n",
      "INFO: \t\tsimplification 1: 0.027453986204151685\n",
      "INFO: \t\tsimplification 2: 0.039938968849407196\n",
      "INFO: \t\tsimplification 3: 0.05376572918331173\n",
      "INFO: mean loss for iters 38400 - 43200:\n",
      "INFO: \t\tconsistency: 0.11351608401552464\n",
      "INFO: \t\tproj_isometry: 0.0003733078747215283\n",
      "INFO: \t\tproj_error: 1.3076578929604084e-07\n",
      "INFO: \t\tl2 linearization: 0.00475358975425479\n",
      "INFO: \t\tguess the control: 0.06313811017976453\n",
      "INFO: \t\tsimplification: 0.0008174041310909767\n",
      "INFO: \t\tsimplification 0: 0.01134926383359319\n",
      "INFO: \t\tsimplification 1: 0.01845441362961234\n",
      "INFO: \t\tsimplification 2: 0.026147824801446406\n",
      "INFO: \t\tsimplification 3: 0.0347736325780473\n",
      "INFO: mean loss for iters 43199 - 47999:\n",
      "INFO: \t\tconsistency: 0.10733544201124459\n",
      "INFO: \t\tproj_isometry: 0.00030199404377223495\n",
      "INFO: \t\tproj_error: 4.3856406561931794e-07\n",
      "INFO: \t\tl2 linearization: 0.004190664851533559\n",
      "INFO: \t\tguess the control: 0.05750714359261717\n",
      "INFO: \t\tsimplification: 0.0006508354677559207\n",
      "INFO: \t\tsimplification 0: 0.009154489628611677\n",
      "INFO: \t\tsimplification 1: 0.0146666366806312\n",
      "INFO: \t\tsimplification 2: 0.020293768488190835\n",
      "INFO: \t\tsimplification 3: 0.026148915374069475\n",
      "INFO: (LIFTER): fmean = 49.18821334838867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regression:\n",
      "||A||_op = 0.994233250617981\n",
      "||B||_F = 6.6999921798706055\n",
      "||A-BK||_op = 0.6282056570053101\n",
      "eig(A) = [0.99256796 0.9480052  0.49820593 0.43499544 0.3921764  0.2551529 ]\n",
      "svd(B) = [4.678631   2.6783066  2.390675   2.2802794  2.1664255  0.46754327]\n",
      "\n",
      "moments:\n",
      "||A||_op = 0.9856276512145996\n",
      "||B||_F = 0.8313811421394348\n",
      "||A-BK||_op = 0.9569242596626282\n",
      "eig(A) = [0.9803302  0.9560705  0.52224904 0.46715432 0.41272372 0.23813239]\n",
      "svd(B) = [0.5773628  0.34099415 0.29929408 0.28291097 0.2649356  0.04199268]\n",
      "\n",
      "learned:\n",
      "||A||_op = 1.0046931505203247\n",
      "||B||_F = 7.7951226234436035\n",
      "||A-BK||_op = 0.4401011342481997\n",
      "eig(A) = [1.0043043  0.99799126 0.5053068  0.43385407 0.38359633 0.2399173 ]\n",
      "svd(B) = [5.4535313  3.0371237  2.841253   2.5853007  2.5155313  0.84523773]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dimensions\n",
    "n = 6\n",
    "ds, du, dl = n, n, n\n",
    "\n",
    "# define observation fn\n",
    "hh = 2 * ds + 1\n",
    "observable = TimeDelayedObservation(hh=hh, control_dim=du, state_dim=ds,\n",
    "                                    use_states=False, use_controls=True, \n",
    "                                    use_costs=True, use_cost_diffs=False,\n",
    "                                    use_time=False)\n",
    "do = observable.obs_dim\n",
    "\n",
    "# send it\n",
    "T0 = 10000\n",
    "reset_every = 200\n",
    "exploration_args = {'scales': 1., 'avg_len': 4}\n",
    "sysid_args = {\n",
    "    'obs_dim': do,\n",
    "    'control_dim': du,\n",
    "\n",
    "    'exploration_args': {'random 1.0': exploration_args,\n",
    "#                          'impulse 0.25': exploration_args,\n",
    "                        },\n",
    "\n",
    "    'method': 'nn',\n",
    "    'AB_method': 'learned',\n",
    "    \n",
    "    'sigma': 0,\n",
    "    'deterministic_encoder': False,\n",
    "\n",
    "    'depth': 8,\n",
    "    'num_iters': 48000,\n",
    "    'batch_size': 128,\n",
    "    'lifter_lr': 0.001,\n",
    "    'hh': 4,\n",
    "\n",
    "    'seed': SYSID_SEED,\n",
    "}\n",
    "A, B, sysid, max_sq_norm = run(observable, ds, du, dl, T0, reset_every, sysid_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfdc1b11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 544.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward direction:\n",
      "\t||Ahat @ C - C @ A||^2: \t4.2569684982299805\n",
      "\t||Bhat - C @ B||^2: \t\t34.022850036621094\n",
      "reverse direction:\n",
      "\t||A @ C - C @ Ahat||^2: \t2.9133408069610596\n",
      "\t||B - C @ Bhat||^2: \t\t77.0259780883789\n",
      "\n",
      "injectivity: \t||C_reverse @ C_forward - I_ds||^2 / ds^2 = 0.11435025185346603\n",
      "surjectivity: \t||C_forward @ C_reverse - I_dl||^2 / dl^2 = 0.10656081140041351\n",
      "\n",
      "singular values of C_forward: \t[3.4447145  2.5800993  1.1725745  0.7504757  0.40664116 0.19159141]\n",
      "singular values of C_reverse: \t[3.0792890e+00 1.0080560e+00 6.0104096e-01 2.9152694e-01 2.3969273e-01\n",
      " 2.4480140e-03]\n"
     ]
    }
   ],
   "source": [
    "# the same\n",
    "summarize(observable, A, B, sysid, max_sq_norm, sampling_method='ball', reset_every=reset_every, hh=hh)\n",
    "if n == 2: plot_2d(observable, A, B, sysid, max_sq_norm, sampling_method='ball', N=2000, reset_every=reset_every, hh=hh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc544a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "extravaganza",
   "language": "python",
   "name": "extravaganza"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
